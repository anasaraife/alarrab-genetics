# ==============================================================================
#  Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù…Ù† Ø§Ù„ØµÙØ±
# ==============================================================================

import streamlit as st
import gdown
import PyPDF2
import os
import tempfile
from sentence_transformers import SentenceTransformer
import faiss # Ø§Ø³ØªÙŠØ±Ø§Ø¯ FAISS
import numpy as np # Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… NumPy Ù…Ø¹ FAISS
import requests # Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø·Ù„Ø¨Ø§Øª API
import json

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±
# -------------------------------------------------
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª",
    page_icon="ğŸ•Šï¸",
    layout="wide",
)

# Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ¨ (Google Drive PDF Links)
BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
    "https://drive.google.com/file/d/18pc9PptjfcjQfPyVCiaSq30RFs3ZjXF4/view?usp=sharing",
    "https://drive.google.com/file/d/17hklyXm2R6ChYRddDbYRkqrtD8mE_nC_/view?usp=sharing",
    "https://drive.google.com/file/d/1Mq3zgz4NDm6guelOzuni3O4_2kaQpJAi/view?usp=sharing",
    "https://drive.google.com/file/d/1hoCxIPU9xJgsl1J-AnEG2E0AX3H5c5Kg/view?usp=sharing",
    "https://drive.google.com/file/d/14qInRfBTOhOJYsjs6tYRxAq1xFDrD-_O/view?usp=sharing",
    "https://drive.google.com/file/d/1kaVob_EdCP5v_H71nUS3O1-YairROV1b/view?usp=sharing"
]

# -------------------------------------------------
#  2. ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† (Embedding Model)
# -------------------------------------------------
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer(\'paraphrase-multilingual-mpnet-base-v2\')

# -------------------------------------------------
#  3. Ø¯Ø§Ù„Ø© ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ¨ (PDF) ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
# -------------------------------------------------
@st.cache_data(ttl=3600)
def load_process_and_embed_books(model):
    all_chunks = []
    all_metadata = []
    all_ids = []
    doc_id_counter = 0

    with st.status("âš™ï¸ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ¨ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª...", expanded=True) as status:
        for i, link in enumerate(BOOK_LINKS):
            status.update(label=f"Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨ {i+1}/{len(BOOK_LINKS)}...")
            try:
                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:
                    file_id = link.split(\"/d/\")[1].split(\"/\")[0]
                    gdown.download(id=file_id, output=tmp.name, quiet=True)
                    text = ""
                    with open(tmp.name, \"rb\") as f:
                        reader = PyPDF2.PdfReader(f)
                        for page in reader.pages:
                            text += (page.extract_text() or "") + "\n"
                    
                    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ (chunks)
                    chunks = text.split(\'\\n\\n\') # ØªÙ‚Ø³ÙŠÙ… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø³Ø·Ø±ÙŠÙ† ÙØ§Ø±ØºÙŠÙ†
                    for chunk in chunks:
                        if len(chunk.strip()) > 150: # ØªØµÙÙŠØ© Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                            all_chunks.append(chunk.strip())
                            all_metadata.append({\'source\': link, \'book_index\': i+1})
                            all_ids.append(f"doc_{doc_id_counter}")
                            doc_id_counter += 1
            except Exception as e:
                st.error(f"ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨ {link}: {e}")
            finally:
                if \'tmp\' in locals() and os.path.exists(tmp.name):
                    os.remove(tmp.name)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡
        if all_chunks:
            status.update(label="âš™ï¸ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù„Ù„Ù†ØµÙˆØµ...")
            all_embeddings = model.encode(all_chunks).tolist()
            status.update(label="âœ… Ø§ÙƒØªÙ…Ù„ ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ¨ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª!", state="complete")
            return all_chunks, all_metadata, all_ids, all_embeddings
        else:
            status.update(label="âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ù„Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§.", state="complete")
            return [], [], [], []

# -------------------------------------------------
#  4. Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ© (FAISS)
# -------------------------------------------------
@st.cache_resource
def build_faiss_index(embeddings):
    if not embeddings:
        return None
    # ØªØ­ÙˆÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© NumPy
    embeddings_array = np.array(embeddings).astype(\'float32\')
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    dimension = embeddings_array.shape[1]
    # Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS (Ù‡Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… IndexFlatL2 Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚)
    index = faiss.IndexFlatL2(dimension)
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„ÙÙ‡Ø±Ø³
    index.add(embeddings_array)
    return index

# -------------------------------------------------
#  5. Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini API
# -------------------------------------------------
@st.cache_data
def translate_text_with_gemini(text_to_translate):
    """
    ØªØ³ØªØ®Ø¯Ù… Gemini API Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
    """
    API_KEY = "" # Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…ÙØªØ§Ø­ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}"
    
    prompt = f"Translate the following technical text about pigeon genetics into clear and accurate Arabic. Keep the scientific terms if there is no common Arabic equivalent. Text to translate: \"{text_to_translate}\""
    
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }]
    }
    
    try:
        response = requests.post(API_URL, json=payload, headers={"Content-Type": "application/json"})
        response.raise_for_status()
        result = response.json()
        
        if result.get(\'candidates\'):
            return result[\'candidates\'][0][\'content\'][\'parts\'][0][\'text\']
        else:
            return "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ù‡Ùˆ Ø§Ù„Ø£ÙØ¶Ù„ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©."
    except requests.exceptions.RequestException as e:
        print(f"Error calling Gemini API: {e}")
        return f"ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: {text_to_translate}"

# -------------------------------------------------
#  6. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# -------------------------------------------------
st.title("ğŸ•Šï¸ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 4.0")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
model = load_embedding_model()
chunks, metadata, ids, embeddings = load_process_and_embed_books(model)
faiss_index = build_faiss_index(embeddings)

tab1, tab2 = st.tabs(["ğŸ§  Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ", "ğŸ§¬ Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© (Ù‚Ø±ÙŠØ¨Ø§Ù‹)"])

with tab1:
    st.header("Ø­ÙˆØ§Ø± Ù…Ø¹ Ø®Ø¨ÙŠØ± Ø§Ù„ÙˆØ±Ø§Ø«Ø©")
    st.write("Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ØªØ±Ø¬Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ©.")
    
    query = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:", placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡Ùˆ ØªØ£Ø«ÙŠØ± Ø¬ÙŠÙ† SpreadØŸ", label_visibility="collapsed")

    if query:
        if faiss_index:
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©..."):
                # 1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
                query_embedding = model.encode([query]).astype(\'float32\')
                D, I = faiss_index.search(query_embedding, k=1) # k=1 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‚Ø±Ø¨ Ù†ØªÙŠØ¬Ø©
                
                if I[0][0] != -1: # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø©
                    retrieved_chunk_index = I[0][0]
                    retrieved_chunk = chunks[retrieved_chunk_index]
                    retrieved_metadata = metadata[retrieved_chunk_index]
                    
                    translated_text = translate_text_with_gemini(retrieved_chunk)
                    
                    # 3. Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªØ±Ø¬Ù…Ø©
                    st.success("**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù…ØªØ±Ø¬Ù…Ø© Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±):**")
                    st.markdown(f"<div dir=\'rtl\' style=\'text-align: right;\'>{translated_text}</div>", unsafe_allow_html=True)
                    st.caption(f"Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ØµÙ„ÙŠ: {retrieved_metadata[\'source\']}")

                else:
                    st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")
        else:
            st.warning("Ù„Ù… ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†ØµÙˆØµ Ù„Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§.")

with tab2:
    st.header("Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
    st.info("Ø³ÙŠØªÙ… ØªÙØ¹ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø© ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© Ù…Ù† Ø®Ø§Ø±Ø·Ø© Ø§Ù„Ø·Ø±ÙŠÙ‚.")
    st.image("https://placehold.co/600x300/e2e8f0/4a5568?text=Genetic+Calculator+UI", caption="ØªØµÙˆØ± Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©")



