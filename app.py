# ==============================================================================
#  HOT-PATCH FOR SQLITE3 VERSION ON STREAMLIT CLOUD
# ==============================================================================
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# ==============================================================================


# ==============================================================================
#  Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 4.0 (Ù…Ø¹ Ø§Ù„ØªØ±Ø¬Ù…Ø© ÙˆØ§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©)
# ==============================================================================

import streamlit as st
import chromadb
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os
import tempfile
import requests # Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø·Ù„Ø¨Ø§Øª API
import json
import shutil # Ù„Ø¥Ø¶Ø§ÙØ© shutil.rmtree

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±
# -------------------------------------------------
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª",
    page_icon="ğŸ•Šï¸",
    layout="wide",
)

# Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ¨
BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
    "https://drive.google.com/file/d/18pc9PptjfcjQfPyVCiaSq30RFs3ZjXF4/view?usp=sharing",
    "https://drive.google.com/file/d/17hklyXm2R6ChYRddDbYRkqrtD8mE_nC_/view?usp=sharing",
    "https://drive.google.com/file/d/1Mq3zgz4NDm6guelOzuni3O4_2kaQpJAi/view?usp=sharing",
    "https://drive.google.com/file/d/1hoCxIPU9xJgsl1J-AnEG2E0AX3H5c5Kg/view?usp=sharing",
    "https://drive.google.com/file/d/14qInRfBTOhOJYsjs6tYRxAq1xFDrD-_O/view?usp=sharing",
    "https://drive.google.com/file/d/1kaVob_EdCP5v_H71nUS3O1-YairROV1b/view?usp=sharing"
]

# -------------------------------------------------
#  2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# -------------------------------------------------

@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

@st.cache_resource
def init_chroma_db(): # Ø¥Ø²Ø§Ù„Ø© embedding_model ÙƒÙ…Ø¹Ø§Ù…Ù„
    temp_dir = tempfile.gettempdir()
    db_path = os.path.join(temp_dir, "chroma_db_godfather")
    
    # Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© (Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸ÙŠÙ)
    if os.path.exists(db_path):
        try:
            shutil.rmtree(db_path)
            st.warning("ØªÙ… Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ChromaDB Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡.")
        except Exception as e:
            st.error(f"ÙØ´Ù„ Ø­Ø°Ù Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ChromaDB Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©: {e}")

    client = chromadb.PersistentClient(path=db_path)
    # Ù„Ø§ ØªÙ…Ø±Ø± embedding_function Ù‡Ù†Ø§
    return client.get_or_create_collection(name="pigeon_genetics_knowledge")

@st.cache_data(ttl=3600)
def build_knowledge_base(_collection, model): # Ø¥Ø¶Ø§ÙØ© model ÙƒÙ…Ø¹Ø§Ù…Ù„
    if _collection.count() == 0:
        with st.status("âš™ï¸ ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ø£ÙˆÙ„ Ù…Ø±Ø©...", expanded=True) as status:
            all_chunks, all_metadata, all_ids = [], [], []
            doc_id_counter = 0
            for i, link in enumerate(BOOK_LINKS):
                status.update(label=f"Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨ {i+1}/{len(BOOK_LINKS)}...")
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                        file_id = link.split('/d/')[1].split('/')[0]
                        gdown.download(id=file_id, output=tmp.name, quiet=True)
                        text = ""
                        with open(tmp.name, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page in reader.pages:
                                text += (page.extract_text() or "") + "\n"
                        chunks = text.split('\n\n')
                        for chunk in chunks:
                            if len(chunk.strip()) > 150:
                                all_chunks.append(chunk.strip())
                                all_metadata.append({'source': link})
                                all_ids.append(f"doc_{doc_id_counter}")
                                doc_id_counter += 1
                finally:
                    if 'tmp' in locals() and os.path.exists(tmp.name):
                        os.remove(tmp.name)
            if all_chunks:
                # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¶Ø§ÙØ©
                embeddings = model.encode(all_chunks).tolist()
                _collection.add(documents=all_chunks, metadatas=all_metadata, ids=all_ids, embeddings=embeddings)
            status.update(label="âœ… Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©!", state="complete")
    return True

# -------------------------------------------------
#  3. Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini API
# -------------------------------------------------
@st.cache_data
def translate_text_with_gemini(text_to_translate):
    """
    ØªØ³ØªØ®Ø¯Ù… Gemini API Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
    """
    API_KEY = "" # Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…ÙØªØ§Ø­ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}"
    
    prompt = f"Translate the following technical text about pigeon genetics into clear and accurate Arabic. Keep the scientific terms if there is no common Arabic equivalent. Text to translate: \"{text_to_translate}\""
    
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }]
    }
    
    try:
        response = requests.post(API_URL, json=payload, headers={"Content-Type": "application/json"})
        response.raise_for_status()
        result = response.json()
        
        if result.get('candidates'):
            return result['candidates'][0]['content']['parts'][0]['text']
        else:
            return "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ù‡Ùˆ Ø§Ù„Ø£ÙØ¶Ù„ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©."
    except requests.exceptions.RequestException as e:
        print(f"Error calling Gemini API: {e}")
        return f"ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: {text_to_translate}"

# -------------------------------------------------
#  4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# -------------------------------------------------
st.title("ğŸ•Šï¸ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 4.0")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
model = load_embedding_model()
db_collection = init_chroma_db() # Ù„Ø§ ØªÙ…Ø±Ø± model Ù‡Ù†Ø§
# ØªÙ…Ø±ÙŠØ± model Ø¥Ù„Ù‰ build_knowledge_base
build_knowledge_base(db_collection, model)

tab1, tab2 = st.tabs(["ğŸ§  Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ", "ğŸ§¬ Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© (Ù‚Ø±ÙŠØ¨Ø§Ù‹)"])

with tab1:
    st.header("Ø­ÙˆØ§Ø± Ù…Ø¹ Ø®Ø¨ÙŠØ± Ø§Ù„ÙˆØ±Ø§Ø«Ø©")
    st.write("Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ØªØ±Ø¬Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ©.")
    
    query = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:", placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡Ùˆ ØªØ£Ø«ÙŠØ± Ø¬ÙŠÙ† SpreadØŸ", label_visibility="collapsed")

    if query:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©..."):
            # 1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
            # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø­Ø«
            query_embedding = model.encode([query]).tolist()[0]
            results = db_collection.query(query_embeddings=[query_embedding], n_results=1)
            documents = results.get('documents', [[]])[0]

            if documents:
                # 2. Ø£Ø®Ø° Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© ÙˆØªØ±Ø¬Ù…ØªÙ‡Ø§
                best_result_text = documents[0]
                source = results['metadatas'][0][0]['source']
                
                translated_text = translate_text_with_gemini(best_result_text)
                
                # 3. Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªØ±Ø¬Ù…Ø©
                st.success("**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù…ØªØ±Ø¬Ù…Ø© Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±):**")
                st.markdown(f"<div dir='rtl' style='text-align: right;'>{translated_text}</div>", unsafe_allow_html=True)
                st.caption(f"Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ØµÙ„ÙŠ (Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©): {source}")

            else:
                st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")

with tab2:
    st.header("Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
    st.info("Ø³ÙŠØªÙ… ØªÙØ¹ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø© ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© Ù…Ù† Ø®Ø§Ø±Ø·Ø© Ø§Ù„Ø·Ø±ÙŠÙ‚.")
    st.image("https://placehold.co/600x300/e2e8f0/4a5568?text=Genetic+Calculator+UI", caption="ØªØµÙˆØ± Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©")

