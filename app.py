# ==============================================================================
#  Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 14.0 (Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø­Ø³Ù†)
#  - ÙŠØ¯Ù…Ø¬ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹ØµØ±ÙŠØ© Ù…Ø¹ Ø§Ù„Ø¹Ù‚Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©
# ==============================================================================

import streamlit as st
import sqlite3
from sklearn.metrics.pairwise import cosine_similarity
import gdown
import PyPDF2
import os
import tempfile
import requests
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
import time
import hashlib
from datetime import datetime
from itertools import product
import collections
import pandas as pd
import io
import re

# --- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±ÙŠØ© ---
try:
    from sentence_transformers import SentenceTransformer
    VECTOR_SEARCH_AVAILABLE = True
except ImportError:
    VECTOR_SEARCH_AVAILABLE = False

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„ØªØµÙ…ÙŠÙ…
# -------------------------------------------------
st.set_page_config(
    layout="wide",
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V14.0",
    page_icon="ğŸ§¬",
    initial_sidebar_state="expanded"
)

# --- CSS Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹ØµØ±ÙŠØ© ---
st.markdown("""
<style>
    /* Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© */
    .stDeployButton, #MainMenu, footer, header {visibility: hidden;}
    .block-container { padding: 0 !important; }
    
    /* Ø§Ù„Ø®Ù„ÙÙŠØ© ÙˆØ§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¹Ø§Ù… */
    .stApp {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    
    /* Ø­Ø§ÙˆÙŠØ© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© */
    .chat-container {
        background: rgba(255, 255, 255, 0.9);
        border-radius: 20px;
        padding: 0;
        margin: 20px auto;
        max-width: 1000px;
        height: 95vh;
        display: flex;
        flex-direction: column;
        box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
    }
    
    /* Ø´Ø±ÙŠØ· Ø§Ù„Ø¹Ù†ÙˆØ§Ù† */
    .header-bar {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 15px 25px;
        display: flex;
        align-items: center;
        justify-content: space-between;
        border-radius: 20px 20px 0 0;
        flex-shrink: 0;
    }
    
    .header-title { font-size: 24px; font-weight: bold; margin: 0; display: flex; align-items: center; gap: 15px; }
    .status-indicator { width: 10px; height: 10px; background: #00ff88; border-radius: 50%; animation: pulse 2s infinite; box-shadow: 0 0 8px #00ff88; }
    
    @keyframes pulse { 0% { opacity: 1; } 50% { opacity: 0.5; } 100% { opacity: 1; } }
    
    /* Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© */
    .chat-area { flex-grow: 1; overflow-y: auto; padding: 20px 30px; }
    
    /* Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© */
    .message { margin-bottom: 20px; animation: slideIn 0.3s ease-out; }
    @keyframes slideIn { from { opacity: 0; transform: translateY(15px); } to { opacity: 1; transform: translateY(0); } }
    
    .user-message { display: flex; justify-content: flex-end; }
    .user-bubble { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 12px 18px; border-radius: 20px 20px 5px 20px; max-width: 80%; word-wrap: break-word; }
    
    .assistant-message { display: flex; align-items: flex-start; gap: 15px; }
    .avatar { width: 40px; height: 40px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 50%; display: flex; align-items: center; justify-content: center; font-size: 20px; color: white; flex-shrink: 0; }
    .assistant-bubble { background: #f1f3f5; border: 1px solid #e9ecef; padding: 15px 20px; border-radius: 20px 20px 20px 5px; max-width: calc(100% - 55px); word-wrap: break-word; }
    
    /* Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ */
    .input-area { padding: 15px 25px; background: #ffffff; border-radius: 0 0 20px 20px; border-top: 1px solid #e9ecef; flex-shrink: 0; }
    
    /* Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹ */
    .quick-actions { display: flex; gap: 8px; margin-bottom: 10px; flex-wrap: wrap; }
    .quick-btn { background: #e9ecef; border: none; color: #495057; padding: 6px 14px; border-radius: 15px; cursor: pointer; transition: all 0.2s ease; font-size: 13px; }
    .quick-btn:hover { background: #dee2e6; transform: translateY(-1px); }
    
    /* Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© */
    .genetics-calculator { background: #f8f9fa; border: 1px solid #e9ecef; border-radius: 15px; padding: 20px; margin-top: 15px; }
    .calc-header { font-size: 18px; font-weight: bold; margin-bottom: 15px; color: #495057; }
    
    /* Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª */
    .sources-section { background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 10px; padding: 15px; margin-top: 15px; }
    .source-item { margin-bottom: 8px; padding: 8px; background: white; border-radius: 5px; border-left: 3px solid #667eea; }
    
</style>
""", unsafe_allow_html=True)

# -------------------------------------------------
#  2. Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…Ø¯Ø±Ø§Ø¡
# -------------------------------------------------

# --- Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬ÙŠÙ†Ø§Øª (Ù„Ù„Ø­Ø§Ø³Ø¨Ø©) ---
GENE_DATA = {
    'B': {'display_name_ar': "Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ", 'type_en': 'sex-linked', 'emoji': 'ğŸ¨', 'alleles': {'BA': 'Ø¢Ø´ Ø±ÙŠØ¯', '+': 'Ø£Ø²Ø±Ù‚/Ø£Ø³ÙˆØ¯', 'b': 'Ø¨Ù†ÙŠ'}, 'dominance': ['BA', '+', 'b']},
    'd': {'display_name_ar': "Ø§Ù„ØªØ®ÙÙŠÙ", 'type_en': 'sex-linked', 'emoji': 'ğŸ’§', 'alleles': {'+': 'Ø¹Ø§Ø¯ÙŠ', 'd': 'Ù…Ø®ÙÙ'}, 'dominance': ['+', 'd']},
    'e': {'display_name_ar': "Ø£Ø­Ù…Ø± Ù…ØªÙ†Ø­ÙŠ", 'type_en': 'autosomal', 'emoji': 'ğŸ”´', 'alleles': {'+': 'Ø¹Ø§Ø¯ÙŠ', 'e': 'Ø£Ø­Ù…Ø± Ù…ØªÙ†Ø­ÙŠ'}, 'dominance': ['+', 'e']},
    'C': {'display_name_ar': "Ø§Ù„Ù†Ù…Ø·", 'type_en': 'autosomal', 'emoji': 'ğŸ“', 'alleles': {'CT': 'Ù†Ù…Ø· ØªÙŠ', 'C': 'ØªØ´ÙŠÙƒØ±', '+': 'Ø¨Ø§Ø±', 'c': 'Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø±'}, 'dominance': ['CT', 'C', '+', 'c']},
    'S': {'display_name_ar': "Ø§Ù„Ø§Ù†ØªØ´Ø§Ø±", 'type_en': 'autosomal', 'emoji': 'ğŸŒŠ', 'alleles': {'S': 'Ù…Ù†ØªØ´Ø±', '+': 'Ø¹Ø§Ø¯ÙŠ'}, 'dominance': ['S', '+']}
}
GENE_ORDER = list(GENE_DATA.keys())
NAME_TO_SYMBOL_MAP = {g: {n: s for s, n in d['alleles'].items()} for g, d in GENE_DATA.items()}

# --- Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ¨ ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø± ---
BOOK_LINKS = {
    "ÙƒØªØ§Ø¨ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ": "https://drive.google.com/file/d/1ABC123/view",
    "Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©": "https://drive.google.com/file/d/2DEF456/view", 
    "Ø£Ø³Ø±Ø§Ø± Ø§Ù„ØªØ±Ø¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©": "https://drive.google.com/file/d/3GHI789/view"
}

# --- Ù…Ø¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø­Ø³Ù† ---
class AIModelManager:
    def __init__(self):
        self.models = {
            "gemini": {
                "name": "Google Gemini", 
                "available": self._check_secret("GEMINI_API_KEY"), 
                "priority": 1,
                "endpoint": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
            },
            "deepseek": {
                "name": "DeepSeek", 
                "available": self._check_secret("DEEPSEEK_API_KEY"), 
                "priority": 2,
                "endpoint": "https://api.deepseek.com/v1/chat/completions"
            },
            "openai": {
                "name": "OpenAI GPT", 
                "available": self._check_secret("OPENAI_API_KEY"), 
                "priority": 3,
                "endpoint": "https://api.openai.com/v1/chat/completions"
            }
        }
    
    def _check_secret(self, key: str) -> bool:
        try: 
            return st.secrets.get(key) is not None and st.secrets[key] != ""
        except Exception: 
            return False
    
    def get_available_models(self) -> List[str]:
        available = [model for model, config in self.models.items() if config["available"]]
        return sorted(available, key=lambda x: self.models[x]["priority"])
    
    def get_model_info(self, model_key: str) -> Dict:
        return self.models.get(model_key, {})

# --- Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ø³Ù† ---
class KnowledgeManager:
    def __init__(self, embedder=None):
        self.embedder = embedder
        self.db_path = os.path.join(tempfile.gettempdir(), "genetics_knowledge_v14.db")
        self.conn = None
        self._init_database()
    
    def _init_database(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¬Ø¯Ø§ÙˆÙ„Ù‡Ø§"""
        try:
            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
            cursor = self.conn.cursor()
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø¹Ø±ÙØ©
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS knowledge (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source TEXT NOT NULL,
                    content TEXT NOT NULL,
                    content_hash TEXT UNIQUE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    embedding BLOB
                )
            """)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS search_index (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    knowledge_id INTEGER,
                    keywords TEXT,
                    FOREIGN KEY (knowledge_id) REFERENCES knowledge (id)
                )
            """)
            
            self.conn.commit()
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    
    def add_content(self, source: str, content: str) -> bool:
        """Ø¥Ø¶Ø§ÙØ© Ù…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        try:
            content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
            cursor = self.conn.cursor()
            
            # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            cursor.execute("SELECT id FROM knowledge WHERE content_hash = ?", (content_hash,))
            if cursor.fetchone():
                return False  # Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø§Ù‹
            
            # Ø¥Ù†Ø´Ø§Ø¡ embedding Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            embedding_blob = None
            if self.embedder:
                try:
                    embedding = self.embedder.encode([content])[0]
                    embedding_blob = embedding.tobytes()
                except Exception as e:
                    st.warning(f"ØªØ¹Ø°Ø± Ø¥Ù†Ø´Ø§Ø¡ embedding: {e}")
            
            # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            cursor.execute("""
                INSERT INTO knowledge (source, content, content_hash, embedding)
                VALUES (?, ?, ?, ?)
            """, (source, content, content_hash, embedding_blob))
            
            knowledge_id = cursor.lastrowid
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            keywords = self._extract_keywords(content)
            cursor.execute("""
                INSERT INTO search_index (knowledge_id, keywords)
                VALUES (?, ?)
            """, (knowledge_id, keywords))
            
            self.conn.commit()
            return True
            
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {e}")
            return False
    
    def _extract_keywords(self, content: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„Ø±Ù…ÙˆØ²
        clean_content = re.sub(r'[^\w\s]', ' ', content)
        words = clean_content.split()
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆØ§Ù„Ø´Ø§Ø¦Ø¹Ø©
        stop_words = {'ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'Ø¹Ù†', 'Ù…Ø¹', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ù‡', 'Ø°Ù„Ùƒ', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ'}
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        return ' '.join(keywords[:50])  # Ø£Ø®Ø° Ø£ÙˆÙ„ 50 ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ©
    
    def search_content(self, query: str, limit: int = 5) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        try:
            cursor = self.conn.cursor()
            results = []
            
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø£ÙˆÙ„Ø§Ù‹
            text_results = self._text_search(query, limit)
            results.extend(text_results)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            if self.embedder and len(results) < limit:
                vector_results = self._vector_search(query, limit - len(results))
                results.extend(vector_results)
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            seen_ids = set()
            unique_results = []
            for result in results:
                if result['id'] not in seen_ids:
                    unique_results.append(result)
                    seen_ids.add(result['id'])
            
            return unique_results[:limit]
            
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
            return []
    
    def _text_search(self, query: str, limit: int) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ"""
        try:
            cursor = self.conn.cursor()
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
            query_words = query.split()
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… SQL Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            placeholders = ' OR '.join(['content LIKE ?' for _ in query_words])
            sql = f"""
                SELECT DISTINCT k.id, k.source, k.content, 
                       (CASE 
                        WHEN {placeholders} THEN 1 
                        ELSE 0 
                        END) as relevance_score
                FROM knowledge k
                LEFT JOIN search_index si ON k.id = si.knowledge_id
                WHERE {placeholders}
                ORDER BY relevance_score DESC
                LIMIT ?
            """
            
            params = [f'%{word}%' for word in query_words] * 2 + [limit]
            cursor.execute(sql, params)
            
            results = []
            for row in cursor.fetchall():
                results.append({
                    'id': row[0],
                    'source': row[1],
                    'content': row[2][:500] + '...' if len(row[2]) > 500 else row[2],
                    'full_content': row[2],
                    'score': row[3]
                })
            
            return results
            
        except Exception as e:
            st.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ: {e}")
            return []
    
    def _vector_search(self, query: str, limit: int) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
        try:
            cursor = self.conn.cursor()
            
            # Ø¥Ù†Ø´Ø§Ø¡ embedding Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query_embedding = self.embedder.encode([query])[0]
            
            # Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ embeddings Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            cursor.execute("SELECT id, source, content, embedding FROM knowledge WHERE embedding IS NOT NULL")
            rows = cursor.fetchall()
            
            if not rows:
                return []
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            similarities = []
            for row in rows:
                try:
                    stored_embedding = np.frombuffer(row[3], dtype=np.float32)
                    similarity = cosine_similarity([query_embedding], [stored_embedding])[0][0]
                    
                    if similarity > 0.3:  # Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                        similarities.append({
                            'id': row[0],
                            'source': row[1],
                            'content': row[2][:500] + '...' if len(row[2]) > 500 else row[2],
                            'full_content': row[2],
                            'score': float(similarity)
                        })
                except Exception as e:
                    continue
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            similarities.sort(key=lambda x: x['score'], reverse=True)
            return similarities[:limit]
            
        except Exception as e:
            st.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡: {e}")
            return []
    
    def get_knowledge_stats(self) -> Dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        try:
            cursor = self.conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM knowledge")
            total_docs = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT source) FROM knowledge")
            total_sources = cursor.fetchone()[0]
            
            return {
                'total_documents': total_docs,
                'total_sources': total_sources,
                'vector_search_enabled': self.embedder is not None
            }
        except Exception:
            return {'total_documents': 0, 'total_sources': 0, 'vector_search_enabled': False}

# --- Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ---
class AdvancedGeneticCalculator:
    def describe_phenotype(self, gt_dict: Dict) -> Tuple[str, str]:
        """ÙˆØµÙ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¸Ø§Ù‡Ø±ÙŠ Ù…Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„ÙˆØ±Ø§Ø«ÙŠ"""
        phenotypes = {g: "" for g in GENE_ORDER}
        
        for gene, gt_part in gt_dict.items():
            alleles = gt_part.replace('â€¢//', '').split('//')
            for dom_allele in GENE_DATA[gene]['dominance']:
                if dom_allele in alleles:
                    phenotypes[gene] = GENE_DATA[gene]['alleles'][dom_allele]
                    break
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø­Ù…Ø± Ø§Ù„Ù…ØªÙ†Ø­ÙŠ
        if 'e//e' in gt_dict.get('e', ''):
            phenotypes['B'] = 'Ø£Ø­Ù…Ø± Ù…ØªÙ†Ø­ÙŠ'
            phenotypes['C'] = ''
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ù†ØªØ´Ø§Ø±
        if 'S' in gt_dict.get('S', ''):
            if 'e//e' not in gt_dict.get('e', ''):
                phenotypes['C'] = 'Ù…Ù†ØªØ´Ø±'
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù†Ø³
        sex = "Ø£Ù†Ø«Ù‰" if any('â€¢' in gt_dict.get(g, '') for g, d in GENE_DATA.items() if d['type_en'] == 'sex-linked') else "Ø°ÙƒØ±"
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙˆØµÙ
        desc_parts = [
            phenotypes.get('B'),
            'Ù…Ø®ÙÙ' if phenotypes.get('d') == 'Ù…Ø®ÙÙ' else None,
            phenotypes.get('C')
        ]
        
        phenotype_desc = f"{sex} {' '.join(filter(None, desc_parts))}"
        genotype_str = " | ".join([gt_dict[g].strip() for g in GENE_ORDER])
        
        return phenotype_desc, genotype_str

    def calculate(self, parent_inputs: Dict) -> Dict:
        """Ø­Ø³Ø§Ø¨ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ²Ø§ÙˆØ¬"""
        try:
            parent_gts = {}
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ù„Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†
            for parent in ['male', 'female']:
                gt_parts = []
                for gene in GENE_ORDER:
                    info = GENE_DATA[gene]
                    vis = parent_inputs[parent].get(f'{gene}_visible')
                    hid = parent_inputs[parent].get(f'{gene}_hidden', vis)
                    
                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ²
                    vis_sym = NAME_TO_SYMBOL_MAP[gene].get(vis, info['dominance'][0])
                    hid_sym = NAME_TO_SYMBOL_MAP[gene].get(hid, vis_sym)
                    
                    if info['type_en'] == 'sex-linked' and parent == 'female':
                        gt_parts.append(f"â€¢//{vis_sym}")
                    else:
                        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ù„ÙŠÙ„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù‡ÙŠÙ…Ù†Ø©
                        alleles = sorted([vis_sym, hid_sym], key=lambda x: info['dominance'].index(x))
                        gt_parts.append(f"{alleles[0]}//{alleles[1]}")
                
                parent_gts[parent] = gt_parts
            
            # Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø£Ù…Ø´Ø§Ø¬
            def get_gametes(gt_parts, is_female):
                parts_for_prod = []
                for i, part in enumerate(gt_parts):
                    gene = GENE_ORDER[i]
                    if GENE_DATA[gene]['type_en'] == 'sex-linked' and is_female:
                        parts_for_prod.append([part.replace('â€¢//','').strip()])
                    else:
                        parts_for_prod.append(part.split('//'))
                return list(product(*parts_for_prod))

            male_gametes = get_gametes(parent_gts['male'], False)
            female_gametes = get_gametes(parent_gts['female'], True)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ù„
            offspring = collections.Counter()
            for m_g in male_gametes:
                for f_g in female_gametes:
                    son_gt, daughter_gt = {}, {}
                    
                    for i, gene in enumerate(GENE_ORDER):
                        alleles = sorted([m_g[i], f_g[i]], key=lambda x: GENE_DATA[gene]['dominance'].index(x))
                        
                        if GENE_DATA[gene]['type_en'] == 'sex-linked':
                            son_gt[gene] = f"{alleles[0]}//{alleles[1]}"
                            daughter_gt[gene] = f"â€¢//{m_g[i]}"
                        else:
                            gt = f"{alleles[0]}//{alleles[1]}"
                            son_gt[gene] = gt
                            daughter_gt[gene] = gt
                    
                    offspring[self.describe_phenotype(son_gt)] += 1
                    offspring[self.describe_phenotype(daughter_gt)] += 1
            
            total = sum(offspring.values())
            
            return {
                'results': offspring,
                'total': total,
                'parent_genotypes': parent_gts,
                'success': True
            }
            
        except Exception as e:
            return {
                'error': f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨: {str(e)}",
                'success': False
            }

# -------------------------------------------------
#  3. Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù†
# -------------------------------------------------
class IntelligentResponder:
    def __init__(self, ai_manager, knowledge_manager):
        self.ai_manager = ai_manager
        self.knowledge_manager = knowledge_manager
        self.available_models = ai_manager.get_available_models()
        
        # Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©
        self.genetics_terms = {
            'Ø¬ÙŠÙ†': 'gene', 'Ø£Ù„ÙŠÙ„': 'allele', 'Ù†Ù…Ø· ÙˆØ±Ø§Ø«ÙŠ': 'genotype',
            'Ù†Ù…Ø· Ø¸Ø§Ù‡Ø±ÙŠ': 'phenotype', 'Ù‡ÙŠÙ…Ù†Ø©': 'dominance', 'ØªÙ†Ø­ÙŠ': 'recessive',
            'Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ø¬Ù†Ø³': 'sex-linked', 'Ø¬Ø³Ù…ÙŠ': 'autosomal'
        }

    def understand_intent(self, query: str) -> Dict:
        """ÙÙ‡Ù… Ù†ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        query_lower = query.lower()
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø­Ø³Ø§Ø¨
        calc_keywords = ['Ø§Ø­Ø³Ø¨', 'Ø­Ø³Ø§Ø¨', 'Ù†ØªØ§Ø¦Ø¬', 'ØªØ²Ø§ÙˆØ¬', 'ØªØ±Ø¨ÙŠØ©', 'Ù†Ø³Ù„']
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø£Ù„ÙˆØ§Ù†
        color_keywords = ['Ù„ÙˆÙ†', 'Ø£Ù„ÙˆØ§Ù†', 'Ø£Ø­Ù…Ø±', 'Ø£Ø²Ø±Ù‚', 'Ø¨Ù†ÙŠ', 'Ø¢Ø´ Ø±ÙŠØ¯']
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„ÙˆØ±Ø§Ø«Ø©
        genetics_keywords = ['ÙˆØ±Ø§Ø«Ø©', 'Ø¬ÙŠÙ†', 'Ø¬ÙŠÙ†Ø§Øª', 'Ø£Ù„ÙŠÙ„', 'Ù†Ù…Ø·']
        
        intent = {'type': 'general', 'confidence': 0.5, 'keywords': []}
        
        if any(word in query_lower for word in calc_keywords):
            intent = {'type': 'calculation', 'confidence': 0.9, 'keywords': calc_keywords}
        elif any(word in query_lower for word in color_keywords):
            intent = {'type': 'colors', 'confidence': 0.8, 'keywords': color_keywords}
        elif any(word in query_lower for word in genetics_keywords):
            intent = {'type': 'genetics', 'confidence': 0.8, 'keywords': genetics_keywords}
        
        return intent

    def generate_response(self, query: str) -> Dict:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø°ÙƒÙŠ"""
        try:
            # ÙÙ‡Ù… Ø§Ù„Ù†ÙŠØ©
            intent = self.understand_intent(query)
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„Ø­Ø³Ø§Ø¨ØŒ Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø§Ø³Ø¨Ø©
            if intent['type'] == 'calculation':
                return {
                    "answer": "ğŸ§® Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯! ØªÙØ¶Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø£Ø¯Ù†Ø§Ù‡ Ù„Ø­Ø³Ø§Ø¨ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ²Ø§ÙˆØ¬.",
                    "show_calculator": True,
                    "sources": [],
                    "intent": intent
                }
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
            context_docs = self.knowledge_manager.search_content(query, limit=3)
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
            for model_key in self.available_models:
                try:
                    answer = self._get_model_response(model_key, query, context_docs, intent)
                    
                    if answer and "Ø®Ø·Ø£" not in answer and len(answer.strip()) > 10:
                        return {
                            "answer": answer,
                            "show_calculator": False,
                            "sources": context_docs,
                            "model_used": model_key,
                            "intent": intent
                        }
                        
                except Exception as e:
                    st.warning(f"Ø®Ø·Ø£ ÙÙŠ Ù†Ù…ÙˆØ°Ø¬ {model_key}: {e}")
                    continue
            
            # Ø±Ø¯ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¥Ø°Ø§ ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            fallback_answer = self._generate_fallback_response(query, context_docs, intent)
            return {
                "answer": fallback_answer,
                "show_calculator": False,
                "sources": context_docs,
                "model_used": "fallback",
                "intent": intent
            }
            
        except Exception as e:
            return {
                "answer": f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ: {str(e)}",
                "show_calculator": False,
                "sources": [],
                "intent": {'type': 'error'}
            }

    def _get_model_response(self, model_key: str, query: str, context_docs: List[Dict], intent: Dict) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒÙŠ Ù…Ø­Ø¯Ø¯"""
        if model_key == "gemini":
            return self._get_gemini_response(query, context_docs, intent)
        elif model_key == "deepseek":
            return self._get_deepseek_response(query, context_docs, intent)
        elif model_key == "openai":
            return self._get_openai_response(query, context_docs, intent)
        else:
            return "Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…"

    def _build_context_prompt(self, query: str, context_docs: List[Dict], intent: Dict) -> str:
        """Ø¨Ù†Ø§Ø¡ prompt Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒÙŠØ©"""
        context_text = ""
        if context_docs:
            context_text = "\n\n".join([
                f"Ø§Ù„Ù…ØµØ¯Ø±: {doc['source']}\nØ§Ù„Ù…Ø­ØªÙˆÙ‰: {doc['full_content'][:800]}..."
                for doc in context_docs
            ])
        
        intent_instruction = ""
        if intent['type'] == 'colors':
            intent_instruction = "Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø´Ø±Ø­ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© ÙˆØ£Ù†Ù…Ø§Ø·Ù‡Ø§."
        elif intent['type'] == 'genetics':
            intent_instruction = "Ø§Ø´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…ÙØµÙ„ ÙˆÙ…Ø¨Ø³Ø·."
        
        prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… ØªØ¬ÙŠØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.

Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªØ§Ø­:
{context_text if context_text else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³ÙŠØ§Ù‚ Ù…Ø­Ø¯Ø¯"}

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ©: {intent_instruction}

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {query}

ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…ÙÙŠØ¯ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªÙˆÙØ±Ø©. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­."""

        return prompt

    def _get_gemini_response(self, query: str, context_docs: List[Dict], intent: Dict) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Gemini"""
        try:
            API_KEY = st.secrets["GEMINI_API_KEY"]
            API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}"
            
            prompt = self._build_context_prompt(query, context_docs, intent)
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "topK": 40,
                    "topP": 0.95,
                    "maxOutputTokens": 1024
                }
            }
            
            response = requests.post(API_URL, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if 'candidates' in result and result['candidates']:
                return result['candidates'][0]['content']['parts'][0]['text'].strip()
            else:
                return "Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Gemini"
                
        except requests.exceptions.Timeout:
            return "Ø®Ø·Ø£: Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ Gemini"
        except requests.exceptions.RequestException as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ Gemini: {e}"
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø¯ Gemini: {e}"

    def _get_deepseek_response(self, query: str, context_docs: List[Dict], intent: Dict) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† DeepSeek"""
        try:
            API_KEY = st.secrets["DEEPSEEK_API_KEY"]
            API_URL = "https://api.deepseek.com/v1/chat/completions"
            
            prompt = self._build_context_prompt(query, context_docs, intent)
            
            headers = {
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 1024
            }
            
            response = requests.post(API_URL, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and result['choices']:
                return result['choices'][0]['message']['content'].strip()
            else:
                return "Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø© DeepSeek"
                
        except requests.exceptions.Timeout:
            return "Ø®Ø·Ø£: Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ DeepSeek"
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ DeepSeek: {e}"

    def _get_openai_response(self, query: str, context_docs: List[Dict], intent: Dict) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† OpenAI"""
        try:
            API_KEY = st.secrets["OPENAI_API_KEY"]
            API_URL = "https://api.openai.com/v1/chat/completions"
            
            prompt = self._build_context_prompt(query, context_docs, intent)
            
            headers = {
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 1024
            }
            
            response = requests.post(API_URL, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and result['choices']:
                return result['choices'][0]['message']['content'].strip()
            else:
                return "Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø© OpenAI"
                
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ OpenAI: {e}"

    def _generate_fallback_response(self, query: str, context_docs: List[Dict], intent: Dict) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¨Ø¯ÙˆÙ† Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒÙŠØ©"""
        if context_docs:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ØªØ§Ø­ Ù„ØªÙƒÙˆÙŠÙ† Ø±Ø¯ Ø£Ø³Ø§Ø³ÙŠ
            relevant_content = context_docs[0]['content']
            return f"Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:\n\n{relevant_content}\n\nÙ„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø£Ùˆ Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ø§Ù‹."
        
        # Ø±Ø¯ÙˆØ¯ Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙŠØ©
        if intent['type'] == 'colors':
            return """ğŸ¨ Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø­Ù…Ø§Ù… Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©:

Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
â€¢ Ø§Ù„Ø£Ø²Ø±Ù‚/Ø§Ù„Ø£Ø³ÙˆØ¯ (+) - Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ
â€¢ Ø§Ù„Ø¨Ù†ÙŠ (b) - Ù…ØªÙ†Ø­ÙŠ Ù„Ù„Ø£Ø²Ø±Ù‚
â€¢ Ø§Ù„Ø¢Ø´ Ø±ÙŠØ¯ (BA) - Ù…Ù‡ÙŠÙ…Ù† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø²Ø±Ù‚ ÙˆØ§Ù„Ø¨Ù†ÙŠ

Ø§Ù„Ø£Ø­Ù…Ø± Ø§Ù„Ù…ØªÙ†Ø­ÙŠ (e):
â€¢ ÙŠØ­ÙˆÙ„ Ø£ÙŠ Ù„ÙˆÙ† Ø£Ø³Ø§Ø³ÙŠ Ø¥Ù„Ù‰ Ø£Ø­Ù…Ø± Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯Ù‡ ÙÙŠ Ø­Ø§Ù„Ø© Ù…ØªÙ†Ø­ÙŠØ© (e/e)

Ø§Ù„ØªØ®ÙÙŠÙ (d):
â€¢ ÙŠØ®ÙÙ ÙƒØ«Ø§ÙØ© Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
â€¢ Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ø¬Ù†Ø³ Ù…Ø«Ù„ Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

Ù„Ø­Ø³Ø§Ø¨ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ²Ø§ÙˆØ¬ Ø¨Ø¯Ù‚Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©."""
        
        elif intent['type'] == 'genetics':
            return """ğŸ§¬ Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„ÙˆØ±Ø§Ø«Ø© ÙÙŠ Ø§Ù„Ø­Ù…Ø§Ù…:

Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
â€¢ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ÙˆØ±Ø§Ø«ÙŠ (Genotype): Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ø¬ÙŠÙ†ÙŠ Ø§Ù„ÙØ¹Ù„ÙŠ
â€¢ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¸Ø§Ù‡Ø±ÙŠ (Phenotype): Ø§Ù„Ù…Ø¸Ù‡Ø± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ Ø§Ù„Ù…Ø±Ø¦ÙŠ
â€¢ Ø§Ù„Ù‡ÙŠÙ…Ù†Ø©: Ù‚Ø¯Ø±Ø© Ø£Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø¥Ø®ÙØ§Ø¡ ØªØ£Ø«ÙŠØ± Ø£Ù„ÙŠÙ„ Ø¢Ø®Ø±
â€¢ Ø§Ù„ØªÙ†Ø­ÙŠ: Ø§Ù„Ø£Ù„ÙŠÙ„ Ø§Ù„Ø°ÙŠ Ù„Ø§ ÙŠØ¸Ù‡Ø± Ø¥Ù„Ø§ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ØªÙ…Ø§Ø«Ù„

Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙˆØ±Ø§Ø«Ø©:
â€¢ Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ø¬Ù†Ø³: Ø§Ù„Ø¬ÙŠÙ†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ±ÙˆÙ…ÙˆØ³ÙˆÙ… Ø§Ù„Ø¬Ù†Ø³ÙŠ
â€¢ Ø¬Ø³Ù…ÙŠ: Ø§Ù„Ø¬ÙŠÙ†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ±ÙˆÙ…ÙˆØ³ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©

Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ù„ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø¹Ù…Ù„ÙŠØ§Ù‹."""
        
        else:
            return """Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª! 

ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ:
ğŸ§® Ø­Ø³Ø§Ø¨ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ²Ø§ÙˆØ¬ Ø§Ù„ÙˆØ±Ø§Ø«ÙŠ
ğŸ¨ Ø´Ø±Ø­ Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø­Ù…Ø§Ù… ÙˆØ£Ù†Ù…Ø§Ø·Ù‡Ø§  
ğŸ§¬ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©
ğŸ“š Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ±Ø¨ÙŠØ©

Ø­Ø§Ù„ÙŠØ§Ù‹ØŒ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒÙŠØ© ØºÙŠØ± Ù…ØªØ§Ø­Ø©ØŒ Ù„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø£Ùˆ Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ø§Ù‹."""

# -------------------------------------------------
#  4. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙˆØ¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
# -------------------------------------------------
@st.cache_resource
def load_resources():
    """ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…"""
    resources = {"embedder": None, "knowledge_manager": None}
    
    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
    if VECTOR_SEARCH_AVAILABLE:
        try:
            with st.spinner("ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ..."):
                resources["embedder"] = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
        except Exception as e:
            st.warning(f"ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: {e}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙØ©
    resources["knowledge_manager"] = KnowledgeManager(resources["embedder"])
    
    return resources

def load_sample_knowledge(knowledge_manager):
    """ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø±ÙØ© ØªØ¬Ø±ÙŠØ¨ÙŠØ©"""
    sample_content = [
        {
            "source": "Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©",
            "content": """Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„Ø­Ù…Ø§Ù…:

Ø§Ù„Ø£Ø²Ø±Ù‚/Ø§Ù„Ø£Ø³ÙˆØ¯ (+): Ù‡Ùˆ Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØŒ ÙŠØ¸Ù‡Ø± ÙƒØ£Ø²Ø±Ù‚ Ù…Ø¹ Ø£Ø´Ø±Ø·Ø© Ø³ÙˆØ¯Ø§Ø¡ Ø£Ùˆ ÙƒØ£Ø³ÙˆØ¯ ØªØ§Ù… Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…Ø·.

Ø§Ù„Ø¨Ù†ÙŠ (b): Ù„ÙˆÙ† Ù…ØªÙ†Ø­ÙŠ Ù„Ù„Ø£Ø²Ø±Ù‚ØŒ ÙŠØ­ÙˆÙ„ Ø§Ù„Ø£Ø²Ø±Ù‚ Ø¥Ù„Ù‰ Ø¨Ù†ÙŠ ÙˆØ§Ù„Ø£Ø³ÙˆØ¯ Ø¥Ù„Ù‰ Ø´ÙˆÙƒÙˆÙ„Ø§ØªÙŠ.

Ø§Ù„Ø¢Ø´ Ø±ÙŠØ¯ (BA): Ù„ÙˆÙ† Ù…Ù‡ÙŠÙ…Ù† Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø£Ø®Ø±Ù‰ØŒ ÙŠØ¹Ø·ÙŠ Ù„ÙˆÙ†Ø§Ù‹ Ø£Ø­Ù…Ø± Ù…Ø§Ø¦Ù„ Ù„Ù„Ø±Ù…Ø§Ø¯ÙŠ.

Ø§Ù„Ø£Ø­Ù…Ø± Ø§Ù„Ù…ØªÙ†Ø­ÙŠ (e): Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† ÙÙŠ Ø­Ø§Ù„Ø© Ù…ØªÙ†Ø­ÙŠØ© (e/e) ÙŠØ­ÙˆÙ„ Ø£ÙŠ Ù„ÙˆÙ† Ø¥Ù„Ù‰ Ø£Ø­Ù…Ø± ØµØ§ÙÙŠ.

Ø§Ù„ØªØ®ÙÙŠÙ (d): ÙŠØ®ÙÙ ÙƒØ«Ø§ÙØ© Ø§Ù„Ù„ÙˆÙ†ØŒ ÙÙŠØ­ÙˆÙ„ Ø§Ù„Ø£Ø²Ø±Ù‚ Ø¥Ù„Ù‰ ÙØ¶ÙŠ ÙˆØ§Ù„Ø£Ø³ÙˆØ¯ Ø¥Ù„Ù‰ Ø¯Ù† ÙˆØ§Ù„Ø£Ø­Ù…Ø± Ø¥Ù„Ù‰ Ø£ØµÙØ±."""
        },
        {
            "source": "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©", 
            "content": """Ù‚ÙˆØ§Ù†ÙŠÙ† Ù…Ù†Ø¯Ù„ ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…:

Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù‡ÙŠÙ…Ù†Ø©: Ø§Ù„Ø£Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‡ÙŠÙ…Ù† ÙŠØ®ÙÙŠ ØªØ£Ø«ÙŠØ± Ø§Ù„Ø£Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ†Ø­ÙŠ ÙÙŠ Ø§Ù„Ø£ÙØ±Ø§Ø¯ Ù…ØªØºØ§ÙŠØ±Ø© Ø§Ù„Ø£Ù‚Ø±Ø§Ù†.

Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙØµÙ„: Ø£Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø¬ÙŠÙ† Ø§Ù„ÙˆØ§Ø­Ø¯ ØªÙ†ÙØµÙ„ Ø£Ø«Ù†Ø§Ø¡ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø£Ù…Ø´Ø§Ø¬ØŒ ÙÙƒÙ„ Ù…Ø´ÙŠØ¬ ÙŠØ­Ù…Ù„ Ø£Ù„ÙŠÙ„ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·.

Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„: Ø¬ÙŠÙ†Ø§Øª Ù…Ø®ØªÙ„ÙØ© ØªÙˆØ±Ø« Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„ Ø¹Ù† Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶.

Ø§Ù„ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¬Ù†Ø³: ÙÙŠ Ø§Ù„Ø­Ù…Ø§Ù…ØŒ Ø§Ù„Ø¥Ù†Ø§Ø« Ù„Ø¯ÙŠÙ‡Ø§ ÙƒØ±ÙˆÙ…ÙˆØ³ÙˆÙ… Ø¬Ù†Ø³ÙŠ ÙˆØ§Ø­Ø¯ (ZW) Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ø°ÙƒÙˆØ± Ù„Ø¯ÙŠÙ‡Ù… Ø§Ø«Ù†Ø§Ù† (ZZ)ØŒ Ù„Ø°Ù„Ùƒ Ø§Ù„Ø¥Ù†Ø§Ø« ØªØ¸Ù‡Ø± ØµÙØ§Øª Ø§Ù„Ø£Ù„ÙŠÙ„ Ø§Ù„ÙˆØ§Ø­Ø¯ Ù…Ø¨Ø§Ø´Ø±Ø©."""
        },
        {
            "source": "Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±ÙŠØ´ ÙˆØ§Ù„Ø£Ø´Ø±Ø·Ø©",
            "content": """Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø±ÙŠØ´ ÙÙŠ Ø§Ù„Ø­Ù…Ø§Ù…:

Ø§Ù„Ø¨Ø§Ø± (+): Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØŒ ÙŠØ¸Ù‡Ø± Ø´Ø±ÙŠØ·ÙŠÙ† Ø£Ø³ÙˆØ¯ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù†Ø§Ø­.

Ø§Ù„ØªØ´ÙŠÙƒØ± (C): Ù†Ù…Ø· Ù…Ù‡ÙŠÙ…Ù† Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø§Ø±ØŒ ÙŠØ¸Ù‡Ø± Ù†Ù‚Ø· Ø£Ùˆ Ø±Ù‚Ø¹ ØµØºÙŠØ±Ø© Ù…Ù†ØªØ´Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙŠØ´.

Ù†Ù…Ø· Ø§Ù„ØªÙŠ (CT): Ø§Ù„Ø£ÙƒØ«Ø± Ù‡ÙŠÙ…Ù†Ø©ØŒ ÙŠØ¹Ø·ÙŠ Ù„ÙˆÙ†Ø§Ù‹ Ù…ÙˆØ­Ø¯Ø§Ù‹ Ø¨Ø¯ÙˆÙ† Ø£Ø´Ø±Ø·Ø© Ø£Ùˆ Ù†Ù‚Ø·.

Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø± (c): Ù†Ù…Ø· Ù…ØªÙ†Ø­ÙŠØŒ Ù„Ø§ ÙŠØ¸Ù‡Ø± Ø£ÙŠ Ø£Ø´Ø±Ø·Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù†Ø§Ø­.

Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± (S): ÙŠÙˆØ²Ø¹ Ù„ÙˆÙ† Ø§Ù„Ø´Ø±ÙŠØ· Ø£Ùˆ Ø§Ù„Ù†Ù‚Ø· Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ù„Ø±ÙŠØ´Ø©ØŒ Ù…Ù…Ø§ ÙŠØ¹Ø·ÙŠ Ù…Ø¸Ù‡Ø±Ø§Ù‹ Ø£ØºÙ…Ù‚ ÙˆØ£ÙƒØ«Ø± ÙƒØ«Ø§ÙØ©."""
        }
    ]
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ
    for item in sample_content:
        knowledge_manager.add_content(item["source"], item["content"])

# -------------------------------------------------
#  5. Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# -------------------------------------------------
def initialize_session_state():
    """ØªÙ‡ÙŠØ¦Ø© Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø©"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "knowledge_loaded" not in st.session_state:
        st.session_state.knowledge_loaded = False

def render_sources_section(sources: List[Dict]):
    """Ø¹Ø±Ø¶ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"""
    if not sources:
        return
    
    st.markdown("""
    <div class="sources-section">
        <strong>ğŸ” Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:</strong>
    """, unsafe_allow_html=True)
    
    for i, source in enumerate(sources):
        st.markdown(f"""
        <div class="source-item">
            <strong>{source['source']}</strong><br>
            <small>{source['content'][:150]}{'...' if len(source['content']) > 150 else ''}</small>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("</div>", unsafe_allow_html=True)

def render_embedded_calculator():
    """Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©"""
    with st.container():
        st.markdown('<div class="genetics-calculator">', unsafe_allow_html=True)
        st.markdown('<div class="calc-header">ğŸ§® Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©</div>', unsafe_allow_html=True)
        
        parent_inputs = {'male': {}, 'female': {}}
        col1, col2 = st.columns(2)
        
        # Ø¥Ø¯Ø®Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†
        for parent, col in [('male', col1), ('female', col2)]:
            with col:
                st.markdown(f"#### {'â™‚ï¸ Ø§Ù„Ø°ÙƒØ±' if parent == 'male' else 'â™€ï¸ Ø§Ù„Ø£Ù†Ø«Ù‰'}")
                
                for gene, data in GENE_DATA.items():
                    choices = list(data['alleles'].values())
                    
                    # Ø§Ù„Ø£Ù„ÙŠÙ„ Ø§Ù„Ø¸Ø§Ù‡Ø±
                    parent_inputs[parent][f'{gene}_visible'] = st.selectbox(
                        f"{data['emoji']} {data['display_name_ar']} (Ø§Ù„Ø¸Ø§Ù‡Ø±):",
                        choices,
                        key=f"emb_{parent}_{gene}_vis"
                    )
                    
                    # Ø§Ù„Ø£Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®ÙÙŠ (Ù„Ù„Ø°ÙƒÙˆØ± ÙˆÙ„Ù„Ø¬ÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø³Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ø¥Ù†Ø§Ø«)
                    if not (data['type_en'] == 'sex-linked' and parent == 'female'):
                        parent_inputs[parent][f'{gene}_hidden'] = st.selectbox(
                            f"{data['emoji']} {data['display_name_ar']} (Ø§Ù„Ù…Ø®ÙÙŠ):",
                            choices,
                            key=f"emb_{parent}_{gene}_hid",
                            index=choices.index(parent_inputs[parent][f'{gene}_visible'])
                        )
                    else:
                        parent_inputs[parent][f'{gene}_hidden'] = parent_inputs[parent][f'{gene}_visible']
        
        # Ø²Ø± Ø§Ù„Ø­Ø³Ø§Ø¨
        if st.button("ğŸš€ Ø§Ø­Ø³Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", use_container_width=True, type="primary"):
            calculator = AdvancedGeneticCalculator()
            result_data = calculator.calculate(parent_inputs)
            
            if not result_data.get('success', False):
                st.error(result_data.get('error', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'))
            else:
                # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø¬Ø¯ÙˆÙ„
                results_list = []
                for (phenotype, genotype), count in result_data['results'].items():
                    percentage = (count / result_data['total']) * 100
                    results_list.append({
                        'Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¸Ø§Ù‡Ø±ÙŠ': phenotype,
                        'Ø§Ù„Ù†Ù…Ø· Ø§Ù„ÙˆØ±Ø§Ø«ÙŠ': genotype,
                        'Ø§Ù„Ø¹Ø¯Ø¯': count,
                        'Ø§Ù„Ù†Ø³Ø¨Ø© %': f"{percentage:.1f}%"
                    })
                
                df = pd.DataFrame(results_list)
                st.dataframe(df, use_container_width=True, hide_index=True)
                
                # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©
                with st.expander("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©"):
                    st.write(f"**Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†Ø³Ù„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:** {result_data['total']}")
                    st.write(f"**Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø®ØªÙ„ÙØ©:** {len(result_data['results'])}")
                    
                    # Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ù„Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†
                    st.write("**Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ù„Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†:**")
                    for parent, genotype in result_data['parent_genotypes'].items():
                        parent_name = "Ø§Ù„Ø°ÙƒØ±" if parent == 'male' else "Ø§Ù„Ø£Ù†Ø«Ù‰"
                        st.write(f"- {parent_name}: {' | '.join(genotype)}")
        
        st.markdown('</div>', unsafe_allow_html=True)

def handle_user_message(prompt: str, responder: IntelligentResponder):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ±..."):
        response_data = responder.generate_response(prompt)
    
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯
    st.session_state.messages.append({
        "role": "assistant",
        "content": response_data["answer"],
        "show_calculator": response_data.get("show_calculator", False),
        "sources": response_data.get("sources", []),
        "model_used": response_data.get("model_used"),
        "intent": response_data.get("intent", {})
    })

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    initialize_session_state()
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
    resources = load_resources()
    knowledge_manager = resources["knowledge_manager"]
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø­Ù…Ù„Ø©
    if not st.session_state.knowledge_loaded:
        with st.spinner("ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©..."):
            load_sample_knowledge(knowledge_manager)
            st.session_state.knowledge_loaded = True
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø§Ø¡
    ai_manager = AIModelManager()
    responder = IntelligentResponder(ai_manager, knowledge_manager)

    # Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    
    # Ø´Ø±ÙŠØ· Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    available_models = ai_manager.get_available_models()
    model_status = f"Ù…ØªØ§Ø­ ({len(available_models)} Ù†Ù…Ø§Ø°Ø¬)" if available_models else "ØºÙŠØ± Ù…ØªØ§Ø­"
    
    st.markdown(f'''
    <div class="header-bar">
        <div class="header-title">
            ğŸ§¬ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V14.0
            <small style="font-size: 14px; opacity: 0.8;">Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡: {model_status}</small>
        </div>
        <div style="font-size: 14px; display: flex; align-items: center; gap: 8px;">
            <div class="status-indicator"></div>
            Ù†Ø´Ø· Ø§Ù„Ø¢Ù†
        </div>
    </div>
    ''', unsafe_allow_html=True)

    # Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    chat_area = st.container()
    with chat_area:
        st.markdown('<div class="chat-area">', unsafe_allow_html=True)
        
        # Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„Ù‰
        if not st.session_state.messages:
            welcome_msg = """Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V14.0! ğŸ§¬

Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªØ®ØµØµ ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…. ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ:

ğŸ§® **Ø­Ø³Ø§Ø¨ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ²Ø§ÙˆØ¬** - Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
ğŸ¨ **Ø´Ø±Ø­ Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·** - Ù…Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø¥Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©  
ğŸ§¬ **ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©** - Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©
ğŸ“š **Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ±Ø¨ÙŠØ©** - Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª

ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"""
            
            st.session_state.messages.append({"role": "assistant", "content": welcome_msg})
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                st.markdown(f'''
                <div class="message user-message">
                    <div class="user-bubble">{msg["content"]}</div>
                </div>
                ''', unsafe_allow_html=True)
            else:
                st.markdown(f'''
                <div class="message assistant-message">
                    <div class="avatar">ğŸ¤–</div>
                    <div class="assistant-bubble">{msg["content"]}</div>
                </div>
                ''', unsafe_allow_html=True)
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¥Ù† ÙˆØ¬Ø¯Øª
                if msg.get("sources"):
                    render_sources_section(msg["sources"])
                
                # Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø§Ø³Ø¨Ø© Ø¥Ù† Ø·ÙÙ„Ø¨Øª
                if msg.get("show_calculator"):
                    render_embedded_calculator()
        
        st.markdown('</div>', unsafe_allow_html=True)

    # Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
    st.markdown('<div class="input-area">', unsafe_allow_html=True)
    
    # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹
    quick_actions = ["ğŸ§® Ø­Ø³Ø§Ø¨ ÙˆØ±Ø§Ø«ÙŠ", "ğŸ¨ Ø´Ø±Ø­ Ø§Ù„Ø£Ù„ÙˆØ§Ù†", "ğŸ§¬ Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ÙˆØ±Ø§Ø«Ø©", "ğŸ’¡ Ù†ØµØ§Ø¦Ø­ ØªØ±Ø¨ÙŠØ©"]
    cols = st.columns(len(quick_actions))
    
    for i, action in enumerate(quick_actions):
        if cols[i].button(action, use_container_width=True):
            handle_user_message(action, responder)
            st.rerun()

    # Ø­Ù‚Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§... ğŸ’¬"):
        handle_user_message(prompt, responder)
        st.rerun()
    
    st.markdown('</div>', unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    with st.sidebar:
        st.markdown("### ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        stats = knowledge_manager.get_knowledge_stats()
        st.metric("Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙØ©", stats['total_documents'])
        st.metric("Ø§Ù„Ù…ØµØ§Ø¯Ø±", stats['total_sources'])
        
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        st.markdown("### ğŸ¤– Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
        for model_key, config in ai_manager.models.items():
            status = "âœ… Ù…ØªØ§Ø­" if config['available'] else "âŒ ØºÙŠØ± Ù…ØªØ§Ø­"
            st.write(f"**{config['name']}:** {status}")
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
        with st.expander("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"):
            if st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø±ÙØ©"):
                st.session_state.knowledge_loaded = False
                st.rerun()
            
            if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
                st.session_state.messages = []
                st.rerun()

if __name__ == "__main__":
    main()
