# ===================================================================
# ğŸš€ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V2.0 - ÙˆÙƒÙŠÙ„ Ø¨Ø­Ø«ÙŠ Ø°ÙƒÙŠ Ù…Ø­Ø³Ù‘Ù†
# Ø¨Ù…ÙŠØ²Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©: ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§ØªØŒ Ø°Ø§ÙƒØ±Ø© Ù…Ø­Ø§Ø¯Ø«Ø©ØŒ ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
# ===================================================================

import streamlit as st
from itertools import product
import collections
import pandas as pd
import google.generativeai as genai
import faiss
import numpy as np
import pickle
import os
import json
from datetime import datetime
import plotly.express as px
from typing import List, Dict

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© ---
st.set_page_config(
    layout="wide", 
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V2.0",
    page_icon="ğŸ§¬",
    initial_sidebar_state="expanded"
)

# --- 2. ÙØ¦Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ© ---
QUESTION_CATEGORIES = {
    "basic": ["Ù…Ø§ Ù‡Ùˆ", "Ù…Ø§ Ù‡ÙŠ", "ØªØ¹Ø±ÙŠÙ", "Ù…Ø¹Ù†Ù‰"],
    "genetic_inheritance": ["ÙˆØ±Ø§Ø«Ø©", "Ø¬ÙŠÙ†", "ÙƒØ±ÙˆÙ…ÙˆØ³ÙˆÙ…", "Ø¯ÙŠ Ø§Ù† Ø§ÙŠÙ‡", "DNA"],
    "breeding": ["ØªØ±Ø¨ÙŠØ©", "ØªØ²Ø§ÙˆØ¬", "Ø§Ù†ØªØ§Ø¬", "ØªØ­Ø³ÙŠÙ†"],
    "colors": ["Ù„ÙˆÙ†", "Ø£Ù„ÙˆØ§Ù†", "ØªÙ„ÙˆÙŠÙ†", "ØµØ¨ØºØ©"],
    "diseases": ["Ù…Ø±Ø¶", "Ø£Ù…Ø±Ø§Ø¶", "Ø¹Ù„Ø§Ø¬", "ØµØ­Ø©"],
    "analysis": ["Ø­Ù„Ù„", "Ø§Ø´Ø±Ø­", "ÙØ³Ø±", "Ù‚Ø§Ø±Ù†", "Ø§Ø±Ø¨Ø·"]
}

# --- 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø£ÙØ¶Ù„ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡ ---
@st.cache_resource
def load_resources():
    """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª ÙˆÙ†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙØ¶Ù„ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡."""
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        st.error("âŒ Ù…ÙƒØªØ¨Ø© sentence-transformers ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
        return None, None, None

    vector_db_path = "vector_db.pkl"
    metadata_path = "vector_metadata.json"
    embedding_model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
    vector_db = None
    metadata = {}
    
    if os.path.exists(vector_db_path):
        try:
            with open(vector_db_path, "rb") as f:
                vector_db = pickle.load(f)
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª: {e}")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
        except Exception as e:
            st.warning(f"Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©: {e}")
    
    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†
    try:
        embedder = SentenceTransformer(embedding_model_name)
    except Exception as e:
        st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {e}")
        return None, None, None
    
    return vector_db, embedder, metadata

# --- 4. Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Gemini Ø§Ù„Ù…Ø­Ø³Ù‘Ù† ---
@st.cache_resource
def initialize_gemini():
    """ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Gemini Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø©."""
    if "GEMINI_API_KEY" not in st.secrets:
        return None
    
    try:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        model = genai.GenerativeModel(
            'gemini-1.5-flash',
            generation_config={
                "temperature": 0.1,  # Ø£Ù‚Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©
                "max_output_tokens": 3000,
                "top_p": 0.8,
                "top_k": 40
            }
        )
        return model
    except Exception as e:
        st.error(f"ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Gemini: {e}")
        return None

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
vector_db, embedder, metadata = load_resources()
model = initialize_gemini()

# --- 5. ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ---
def classify_question(query: str) -> str:
    """ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¨Ø­Ø«."""
    query_lower = query.lower()
    
    for category, keywords in QUESTION_CATEGORIES.items():
        if any(keyword in query_lower for keyword in keywords):
            return category
    
    return "general"

def search_knowledge_advanced(query: str, category: str = "general", top_k: int = 5) -> List[Dict]:
    """Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªØµÙ†ÙŠÙ Ø°ÙƒÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬."""
    if not vector_db or not embedder:
        return []
    
    index = vector_db["index"]
    chunks = vector_db["chunks"]
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©
    enhanced_query = enhance_query_by_category(query, category)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡
    query_embedding = embedder.encode([enhanced_query])
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ FAISS Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ÙÙ„ØªØ±Ø©
    search_k = min(top_k * 2, len(chunks))
    distances, indices = index.search(np.array(query_embedding, dtype=np.float32), search_k)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØµÙ„Ø©
    results = []
    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
        if idx < len(chunks):
            results.append({
                "content": chunks[idx],
                "relevance_score": 1 / (1 + dist),  # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ Ø¯Ø±Ø¬Ø© ØµÙ„Ø©
                "rank": i + 1,
                "source_info": metadata.get(str(idx), {})
            })
    
    # ÙÙ„ØªØ±Ø© ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    results = filter_and_rank_results(results, query, category)
    
    return results[:top_k]

def enhance_query_by_category(query: str, category: str) -> str:
    """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙØ¦Ø© Ø§Ù„Ø³Ø¤Ø§Ù„."""
    enhancements = {
        "genetic_inheritance": f"{query} ÙˆØ±Ø§Ø«Ø© Ø¬ÙŠÙ†Ø§Øª",
        "breeding": f"{query} ØªØ±Ø¨ÙŠØ© ØªØ²Ø§ÙˆØ¬",
        "colors": f"{query} Ø£Ù„ÙˆØ§Ù† ÙˆØ±Ø§Ø«Ø©",
        "diseases": f"{query} Ø£Ù…Ø±Ø§Ø¶ Ø¹Ù„Ø§Ø¬",
        "analysis": f"{query} ØªØ­Ù„ÙŠÙ„ Ø´Ø±Ø­"
    }
    
    return enhancements.get(category, query)

def filter_and_rank_results(results: List[Dict], query: str, category: str) -> List[Dict]:
    """ÙÙ„ØªØ±Ø© ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ø§ÙŠÙŠØ± Ù…ØªÙ‚Ø¯Ù…Ø©."""
    # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    query_words = set(query.lower().split())
    
    for result in results:
        content_words = set(result["content"].lower().split())
        word_overlap = len(query_words.intersection(content_words))
        result["keyword_score"] = word_overlap / len(query_words) if query_words else 0
        
        # Ø¯Ø±Ø¬Ø© Ù…Ø±ÙƒØ¨Ø©
        result["final_score"] = (
            result["relevance_score"] * 0.7 + 
            result["keyword_score"] * 0.3
        )
    
    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ¨Ø©
    return sorted(results, key=lambda x: x["final_score"], reverse=True)

# --- 6. ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ---
def advanced_research_agent(query: str, search_depth: int) -> Dict:
    """ÙˆÙƒÙŠÙ„ Ø¨Ø­Ø«ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„."""
    if not model:
        return {
            "answer": "âŒ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ù…Ù‡ÙŠØ£ (API KEY Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ ØºÙŠØ± ØµØ§Ù„Ø­).",
            "confidence": 0,
            "sources": [],
            "category": "error"
        }

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­ÙŠØ§Øª Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
    q_lower = query.lower().strip()
    if any(word in q_lower for word in ["Ø³Ù„Ø§Ù…", "Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù‡Ù„Ø§", "Ù‡Ø§ÙŠ", "Ø´ÙƒØ±Ø§"]):
        return {
            "answer": "ğŸ¤— ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… ÙˆÙ…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ø§Ù„Ø¨Ø­Ø«ÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†ØŒ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø£Ø³Ø¦Ù„ØªÙƒ Ø­ÙˆÙ„ Ø¹Ù„Ù… ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¹Ù„Ù…ÙŠ Ù…ØªÙ‚Ø¯Ù….",
            "confidence": 1.0,
            "sources": [],
            "category": "greeting"
        }

    # ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„
    category = classify_question(query)
    
    # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    with st.spinner("ğŸ”¬ Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©..."):
        search_results = search_knowledge_advanced(query, category, top_k=search_depth)
    
    if not search_results:
        return {
            "answer": "ğŸ¤” Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©. Ø¬Ø±Ø¨ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØµØ·Ù„Ø­Ø§Øª Ø£Ø®Ø±Ù‰.",
            "confidence": 0,
            "sources": [],
            "category": category
        }

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØµÙ„Ø©
    context_parts = []
    source_info = []
    
    for i, result in enumerate(search_results):
        context_parts.append(f"[Ù…Ø±Ø¬Ø¹ {i+1} - Ø¯Ø±Ø¬Ø© Ø§Ù„ØµÙ„Ø©: {result['final_score']:.2f}]\n{result['content']}")
        source_info.append({
            "rank": i+1,
            "score": result['final_score'],
            "content_preview": result['content'][:100] + "..."
        })
    
    context_text = "\n\n" + "="*50 + "\n\n".join(context_parts)

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    prompt = create_advanced_prompt(query, context_text, category)
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    with st.spinner("ğŸ§  Ø¬Ø§Ø±Ù ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØµÙŠØ§ØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªØ®ØµØµØ©..."):
        try:
            response = model.generate_content(prompt)
            answer = response.text
            
            # ØªÙ‚Ø¯ÙŠØ± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©
            confidence = estimate_confidence(search_results, answer)
            
            return {
                "answer": answer,
                "confidence": confidence,
                "sources": source_info,
                "category": category,
                "search_results": search_results
            }
            
        except Exception as e:
            return {
                "answer": f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {str(e)}",
                "confidence": 0,
                "sources": source_info,
                "category": category
            }

def create_advanced_prompt(query: str, context: str, category: str) -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…ØªÙ‚Ø¯Ù… Ø­Ø³Ø¨ ÙØ¦Ø© Ø§Ù„Ø³Ø¤Ø§Ù„."""
    
    base_prompt = f"""
Ø£Ù†Øª "Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ø§Ù„Ø°ÙƒÙŠ V2.0"ØŒ Ø®Ø¨ÙŠØ± Ø¹Ø§Ù„Ù…ÙŠ ÙÙŠ Ø¹Ù„Ù… ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… ÙˆØªØ±Ø¨ÙŠØªÙ‡ØŒ Ù…Ø²ÙˆØ¯ Ø¨Ù‚Ø¯Ø±Ø§Øª ØªØ­Ù„ÙŠÙ„ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©.

**ÙØ¦Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©:** {category}
**Ø§Ù„Ø³Ø¤Ø§Ù„:** {query}

**Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ù…Ø±ØªØ¨Ø© Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø©:**
{context}

**Ù…Ù‡Ø§Ù…Ùƒ Ø§Ù„Ù…ØªØ®ØµØµØ©:**

1. **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚:** Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù…ÙŠØ© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø§Ù„Ø£Ø¯Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
2. **Ø§Ù„ØªØµÙ†ÙŠÙ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ…:** Ø±ØªØ¨ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ Ù…Ø¹ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ø¶Ø­Ø©
3. **Ø§Ù„Ø±Ø¨Ø· ÙˆØ§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬:** Ø§Ø±Ø¨Ø· Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆÙ‚Ø¯Ù… Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ù…Ù†Ø·Ù‚ÙŠØ©
4. **Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ÙŠ:** Ø§Ø°ÙƒØ± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù…ÙƒØ§Ù†

**Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©:**
âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø­ØµØ±ÙŠØ§Ù‹
âœ… Ø£Ø³Ù„ÙˆØ¨ Ø¹Ù„Ù…ÙŠ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø¸Ù…
âœ… ØªØ¯Ø±Ø¬ Ù…Ù† Ø§Ù„Ø¹Ø§Ù… Ø¥Ù„Ù‰ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ
âœ… Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
âœ… Ø§Ø°ÙƒØ± Ø£ÙŠ Ù‚ÙŠÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

**ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† (##) ÙˆØ§Ù„Ù†Ù‚Ø§Ø· ÙˆØ§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‚Ø§Ø¨Ù„ÙŠØ© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©.
"""

    # ØªØ®ØµÙŠØµØ§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©
    category_specific = {
        "genetic_inheritance": "\n**ØªØ±ÙƒÙŠØ² Ø®Ø§Øµ:** Ø§Ø´Ø±Ø­ Ø¢Ù„ÙŠØ§Øª Ø§Ù„ÙˆØ±Ø§Ø«Ø© ÙˆØ§Ù„Ø¬ÙŠÙ†Ø§Øª Ø¨Ø§Ù„ØªÙØµÙŠÙ„ Ù…Ø¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø©.",
        "breeding": "\n**ØªØ±ÙƒÙŠØ² Ø®Ø§Øµ:** Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù„ØªØ±Ø¨ÙŠØ© ÙˆØ§Ù„ØªØ²Ø§ÙˆØ¬ ÙˆØ§Ù„Ø§Ù†ØªÙ‚Ø§Ø¡.",
        "colors": "\n**ØªØ±ÙƒÙŠØ² Ø®Ø§Øµ:** ÙØµÙ„ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø·ÙØ±Ø§Øª Ø§Ù„Ù„ÙˆÙ†ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…ÙØµÙ„.",
        "diseases": "\n**ØªØ±ÙƒÙŠØ² Ø®Ø§Øµ:** Ø§Ø´Ø±Ø­ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ ÙˆØ£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ÙˆÙ‚Ø§ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ø¬.",
        "analysis": "\n**ØªØ±ÙƒÙŠØ² Ø®Ø§Øµ:** Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹ ÙˆÙ…Ù‚Ø§Ø±Ù†Ø§Øª ÙˆØªÙØ³ÙŠØ±Ø§Øª Ø´Ø§Ù…Ù„Ø©."
    }
    
    return base_prompt + category_specific.get(category, "")

def estimate_confidence(search_results: List[Dict], answer: str) -> float:
    """ØªÙ‚Ø¯ÙŠØ± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©."""
    if not search_results:
        return 0.0
    
    # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø«Ù‚Ø©
    avg_relevance = np.mean([r['final_score'] for r in search_results])
    num_sources = len(search_results)
    answer_length = len(answer.split())
    
    # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©
    confidence = (
        avg_relevance * 0.5 +
        min(num_sources / 5, 1.0) * 0.3 +
        min(answer_length / 200, 1.0) * 0.2
    )
    
    return min(confidence, 1.0)

# --- 7. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© ---
def main():
    # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    st.markdown("""
    # ğŸš€ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V2.0
    ## ÙˆÙƒÙŠÙ„ Ø¨Ø­Ø«ÙŠ Ø°ÙƒÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¹Ù„Ù… ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…
    ---
    """)

    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
    settings = create_enhanced_sidebar()
    
    # Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    create_chat_interface(settings)
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØªØ­Ù„ÙŠÙ„Ø§Øª
    if st.checkbox("ğŸ“Š Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ù„Ø³Ø©"):
        show_session_statistics()

def create_enhanced_sidebar():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø´Ø±ÙŠØ· Ø¬Ø§Ù†Ø¨ÙŠ Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ©."""
    st.sidebar.markdown("## ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
    
    # Ø­Ø§Ù„Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
    if vector_db and embedder:
        st.sidebar.success("âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¬Ø§Ù‡Ø²Ø©")
        
        col1, col2 = st.sidebar.columns(2)
        with col1:
            st.metric("ğŸ§  Ù…Ù‚Ø§Ø·Ø¹ Ù…Ø¹Ø±ÙÙŠØ©", len(vector_db['chunks']))
        with col2:
            st.metric("ğŸ“ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©", len(metadata) if metadata else 0)
            
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        if metadata:
            st.sidebar.info(f"ğŸ“… Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: {metadata.get('last_updated', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
            
    else:
        st.sidebar.error("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
    
    # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    if model:
        st.sidebar.success("âœ… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø¬Ø§Ù‡Ø²")
        st.sidebar.info("ğŸ¤– Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: Gemini 1.5 Flash")
    else:
        st.sidebar.error("âŒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ ØºÙŠØ± Ù…ØªØ§Ø­")
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø«
    st.sidebar.markdown("## âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø«")
    search_depth = st.sidebar.slider("Ø¹Ù…Ù‚ Ø§Ù„Ø¨Ø­Ø«", 3, 10, 5)
    
    return {"search_depth": search_depth}

def create_chat_interface(settings: Dict):
    """Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©."""
    st.markdown("### ğŸ’¬ Ù…Ø­Ø§Ø¯Ø«Ø© Ø°ÙƒÙŠØ© Ù…Ø¹ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨")
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ù„Ø³Ø©
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "session_stats" not in st.session_state:
        st.session_state.session_stats = {
            "questions_asked": 0,
            "categories_used": set(),
            "avg_confidence": 0.0,
            "start_time": datetime.now()
        }

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant" and "metadata" in message:
                display_enhanced_response(message["content"], message["metadata"])
            else:
                st.markdown(message["content"])

    # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    if prompt := st.chat_input("ğŸ§¬ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø£ÙŠ Ø´ÙŠØ¡ Ø¹Ù† ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…..."):
        # Ø¥Ø¶Ø§ÙØ© Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        with st.chat_message("assistant"):
            response_data = advanced_research_agent(prompt, settings["search_depth"])
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
            display_enhanced_response(response_data["answer"], response_data)
            
            # Ø­ÙØ¸ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
            st.session_state.messages.append({
                "role": "assistant", 
                "content": response_data["answer"],
                "metadata": response_data
            })
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            update_session_stats(response_data)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        st.rerun()

def display_enhanced_response(answer: str, metadata: dict):
    """Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©."""
    # Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    st.markdown(answer)
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    if metadata.get("confidence", 0) > 0:
        # Ø´Ø±ÙŠØ· Ø§Ù„Ø«Ù‚Ø©
        confidence = metadata["confidence"]
        confidence_color = "green" if confidence > 0.7 else "orange" if confidence > 0.4 else "red"
        
        st.markdown(f"""
        <div style="margin: 10px 0;">
            <strong>ğŸ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©:</strong>
            <div style="background-color: #f0f0f0; border-radius: 10px; padding: 5px;">
                <div style="background-color: {confidence_color}; width: {confidence*100}%; height: 20px; border-radius: 10px; text-align: center; color: white; line-height: 20px;">
                    {confidence:.1%}
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØµØ§Ø¯Ø±
        if metadata.get("sources") and len(metadata["sources"]) > 0:
            with st.expander(f"ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ({len(metadata['sources'])})"):
                for i, source in enumerate(metadata["sources"]):
                    st.markdown(f"""
                    **Ù…ØµØ¯Ø± {source['rank']}** (Ø¯Ø±Ø¬Ø© Ø§Ù„ØµÙ„Ø©: {source['score']:.2f})
                    {source['content_preview']}
                    """)

def update_session_stats(response_data: dict):
    """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ù„Ø³Ø©."""
    stats = st.session_state.session_stats
    stats["questions_asked"] += 1
    stats["categories_used"].add(response_data.get("category", "unknown"))
    
    if response_data.get("confidence", 0) > 0:
        # ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©
        current_avg = stats["avg_confidence"]
        new_avg = (current_avg * (stats["questions_asked"] - 1) + response_data["confidence"]) / stats["questions_asked"]
        stats["avg_confidence"] = new_avg

def show_session_statistics():
    """Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ù„Ø³Ø©."""
    stats = st.session_state.session_stats
    
    st.markdown("### ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ¤” Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø·Ø±ÙˆØ­Ø©", stats["questions_asked"])
    
    with col2:
        st.metric("ğŸ¯ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©", f"{stats['avg_confidence']:.1%}")
    
    with col3:
        st.metric("ğŸ“‚ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©", len(stats["categories_used"]))
    
    with col4:
        duration = datetime.now() - stats["start_time"]
        st.metric("â±ï¸ Ù…Ø¯Ø© Ø§Ù„Ø¬Ù„Ø³Ø©", f"{duration.seconds//60} Ø¯Ù‚ÙŠÙ‚Ø©")
    
    # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„ÙØ¦Ø§Øª
    if len(stats["categories_used"]) > 0:
        categories_list = list(stats["categories_used"])
        fig = px.pie(
            values=[1] * len(categories_list), 
            names=categories_list,
            title="ØªÙˆØ²ÙŠØ¹ ÙØ¦Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"
        )
        st.plotly_chart(fig, use_container_width=True)

# --- 8. ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ---
if __name__ == "__main__":
    main()
