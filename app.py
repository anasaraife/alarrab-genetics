# ==============================================================================
#  Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0 Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Ù…Ø¹ AI Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬)
# ==============================================================================

import streamlit as st
import sqlite3
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os
import tempfile
import hashlib
import requests
import json
import numpy as np
from typing import List, Dict, Optional
import time
from datetime import datetime

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±
# -------------------------------------------------
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0 Ø§Ù„Ù…ØªÙ‚Ø¯Ù…",
    page_icon="ğŸ§¬",
    layout="wide",
)

# Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ¨
BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
]

# -------------------------------------------------
#  2. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
# -------------------------------------------------

class AIModelManager:
    """Ù…Ø¯ÙŠØ± Ù„Ø¥Ø¯Ø§Ø±Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©"""
    
    def __init__(self):
        self.models = {
            "gemini": {
                "name": "Google Gemini",
                "available": self._check_gemini_availability(),
                "priority": 1
            },
            "deepseek": {
                "name": "DeepSeek",
                "available": self._check_deepseek_availability(),
                "priority": 2
            },
            "huggingface": {
                "name": "Hugging Face",
                "available": True,
                "priority": 3
            },
            "ollama": {
                "name": "Ollama Local",
                "available": self._check_ollama_availability(),
                "priority": 4
            }
        }
    
    def _check_gemini_availability(self) -> bool:
        """ÙØ­Øµ ØªÙˆÙØ± Ù…ÙØªØ§Ø­ Gemini API"""
        try:
            return "GEMINI_API_KEY" in st.secrets
        except:
            return False
    
    def _check_deepseek_availability(self) -> bool:
        """ÙØ­Øµ ØªÙˆÙØ± Ù…ÙØªØ§Ø­ DeepSeek API"""
        try:
            return "DEEPSEEK_API_KEY" in st.secrets
        except:
            return False
    
    def _check_ollama_availability(self) -> bool:
        """ÙØ­Øµ ØªÙˆÙØ± Ø®Ø¯Ù…Ø© Ollama Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ø±ØªØ¨Ø© Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©"""
        available = [model for model, config in self.models.items() if config["available"]]
        return sorted(available, key=lambda x: self.models[x]["priority"])

# -------------------------------------------------
#  3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
# -------------------------------------------------

@st.cache_resource
def load_embedding_model():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª."""
    return SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

@st.cache_resource
def initialize_ai_manager():
    """ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    return AIModelManager()

@st.cache_data(ttl=86400)
def load_knowledge_base(_model):
    """
    Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© (Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù…ØªØ¬Ù‡Ø§Øª) Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±.
    """
    db_path = os.path.join(tempfile.gettempdir(), "text_knowledge_v11.db")
    conn = sqlite3.connect(db_path, check_same_thread=False)
    cursor = conn.cursor()
    cursor.execute("CREATE TABLE IF NOT EXISTS knowledge (source TEXT, content TEXT UNIQUE)")
    
    cursor.execute("SELECT COUNT(*) FROM knowledge")
    if cursor.fetchone()[0] == 0:
        with st.spinner("ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ©..."):
            for i, link in enumerate(BOOK_LINKS):
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                        file_id = link.split('/d/')[1].split('/')[0]
                        gdown.download(id=file_id, output=tmp.name, quiet=True)
                        with open(tmp.name, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page_num, page in enumerate(reader.pages):
                                text = page.extract_text() or ""
                                if len(text.strip()) > 150:
                                    cursor.execute("INSERT OR IGNORE INTO knowledge (source, content) VALUES (?, ?)",
                                                   (f"Book {i+1}, Page {page_num+1}", text.strip()))
                        os.remove(tmp.name)
                except Exception as e:
                    print(f"Could not process book {i+1}: {e}")
            conn.commit()

    cursor.execute("SELECT source, content FROM knowledge")
    all_docs = [{"source": row[0], "content": row[1]} for row in cursor.fetchall()]
    conn.close()

    if not all_docs:
        return None

    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ ÙˆÙÙ‡Ø±Ø³Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©..."):
        contents = [doc['content'] for doc in all_docs]
        embeddings = _model.encode(contents, show_progress_bar=True)
    
    return {"documents": all_docs, "embeddings": embeddings}

# -------------------------------------------------
#  4. Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# -------------------------------------------------

def search_semantic_knowledge(query, model, knowledge_base, limit=3):
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ."""
    if not knowledge_base:
        return []
    
    query_embedding = model.encode([query])
    similarities = cosine_similarity(query_embedding, knowledge_base['embeddings'])[0]
    top_indices = np.argsort(similarities)[-limit:][::-1]
    
    # Ø±ÙØ¹ Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„Ø¶Ù…Ø§Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    return [knowledge_base['documents'][i] for i in top_indices if similarities[i] > 0.4]

class EnhancedAIResponder:
    """Ù…Ø¬ÙŠØ¨ Ø°ÙƒÙŠ Ù…ØªÙ‚Ø¯Ù… ÙŠØ³ØªØ®Ø¯Ù… Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬ AI"""
    
    def __init__(self, ai_manager: AIModelManager):
        self.ai_manager = ai_manager
        self.available_models = ai_manager.get_available_models()
    
    def get_gemini_response(self, query: str, context_docs: List[Dict]) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Gemini"""
        try:
            API_KEY = st.secrets["GEMINI_API_KEY"]
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Gemini 1.5 Flash Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„ Ù…Ø¹ Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©
            API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}"
            
            context = "\n\n".join([f"Source: {doc['source']}\nContent: {doc['content']}" for doc in context_docs])
            
            prompt = f"""
            Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¹Ø§Ù„Ù…ÙŠ ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… ØªÙØ¯Ø¹Ù‰ 'Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨'.
            Ø¨Ù†Ø§Ø¡Ù‹ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ù„Ù…ÙŠ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø£Ø¯Ù†Ø§Ù‡ØŒ Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø© ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
            Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ°ÙƒØ± Ø£Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.

            **Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ù„Ù…ÙŠ:**
            ---
            {context}
            ---

            **Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:**
            {query}

            **Ø¥Ø¬Ø§Ø¨ØªÙƒ (Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·):**
            """
            
            payload = {"contents": [{"parts": [{"text": prompt}]}]}
            response = requests.post(API_URL, json=payload, headers={"Content-Type": "application/json"})
            response.raise_for_status()
            result = response.json()
            return result['candidates'][0]['content']['parts'][0]['text']
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Gemini: {str(e)}"
    
    def get_deepseek_response(self, query: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† DeepSeek (Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø©)"""
        try:
            API_KEY = st.secrets["DEEPSEEK_API_KEY"]
            API_URL = "https://api.deepseek.com/v1/chat/completions"
            
            headers = {
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… ÙˆØ§Ù„Ø·ÙŠÙˆØ±. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù„Ù…ÙŠØ© ÙˆØ¯Ù‚ÙŠÙ‚Ø©."
                    },
                    {
                        "role": "user",
                        "content": query
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.7
            }
            
            response = requests.post(API_URL, json=payload, headers=headers)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content']
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ DeepSeek: {str(e)}"
    
    def get_huggingface_response(self, query: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Hugging Face (Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ø§Ù†ÙŠ)"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ø§Ù†ÙŠ Ù…Ù† Hugging Face
            API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"
            
            # Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ GPT-2 Ø£Ùˆ Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø±Ø¨ÙŠØ©
            headers = {"Authorization": f"Bearer {st.secrets.get('HUGGINGFACE_API_KEY', '')}"}
            
            payload = {
                "inputs": f"ÙƒØ®Ø¨ÙŠØ± ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…ØŒ Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}",
                "parameters": {"max_length": 500, "temperature": 0.7}
            }
            
            response = requests.post(API_URL, json=payload, headers=headers)
            if response.status_code == 200:
                result = response.json()
                return result[0]['generated_text'] if isinstance(result, list) else "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø©."
            else:
                return "Ø®Ø¯Ù…Ø© Hugging Face ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Hugging Face: {str(e)}"
    
    def get_ollama_response(self, query: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ollama Ø§Ù„Ù…Ø­Ù„ÙŠ"""
        try:
            payload = {
                "model": "llama3.2",  # Ø£Ùˆ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ§Ø­ Ù…Ø­Ù„ÙŠØ§Ù‹
                "prompt": f"ÙƒØ®Ø¨ÙŠØ± ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… ÙˆØ§Ù„Ø·ÙŠÙˆØ±ØŒ Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: {query}",
                "stream": False
            }
            
            response = requests.post("http://localhost:11434/api/generate", json=payload)
            response.raise_for_status()
            result = response.json()
            return result.get('response', 'Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©.')
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ollama: {str(e)}"
    
    def get_comprehensive_answer(self, query: str, context_docs: List[Dict]) -> tuple:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬"""
        
        # Ø£ÙˆÙ„Ø§Ù‹: Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        if context_docs:
            if "gemini" in self.available_models:
                local_answer = self.get_gemini_response(query, context_docs)
                sources = "ØŒ ".join(list(set([doc['source'] for doc in context_docs])))
                return local_answer, f"Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø­Ù„ÙŠØ©: {sources}", "Ù…Ø­Ù„ÙŠ"
        
        # Ø«Ø§Ù†ÙŠØ§Ù‹: Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù†ØªØ§Ø¦Ø¬ Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
        for model in self.available_models:
            try:
                if model == "deepseek":
                    answer = self.get_deepseek_response(query)
                    if not answer.startswith("Ø®Ø·Ø£"):
                        return answer, "DeepSeek AI", "Ø®Ø§Ø±Ø¬ÙŠ"
                elif model == "huggingface":
                    answer = self.get_huggingface_response(query)
                    if not answer.startswith("Ø®Ø·Ø£"):
                        return answer, "Hugging Face AI", "Ø®Ø§Ø±Ø¬ÙŠ"
                elif model == "ollama":
                    answer = self.get_ollama_response(query)
                    if not answer.startswith("Ø®Ø·Ø£"):
                        return answer, "Ollama Ø§Ù„Ù…Ø­Ù„ÙŠ", "Ù…Ø­Ù„ÙŠ Ù…ØªÙ‚Ø¯Ù…"
            except Exception as e:
                continue
        
        return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø³Ø¤Ø§Ù„Ùƒ Ù…Ù† Ø£ÙŠ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©.", "ØºÙŠØ± Ù…ØªØ§Ø­", "ÙØ´Ù„"

# -------------------------------------------------
#  5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
# -------------------------------------------------

def main():
    st.title("ğŸ§¬ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0 Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    st.markdown("### Ø­Ø§ÙˆØ± Ø®Ø¨ÙŠØ± Ø§Ù„ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ù‚Ø¯Ø±Ø§Øª AI Ù…ØªÙ‚Ø¯Ù…Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª
    model = load_embedding_model()
    ai_manager = initialize_ai_manager()
    knowledge_base = load_knowledge_base(model)
    ai_responder = EnhancedAIResponder(ai_manager)
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("ğŸ¤– Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©")
        available_models = ai_manager.get_available_models()
        
        if available_models:
            for model_key in available_models:
                model_info = ai_manager.models[model_key]
                st.success(f"âœ… {model_info['name']}")
        else:
            st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ§Ø­Ø©")
        
        st.header("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ù„Ø³Ø©")
        if "query_count" not in st.session_state:
            st.session_state.query_count = 0
        if "local_answers" not in st.session_state:
            st.session_state.local_answers = 0
        if "external_answers" not in st.session_state:
            st.session_state.external_answers = 0
        
        st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©", st.session_state.query_count)
        st.metric("Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø­Ù„ÙŠØ©", st.session_state.local_answers)
        st.metric("Ø¥Ø¬Ø§Ø¨Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©", st.session_state.external_answers)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant", 
                "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0. Ø£Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„ØªÙƒ Ù…Ù† Ù…ØµØ§Ø¯Ø±ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠØ©ØŒ ÙˆØ¥Ø°Ø§ Ù„Ù… Ø£Ø¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ø³Ø£ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø£Ø®Ø±Ù‰ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù…ÙƒÙ†Ø©. Ø§Ø³Ø£Ù„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ Ù…ØªØ¹Ù„Ù‚ Ø¨ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…!"
            }
        ]
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    if prompt := st.chat_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø¬ÙŠÙ†ØŒ Ø·ÙØ±Ø©ØŒ Ø£Ùˆ Ù†Ù…Ø· ÙˆØ±Ø§Ø«ÙŠ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            if not available_models:
                st.error("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª.")
            else:
                with st.spinner("Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ ÙŠÙÙƒØ± ÙˆÙŠØ¨Ø­Ø« ÙÙŠ Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©..."):
                    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ©
                    relevant_docs = []
                    if knowledge_base:
                        relevant_docs = search_semantic_knowledge(prompt, model, knowledge_base)
                    
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø©
                    answer, source_info, answer_type = ai_responder.get_comprehensive_answer(prompt, relevant_docs)
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                    st.session_state.query_count += 1
                    if answer_type == "Ù…Ø­Ù„ÙŠ":
                        st.session_state.local_answers += 1
                    elif answer_type in ["Ø®Ø§Ø±Ø¬ÙŠ", "Ù…Ø­Ù„ÙŠ Ù…ØªÙ‚Ø¯Ù…"]:
                        st.session_state.external_answers += 1
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØµØ¯Ø±
                    response_with_source = f"{answer}\n\n*Ø§Ù„Ù…ØµØ¯Ø±: {source_info}*"
                    
                    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¤Ø´Ø± Ù†ÙˆØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                    if answer_type == "Ù…Ø­Ù„ÙŠ":
                        response_with_source += "\n\nğŸ  *ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ©*"
                    elif answer_type == "Ø®Ø§Ø±Ø¬ÙŠ":
                        response_with_source += "\n\nğŸŒ *ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø®Ø§Ø±Ø¬ÙŠ*"
                    elif answer_type == "Ù…Ø­Ù„ÙŠ Ù…ØªÙ‚Ø¯Ù…":
                        response_with_source += "\n\nğŸ–¥ï¸ *ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù„ÙŠ Ù…ØªÙ‚Ø¯Ù…*"
                
                st.markdown(response_with_source)
                st.session_state.messages.append({"role": "assistant", "content": response_with_source})

if __name__ == "__main__":
    main()
