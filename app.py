# ==============================================================================
#  Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 18.0 (Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ)
#  - ÙŠØ¯Ù…Ø¬ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¯Ø§Ø¦Ù…Ø© Ù…Ø¹ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§.
# ==============================================================================

# --- HOT-PATCH FOR SQLITE3 ON STREAMLIT CLOUD ---
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# --------------------------------------------------

import streamlit as st
import sqlite3
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os
import tempfile
import hashlib
import requests
import json
import numpy as np

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±
# -------------------------------------------------
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 18.0",
    page_icon="ğŸ§¬",
    layout="wide",
)

# Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ¨ Ù„Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¯Ø§Ø¦Ù…Ø©
BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
]

# -------------------------------------------------
#  2. Ø¨Ù†Ø§Ø¡ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø±ÙØ© (Ø§Ù„Ø¯Ø§Ø¦Ù…Ø© ÙˆØ§Ù„Ù…Ø¤Ù‚ØªØ©)
# -------------------------------------------------

@st.cache_resource
def load_embedding_model():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª."""
    return SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

@st.cache_data(ttl=86400)
def load_permanent_knowledge_base(_model):
    """Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¯Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„ÙƒØªØ¨."""
    db_path = os.path.join(tempfile.gettempdir(), "permanent_knowledge_v18.db")
    conn = sqlite3.connect(db_path, check_same_thread=False)
    cursor = conn.cursor()
    cursor.execute("CREATE TABLE IF NOT EXISTS knowledge (source TEXT, content TEXT UNIQUE)")
    
    cursor.execute("SELECT COUNT(*) FROM knowledge")
    if cursor.fetchone()[0] == 0:
        with st.spinner("ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¯Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹..."):
            for i, link in enumerate(BOOK_LINKS):
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                        file_id = link.split('/d/')[1].split('/')[0]
                        gdown.download(id=file_id, output=tmp.name, quiet=True)
                        with open(tmp.name, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page_num, page in enumerate(reader.pages):
                                text = page.extract_text() or ""
                                if len(text.strip()) > 150:
                                    cursor.execute("INSERT OR IGNORE INTO knowledge (source, content) VALUES (?, ?)",
                                                   (f"Book {i+1}, Page {page_num+1}", text.strip()))
                        os.remove(tmp.name)
                except Exception as e:
                    print(f"Could not process book {i+1}: {e}")
            conn.commit()

    cursor.execute("SELECT source, content FROM knowledge")
    all_docs = [{"source": row[0], "content": row[1]} for row in cursor.fetchall()]
    conn.close()

    if not all_docs: return None

    contents = [doc['content'] for doc in all_docs]
    embeddings = _model.encode(contents, show_progress_bar=False)
    return {"documents": all_docs, "embeddings": embeddings}

def process_uploaded_file(uploaded_file, model):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ù…Ø¤Ù‚ØªØ© Ù„Ù‡."""
    if uploaded_file is None:
        return None
    
    with st.spinner(f"Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù '{uploaded_file.name}'..."):
        try:
            text = ""
            if uploaded_file.type == "application/pdf":
                reader = PyPDF2.PdfReader(uploaded_file)
                for page in reader.pages:
                    text += (page.extract_text() or "") + "\n"
            else: # for .txt files
                text = uploaded_file.getvalue().decode("utf-8")

            chunks = [chunk.strip() for chunk in text.split('\n\n') if len(chunk.strip()) > 100]
            if not chunks:
                st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ ÙƒØ§ÙÙ ÙÙŠ Ø§Ù„Ù…Ù„Ù Ù„ØªØ­Ù„ÙŠÙ„Ù‡.")
                return None

            embeddings = model.encode(chunks, show_progress_bar=False)
            documents = [{"source": f"Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹: {uploaded_file.name}", "content": chunk} for chunk in chunks]
            
            st.success(f"ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„Ù‡.")
            return {"documents": documents, "embeddings": embeddings}
        except Exception as e:
            st.error(f"ÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {e}")
            return None

# -------------------------------------------------
#  3. Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (RAG)
# -------------------------------------------------

def search_knowledge_base(query, model, knowledge_base, limit=2):
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø£ÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© (Ø¯Ø§Ø¦Ù…Ø© Ø£Ùˆ Ù…Ø¤Ù‚ØªØ©)."""
    if not knowledge_base: return []
    query_embedding = model.encode([query])
    similarities = cosine_similarity(query_embedding, knowledge_base['embeddings'])[0]
    top_indices = np.argsort(similarities)[-limit:][::-1]
    return [knowledge_base['documents'][i] for i in top_indices if similarities[i] > 0.3]

@st.cache_data
def get_rag_answer_with_gemini(query, context_docs):
    try:
        API_KEY = st.secrets["GEMINI_API_KEY"]
    except (FileNotFoundError, KeyError):
        return "Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ GEMINI_API_KEY."

    API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}"
    context = "\n\n".join([f"Source: {doc['source']}\nContent: {doc['content']}" for doc in context_docs])
    prompt = f"Based exclusively on the scientific context provided, answer the user's question in Arabic.\n\nContext:\n---\n{context}\n---\n\nUser's Question:\n{query}\n\nAnswer (in Arabic):"
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    try:
        response = requests.post(API_URL, json=payload, headers={"Content-Type": "application/json"}, timeout=30)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text']
    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ: {str(e)}"

# -------------------------------------------------
#  4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨)
# -------------------------------------------------

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹ ---
model = load_embedding_model()
permanent_kb = load_permanent_knowledge_base(model)

# --- Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ (Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©) ---
with st.sidebar:
    st.header("ğŸ§  Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©")
    st.write("Ù‚Ù… Ø¨ØªÙˆØ³ÙŠØ¹ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ø¨Ø´ÙƒÙ„ Ù…Ø¤Ù‚Øª Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø±ÙØ¹ Ù…Ù„ÙØ§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ©.")
    
    uploaded_file = st.file_uploader(
        "Ø§Ø±ÙØ¹ Ù…Ù„Ù (PDF, TXT)",
        type=['pdf', 'txt'],
        help="Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„Ù‡ Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø© ÙÙ‚Ø·."
    )
    
    if uploaded_file:
        st.session_state.temporary_kb = process_uploaded_file(uploaded_file, model)
    
    if st.button("ğŸ—‘ï¸ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª"):
        st.session_state.temporary_kb = None
        st.success("ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª.")
        st.rerun()

# --- ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
st.title("ğŸ•Šï¸ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 18.0 (Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ)")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨. ÙŠÙ…ÙƒÙ†Ùƒ Ø³Ø¤Ø§Ù„ÙŠ Ù…Ù† Ù…Ø¹Ø±ÙØªÙŠ Ø§Ù„Ø¯Ø§Ø¦Ù…Ø©ØŒ Ø£Ùˆ Ø±ÙØ¹ Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù…Ù†Ø§Ù‚Ø´ØªÙ‡."}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø¬ÙŠÙ†ØŒ Ø£Ùˆ Ø¹Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ ÙŠØ¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ù…ØµØ§Ø¯Ø±Ù‡ ÙˆÙŠÙÙƒØ±..."):
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙƒÙ„Ø§ Ù‚Ø§Ø¹Ø¯ØªÙŠ Ø§Ù„Ù…Ø¹Ø±ÙØ©
            # *** ØªÙ… ØªØµØ­ÙŠØ­ Ø§Ø³Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ù‡Ù†Ø§ ***
            permanent_docs = search_knowledge_base(prompt, model, permanent_kb)
            temporary_docs = []
            if 'temporary_kb' in st.session_state and st.session_state.temporary_kb:
                # *** ØªÙ… ØªØµØ­ÙŠØ­ Ø§Ø³Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ù‡Ù†Ø§ ***
                temporary_docs = search_knowledge_base(prompt, model, st.session_state.temporary_kb)

            combined_docs = permanent_docs + temporary_docs
            
            if not combined_docs:
                response = "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ø£ÙŠ Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©."
            else:
                response = get_rag_answer_with_gemini(prompt, combined_docs)
                sources = "ØŒ ".join(list(set([doc['source'] for doc in combined_docs])))
                response += f"\n\n*Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„ÙŠÙ‡Ø§: {sources}*"
        
        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
