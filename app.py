# ==============================================================================
#  Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0 Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Ù…Ø¹ AI Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬)
# ==============================================================================

import streamlit as st
import sqlite3
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os
import tempfile
import hashlib
import requests
import json
import numpy as np
from typing import List, Dict

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±
# -------------------------------------------------
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0",
    page_icon="ğŸ§¬",
    layout="wide",
)

# Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ¨ (Ù…Ø­Ø¯ÙˆØ¯Ø© Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠ)
BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
]

# -------------------------------------------------
#  2. Ù…Ø¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
# -------------------------------------------------

class AIModelManager:
    """Ù…Ø¯ÙŠØ± Ù„Ø¥Ø¯Ø§Ø±Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©"""
    def __init__(self):
        self.models = {
            "gemini": {"name": "Google Gemini", "available": self._check_secret("GEMINI_API_KEY"), "priority": 1},
            "deepseek": {"name": "DeepSeek", "available": self._check_secret("DEEPSEEK_API_KEY"), "priority": 2},
            "huggingface": {"name": "Hugging Face", "available": self._check_secret("HUGGINGFACE_API_KEY"), "priority": 3},
        }

    def _check_secret(self, key: str) -> bool:
        try:
            return st.secrets.get(key) is not None
        except Exception:
            return False

    def get_available_models(self) -> List[str]:
        available = [model for model, config in self.models.items() if config["available"]]
        return sorted(available, key=lambda x: self.models[x]["priority"])

# -------------------------------------------------
#  3. Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
# -------------------------------------------------

@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

@st.cache_data(ttl=86400)
def load_knowledge_base(_model):
    db_path = os.path.join(tempfile.gettempdir(), "text_knowledge_v11.db")
    conn = sqlite3.connect(db_path, check_same_thread=False)
    cursor = conn.cursor()
    cursor.execute("CREATE TABLE IF NOT EXISTS knowledge (source TEXT, content TEXT UNIQUE)")
    
    cursor.execute("SELECT COUNT(*) FROM knowledge")
    if cursor.fetchone()[0] == 0:
        with st.spinner("ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹..."):
            for i, link in enumerate(BOOK_LINKS):
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                        file_id = link.split('/d/')[1].split('/')[0]
                        gdown.download(id=file_id, output=tmp.name, quiet=True)
                        with open(tmp.name, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page_num, page in enumerate(reader.pages):
                                text = page.extract_text() or ""
                                if len(text.strip()) > 150:
                                    cursor.execute("INSERT OR IGNORE INTO knowledge (source, content) VALUES (?, ?)",
                                                   (f"Book {i+1}, Page {page_num+1}", text.strip()))
                        os.remove(tmp.name)
                except Exception as e:
                    print(f"Could not process book {i+1}: {e}")
            conn.commit()

    cursor.execute("SELECT source, content FROM knowledge")
    all_docs = [{"source": row[0], "content": row[1]} for row in cursor.fetchall()]
    conn.close()

    if not all_docs: return None
    
    contents = [doc['content'] for doc in all_docs]
    embeddings = _model.encode(contents, show_progress_bar=False)
    return {"documents": all_docs, "embeddings": embeddings}

# -------------------------------------------------
#  4. Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# -------------------------------------------------

def search_semantic_knowledge(query, model, knowledge_base, limit=3):
    if not knowledge_base: return []
    query_embedding = model.encode([query])
    similarities = cosine_similarity(query_embedding, knowledge_base['embeddings'])[0]
    top_indices = np.argsort(similarities)[-limit:][::-1]
    return [knowledge_base['documents'][i] for i in top_indices if similarities[i] > 0.4]

class EnhancedAIResponder:
    def __init__(self, ai_manager: AIModelManager):
        self.ai_manager = ai_manager
        self.available_models = ai_manager.get_available_models()

    def get_gemini_response(self, query: str, context_docs: List[Dict]) -> str:
        API_KEY = st.secrets["GEMINI_API_KEY"]
        API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}"
        context = "\n\n".join([f"Source: {doc['source']}\nContent: {doc['content']}" for doc in context_docs])
        prompt = f"Based ONLY on the context below, answer the user's question in Arabic.\n\nContext:\n{context}\n\nUser Question: {query}\n\nAnswer (in Arabic):"
        payload = {"contents": [{"parts": [{"text": prompt}]}]}
        try:
            response = requests.post(API_URL, json=payload)
            response.raise_for_status()
            return response.json()['candidates'][0]['content']['parts'][0]['text']
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Gemini: {str(e)}"

    def get_deepseek_response(self, query: str) -> str:
        API_KEY = st.secrets["DEEPSEEK_API_KEY"]
        API_URL = "https://api.deepseek.com/v1/chat/completions"
        headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
        payload = {
            "model": "deepseek-chat",
            "messages": [{"role": "system", "content": "You are an expert in pigeon genetics. Answer in Arabic."}, {"role": "user", "content": query}]
        }
        try:
            response = requests.post(API_URL, json=payload, headers=headers)
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ DeepSeek: {str(e)}"
            
    def get_huggingface_response(self, query: str) -> str:
        API_KEY = st.secrets["HUGGINGFACE_API_KEY"]
        API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2"
        headers = {"Authorization": f"Bearer {API_KEY}"}
        payload = {"inputs": f"As an expert in pigeon genetics, answer this question in Arabic: {query}"}
        try:
            response = requests.post(API_URL, json=payload, headers=headers)
            response.raise_for_status()
            return response.json()[0]['generated_text']
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Hugging Face: {str(e)}"

    def get_comprehensive_answer(self, query: str, context_docs: List[Dict]) -> tuple:
        if context_docs and "gemini" in self.available_models:
            local_answer = self.get_gemini_response(query, context_docs)
            sources = ", ".join(list(set([doc['source'] for doc in context_docs])))
            return local_answer, f"Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø­Ù„ÙŠØ©: {sources}", "Ù…Ø­Ù„ÙŠ (Gemini RAG)"

        for model_key in self.available_models:
            try:
                if model_key == "deepseek":
                    answer = self.get_deepseek_response(query)
                    if "Ø®Ø·Ø£" not in answer: return answer, "DeepSeek AI", "Ø®Ø§Ø±Ø¬ÙŠ"
                elif model_key == "huggingface":
                    answer = self.get_huggingface_response(query)
                    if "Ø®Ø·Ø£" not in answer: return answer, "Hugging Face AI", "Ø®Ø§Ø±Ø¬ÙŠ"
            except Exception:
                continue
        
        return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø£ÙŠ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©.", "N/A", "ÙØ´Ù„"

# -------------------------------------------------
#  5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# -------------------------------------------------
st.title("ğŸ§¬ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0 Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
st.markdown("### Ø­Ø§ÙˆØ± Ø®Ø¨ÙŠØ± Ø§Ù„ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ù‚Ø¯Ø±Ø§Øª AI Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")

model = load_embedding_model()
ai_manager = AIModelManager()
knowledge_base = load_knowledge_base(model)
ai_responder = EnhancedAIResponder(ai_manager)

with st.sidebar:
    st.header("ğŸ¤– Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©")
    for model_key, config in ai_manager.models.items():
        if config["available"]:
            st.success(f"âœ… {config['name']}")
        else:
            st.warning(f"âŒ {config['name']} (ØºÙŠØ± Ù…ØªØ§Ø­)")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ø§Ù„Ø¥ØµØ¯Ø§Ø± 11.0. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ Ù…ØªØ¹Ù„Ù‚ Ø¨ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ ÙŠÙÙƒØ± ÙˆÙŠØªØ´Ø§ÙˆØ± Ù…Ø¹ Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡..."):
            relevant_docs = search_semantic_knowledge(prompt, model, knowledge_base)
            answer, source_info, answer_type = ai_responder.get_comprehensive_answer(prompt, relevant_docs)
            
            response_with_source = f"{answer}\n\n*Ø§Ù„Ù…ØµØ¯Ø±: {source_info} ({answer_type})*"
            st.markdown(response_with_source)
            st.session_state.messages.append({"role": "assistant", "content": response_with_source})
