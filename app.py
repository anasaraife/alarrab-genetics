# ==============================================================================
#  Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª: Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
#  Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØ§Ù„ØªÙƒØ§Ù…Ù„
#  -- Ø§Ù„Ø¥ØµØ¯Ø§Ø± 1.0: ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© --
# ==============================================================================

# -------------------------------------------------
#  Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©
# -------------------------------------------------
import streamlit as st
import sqlite3
import pandas as pd
import faiss
import pickle
from sentence_transformers import SentenceTransformer
import numpy as np

# -------------------------------------------------
#  Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
# -------------------------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© Ù„ØµÙØ­Ø© Ø§Ù„ÙˆÙŠØ¨
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª",
    page_icon="ğŸ•Šï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡Ø§Øª (ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù†ÙØ³ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø¨Ù†Ø§Ø¡)
# Ø³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ÙˆØªØ®Ø²ÙŠÙ†Ù‡ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
@st.cache_resource
def load_model(model_name='paraphrase-multilingual-mpnet-base-v2'):
    """
    ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ SentenceTransformer Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª.
    """
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...")
    model = SentenceTransformer(model_name)
    print("Ø§ÙƒØªÙ…Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")
    return model

# ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ©
@st.cache_data
def load_vector_store(file_path="vector_store.pkl"):
    """
    ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù„Ù.
    """
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ© (Ø°Ø§ÙƒØ±Ø© Ø§Ù„ÙƒØªØ¨)...")
    try:
        with open(file_path, "rb") as f:
            vector_store = pickle.load(f)
        print("Ø§ÙƒØªÙ…Ù„ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ©.")
        return vector_store
    except FileNotFoundError:
        st.error(f"Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ© '{file_path}'. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Ø¨Øª Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹.")
        return None

# -------------------------------------------------
#  Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
# -------------------------------------------------
def search_knowledge_base(query, model, vector_store, top_k=3):
    """
    ØªØ¨Ø­Ø« Ø¹Ù† Ø¥Ø¬Ø§Ø¨Ø© Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ø®Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ©.
    """
    if vector_store is None:
        return []

    print(f"Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¥Ø¬Ø§Ø¨Ø© Ù„Ù€: '{query}'")
    # ØªØ­ÙˆÙŠÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡
    query_embedding = model.encode([query], convert_to_tensor=True)
    query_embedding = query_embedding.cpu().detach().numpy().astype('float32')

    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙÙ‡Ø±Ø³ FAISS Ø¹Ù† Ø£Ù‚Ø±Ø¨ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
    # D: Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª (Ù…Ø¯Ù‰ Ø§Ù„Ù‚Ø±Ø¨)ØŒ I: Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª (Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡)
    distances, indices = vector_store['index'].search(query_embedding, top_k)
    
    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    results = []
    for i in range(top_k):
        chunk_index = indices[0][i]
        results.append({
            "text": vector_store['chunks'][chunk_index],
            "source": vector_store['metadata'][chunk_index],
            "score": 1 - distances[0][i] # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ Ø¯Ø±Ø¬Ø© ØªØ´Ø§Ø¨Ù‡
        })
    
    print("Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø¨Ø­Ø«.")
    return results

# -------------------------------------------------
#  Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø¨Ù†Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# -------------------------------------------------
def main():
    # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
    st.title("ğŸ•Šï¸ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª: ÙˆÙƒÙŠÙ„Ùƒ Ø§Ù„Ø°ÙƒÙŠ Ù„ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…")
    st.write("Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„ØªÙŠ Ø¨Ù†ÙŠÙ†Ø§Ù‡Ø§.")

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    model = load_model()
    vector_store = load_vector_store()

    # Ù…Ø±Ø¨Ø¹ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„
    user_query = st.text_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…...", placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡Ùˆ Ø¬ÙŠÙ† Ø§Ù„Ø£ÙˆØ¨Ø§Ù„ Ø§Ù„Ø³Ø§Ø¦Ø¯ØŸ")

    if user_query:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù†Ø¯ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø³Ø¤Ø§Ù„
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ©..."):
            search_results = search_knowledge_base(user_query, model, vector_store)

        st.subheader("Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«:")
        
        if not search_results:
            st.warning("Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")
        else:
            # Ø¹Ø±Ø¶ Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù…ÙŠØ²
            best_result = search_results[0]
            st.success(f"**Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© (Ø¨Ù†Ø³Ø¨Ø© ØªØ´Ø§Ø¨Ù‡ {best_result['score']:.2%}):**")
            st.markdown(f"> {best_result['text']}")
            st.caption(f"Ø§Ù„Ù…ØµØ¯Ø±: {best_result['source']}")
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø®Ø±Ù‰ (Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª)
            if len(search_results) > 1:
                with st.expander("Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ø¥Ø¶Ø§ÙÙŠØ©"):
                    for result in search_results[1:]:
                        st.info(result['text'])
                        st.caption(f"Ø§Ù„Ù…ØµØ¯Ø±: {result['source']} (Ø¨Ù†Ø³Ø¨Ø© ØªØ´Ø§Ø¨Ù‡ {result['score']:.2%})")

# -------------------------------------------------
#  Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# -------------------------------------------------
if __name__ == "__main__":
    main()
