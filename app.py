# ==============================================================================
#  Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.1 Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù† (Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­ Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡)
# ==============================================================================

import streamlit as st
import sqlite3
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os
import tempfile
import requests
import json
import numpy as np
from typing import List, Dict, Tuple
import time

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±
# -------------------------------------------------
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.1",
    page_icon="ğŸ§¬",
    layout="wide",
)

BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
]

# -------------------------------------------------
#  2. Ù…Ø¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†
# -------------------------------------------------
class AIModelManager:
    def __init__(self):
        self.models = {
            "gemini": {
                "name": "Google Gemini", 
                "available": self._check_gemini_key(), 
                "priority": 1,
                "status": "Ø¬Ø§Ù‡Ø²" if self._check_gemini_key() else "Ù…ÙØªØ§Ø­ API Ù…ÙÙ‚ÙˆØ¯"
            },
            "deepseek": {
                "name": "DeepSeek", 
                "available": self._check_deepseek_key(), 
                "priority": 2,
                "status": "Ø¬Ø§Ù‡Ø²" if self._check_deepseek_key() else "Ù…ÙØªØ§Ø­ API Ù…ÙÙ‚ÙˆØ¯"
            },
            "huggingface": {
                "name": "Hugging Face", 
                "available": self._check_huggingface_key(), 
                "priority": 3,
                "status": "Ø¬Ø§Ù‡Ø²" if self._check_huggingface_key() else "Ù…ÙØªØ§Ø­ API Ù…ÙÙ‚ÙˆØ¯"
            },
            "fallback": {
                "name": "Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ", 
                "available": True, 
                "priority": 4,
                "status": "Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ§Ø­"
            }
        }

    def _check_gemini_key(self) -> bool:
        """ÙØ­Øµ Ù…ÙØªØ§Ø­ Gemini Ù…Ø¹ Ø±Ø³Ø§Ø¦Ù„ ØªØ´Ø®ÙŠØµÙŠØ©"""
        try:
            key = st.secrets.get("GEMINI_API_KEY", "")
            if not key:
                st.sidebar.warning("âš ï¸ Ù…ÙØªØ§Ø­ GEMINI_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ secrets")
                return False
            if len(key) < 20:
                st.sidebar.warning("âš ï¸ Ù…ÙØªØ§Ø­ Gemini ÙŠØ¨Ø¯Ùˆ ØºÙŠØ± ØµØ­ÙŠØ­")
                return False
            return True
        except Exception as e:
            st.sidebar.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ù…ÙØªØ§Ø­ Gemini: {e}")
            return False

    def _check_deepseek_key(self) -> bool:
        """ÙØ­Øµ Ù…ÙØªØ§Ø­ DeepSeek Ù…Ø¹ Ø±Ø³Ø§Ø¦Ù„ ØªØ´Ø®ÙŠØµÙŠØ©"""
        try:
            key = st.secrets.get("DEEPSEEK_API_KEY", "")
            if not key:
                st.sidebar.info("ğŸ’¡ Ù…ÙØªØ§Ø­ DEEPSEEK_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
                return False
            return True
        except Exception as e:
            st.sidebar.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ù…ÙØªØ§Ø­ DeepSeek: {e}")
            return False

    def _check_huggingface_key(self) -> bool:
        """ÙØ­Øµ Ù…ÙØªØ§Ø­ Hugging Face Ù…Ø¹ Ø±Ø³Ø§Ø¦Ù„ ØªØ´Ø®ÙŠØµÙŠØ©"""
        try:
            key = st.secrets.get("HUGGINGFACE_API_KEY", "")
            if not key:
                st.sidebar.info("ğŸ’¡ Ù…ÙØªØ§Ø­ HUGGINGFACE_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)")
                return False
            return True
        except Exception as e:
            st.sidebar.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ù…ÙØªØ§Ø­ Hugging Face: {e}")
            return False

    def get_available_models(self) -> List[str]:
        available = [model for model, config in self.models.items() if config["available"]]
        return sorted(available, key=lambda x: self.models[x]["priority"])

    def get_model_status(self) -> Dict:
        return {k: v["status"] for k, v in self.models.items()}

# -------------------------------------------------
#  3. Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†
# -------------------------------------------------
@st.cache_resource
def load_embedding_model():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    try:
        return SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {e}")
        return None

@st.cache_data(ttl=86400, show_spinner=False)
def load_knowledge_base(_model):
    """Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡"""
    if _model is None:
        return None
        
    db_path = os.path.join(tempfile.gettempdir(), "text_knowledge_v12_1.db")
    
    try:
        conn = sqlite3.connect(db_path, check_same_thread=False)
        cursor = conn.cursor()
        cursor.execute("CREATE TABLE IF NOT EXISTS knowledge (source TEXT, content TEXT UNIQUE)")
        
        cursor.execute("SELECT COUNT(*) FROM knowledge")
        if cursor.fetchone()[0] == 0:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, link in enumerate(BOOK_LINKS):
                try:
                    status_text.text(f"ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ {i+1} Ù…Ù† {len(BOOK_LINKS)}...")
                    progress_bar.progress((i) / len(BOOK_LINKS))
                    
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                        file_id = link.split('/d/')[1].split('/')[0]
                        gdown.download(id=file_id, output=tmp.name, quiet=True)
                        
                        with open(tmp.name, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page_num, page in enumerate(reader.pages):
                                text = page.extract_text() or ""
                                if len(text.strip()) > 100:  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰
                                    cursor.execute(
                                        "INSERT OR IGNORE INTO knowledge (source, content) VALUES (?, ?)",
                                        (f"Ø§Ù„ÙƒØªØ§Ø¨ {i+1}ØŒ Ø§Ù„ØµÙØ­Ø© {page_num+1}", text.strip())
                                    )
                        os.remove(tmp.name)
                        
                except Exception as e:
                    st.warning(f"ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ {i+1}: {e}")
                    continue
            
            conn.commit()
            progress_bar.progress(1.0)
            status_text.text("ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©!")
            time.sleep(1)
            progress_bar.empty()
            status_text.empty()

        cursor.execute("SELECT source, content FROM knowledge")
        all_docs = [{"source": row[0], "content": row[1]} for row in cursor.fetchall()]
        conn.close()

        if not all_docs:
            st.warning("âš ï¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙØ§Ø±ØºØ©")
            return None
        
        st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(all_docs)} ÙˆØ«ÙŠÙ‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©")
        
        contents = [doc['content'] for doc in all_docs]
        embeddings = _model.encode(contents, show_progress_bar=False)
        
        return {"documents": all_docs, "embeddings": embeddings}
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©: {e}")
        return None

# -------------------------------------------------
#  4. Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†
# -------------------------------------------------
def search_semantic_knowledge(query, model, knowledge_base, limit=5):
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª"""
    if not knowledge_base or not model:
        return []
    
    try:
        query_embedding = model.encode([query])
        similarities = cosine_similarity(query_embedding, knowledge_base['embeddings'])[0]
        top_indices = np.argsort(similarities)[-limit:][::-1]
        
        # ØªÙ‚Ù„ÙŠÙ„ Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£ÙƒØ«Ø±
        results = [knowledge_base['documents'][i] for i in top_indices if similarities[i] > 0.25]
        
        return results
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: {e}")
        return []

class EnhancedAIResponder:
    def __init__(self, ai_manager: AIModelManager):
        self.ai_manager = ai_manager
        self.available_models = ai_manager.get_available_models()

    def get_gemini_response(self, query: str, context_docs: List[Dict]) -> Tuple[str, bool]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Gemini Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡"""
        try:
            API_KEY = st.secrets.get("GEMINI_API_KEY", "")
            if not API_KEY:
                return "Ù…ÙØªØ§Ø­ Gemini API ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", False
                
            API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}"
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚
            if context_docs:
                context = "\n\n".join([f"Ø§Ù„Ù…ØµØ¯Ø±: {doc['source']}\nØ§Ù„Ù…Ø­ØªÙˆÙ‰: {doc['content'][:800]}..." for doc in context_docs])
                prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©:

Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ù„Ù…ÙŠ:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙØµÙ„Ø©):"""
            else:
                prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… ÙˆØ§Ù„Ø·ÙŠÙˆØ±. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¹Ù„Ù…ÙŠ ÙˆØ¯Ù‚ÙŠÙ‚:

{query}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""

            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 1000,
                    "topP": 0.8,
                    "topK": 40
                }
            }
            
            response = requests.post(
                API_URL, 
                json=payload, 
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    answer = result['candidates'][0]['content']['parts'][0]['text']
                    return answer, True
                else:
                    return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©", False
            else:
                return f"Ø®Ø·Ø£ HTTP {response.status_code}: {response.text}", False
                
        except requests.exceptions.Timeout:
            return "Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ø¹ Gemini", False
        except requests.exceptions.RequestException as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: {str(e)}", False
        except Exception as e:
            return f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}", False

    def get_deepseek_response(self, query: str) -> Tuple[str, bool]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† DeepSeek"""
        try:
            API_KEY = st.secrets.get("DEEPSEEK_API_KEY", "")
            if not API_KEY:
                return "Ù…ÙØªØ§Ø­ DeepSeek API ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", False
                
            API_URL = "https://api.deepseek.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… ÙˆØ§Ù„Ø·ÙŠÙˆØ±. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù„Ù…ÙŠØ© ÙˆØ¯Ù‚ÙŠÙ‚Ø©."
                    },
                    {
                        "role": "user",
                        "content": query
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.7
            }
            
            response = requests.post(API_URL, json=payload, headers=headers, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                answer = result['choices'][0]['message']['content']
                return answer, True
            else:
                return f"Ø®Ø·Ø£ DeepSeek: {response.status_code}", False
                
    def get_huggingface_response(self, query: str) -> Tuple[str, bool]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Hugging Face"""
        try:
            API_KEY = st.secrets.get("HUGGINGFACE_API_KEY", "")
            if not API_KEY:
                return "Ù…ÙØªØ§Ø­ Hugging Face API ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯", False
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            API_URL = "https://api-inference.huggingface.co/models/aubmindlab/bert-base-arabertv2"
            headers = {"Authorization": f"Bearer {API_KEY}"}
            
            # ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"
            
            payload = {
                "inputs": f"As a pigeon genetics expert, answer this question in Arabic: {query}",
                "parameters": {
                    "max_length": 500,
                    "temperature": 0.7,
                    "do_sample": True
                }
            }
            
            response = requests.post(API_URL, json=payload, headers=headers, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    answer = result[0].get('generated_text', 'Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø©.')
                    return answer, True
                else:
                    return "Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Hugging Face", False
            elif response.status_code == 503:
                return "Ù†Ù…ÙˆØ°Ø¬ Hugging Face Ù‚ÙŠØ¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹", False
            else:
                return f"Ø®Ø·Ø£ Hugging Face: {response.status_code}", False
                
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Hugging Face: {str(e)}", False

    def get_fallback_response(self, query: str) -> str:
        """Ø¥Ø¬Ø§Ø¨Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¹Ù†Ø¯Ù…Ø§ ØªÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        fallback_responses = {
            "color": "Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙÙŠ Ø§Ù„Ø­Ù…Ø§Ù… Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ù„Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ø¬ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØµØ¨ØºØ§Øª.",
            "genetics": "Ø§Ù„ÙˆØ±Ø§Ø«Ø© ÙÙŠ Ø§Ù„Ø­Ù…Ø§Ù… ØªØªØ¨Ø¹ Ù‚ÙˆØ§Ù†ÙŠÙ† Ù…Ù†Ø¯Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯Ø§Øª Ø§Ù„Ø®Ø§ØµØ©.",
            "breeding": "Ø§Ù„ØªØ±Ø¨ÙŠØ© Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ø¦ÙŠØ© ØªØªØ·Ù„Ø¨ ÙÙ‡Ù…Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹ Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© ÙˆØ§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©.",
            "mutation": "Ø§Ù„Ø·ÙØ±Ø§Øª ÙÙŠ Ø§Ù„Ø­Ù…Ø§Ù… ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø´ÙƒÙ„ ÙˆØ§Ù„Ù„ÙˆÙ† ÙˆØ£Ø­ÙŠØ§Ù†Ø§Ù‹ Ø§Ù„Ø³Ù„ÙˆÙƒ.",
            "pigeon": "Ø§Ù„Ø­Ù…Ø§Ù… Ù„Ù‡ ØªÙ†ÙˆØ¹ ÙˆØ±Ø§Ø«ÙŠ Ù‡Ø§Ø¦Ù„ ÙŠØ¸Ù‡Ø± ÙÙŠ Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ù…Ø®ØªÙ„ÙØ©."
        }
        
        query_lower = query.lower()
        for keyword, response in fallback_responses.items():
            if keyword in query_lower or any(arabic_word in query for arabic_word in ["Ù„ÙˆÙ†", "ÙˆØ±Ø§Ø«Ø©", "ØªØ±Ø¨ÙŠØ©", "Ø·ÙØ±Ø©", "Ø­Ù…Ø§Ù…"]):
                return f"{response}\n\nÙ„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙØ§ØªÙŠØ­ API Ø£Ùˆ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª."
        
        return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆØ¥Ø¹Ø¯Ø§Ø¯Ø§Øª API."

    def get_comprehensive_answer(self, query: str, context_docs: List[Dict]) -> Tuple[str, str, str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø©"""
        
        # Ø£ÙˆÙ„Ø§Ù‹: Ù…Ø­Ø§ÙˆÙ„Ø© Gemini Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ù„ÙŠ
        if context_docs and "gemini" in self.available_models:
            answer, success = self.get_gemini_response(query, context_docs)
            if success:
                sources = ", ".join(list(set([doc['source'] for doc in context_docs[:3]])))
                return answer, f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ©: {sources}", "Ù…Ø­Ù„ÙŠ + Gemini"
        
        # Ø«Ø§Ù†ÙŠØ§Ù‹: Ù…Ø­Ø§ÙˆÙ„Ø© Gemini Ø¨Ø¯ÙˆÙ† Ø³ÙŠØ§Ù‚ Ù…Ø­Ù„ÙŠ
        if "gemini" in self.available_models:
            answer, success = self.get_gemini_response(query, [])
            if success:
                return answer, "Google Gemini (Ù…Ø¹Ø±ÙØ© Ø¹Ø§Ù…Ø©)", "Gemini Ø¹Ø§Ù…"
        
        # Ø«Ø§Ù„Ø«Ø§Ù‹: Ù…Ø­Ø§ÙˆÙ„Ø© DeepSeek
        if "deepseek" in self.available_models:
            answer, success = self.get_deepseek_response(query)
            if success:
                return answer, "DeepSeek AI", "DeepSeek"
        
        # Ø±Ø§Ø¨Ø¹Ø§Ù‹: Ù…Ø­Ø§ÙˆÙ„Ø© Hugging Face
        if "huggingface" in self.available_models:
            answer, success = self.get_huggingface_response(query)
            if success:
                return answer, "Hugging Face AI", "HuggingFace"
        
        # Ø£Ø®ÙŠØ±Ø§Ù‹: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        fallback_answer = self.get_fallback_response(query)
        return fallback_answer, "Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ", "Ø§Ø­ØªÙŠØ§Ø·ÙŠ"

# -------------------------------------------------
#  5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†Ø©
# -------------------------------------------------
def main():
    st.title("ğŸ§¬ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.1 Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†")
    st.markdown("### Ø­Ø§ÙˆØ± Ø®Ø¨ÙŠØ± Ø§Ù„ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ ØªØ´Ø®ÙŠØµ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ø®Ø·Ø§Ø¡")

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª
    model = load_embedding_model()
    ai_manager = AIModelManager()
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„ØªØ´Ø®ÙŠØµ
    with st.sidebar:
        st.header("ğŸ” ØªØ´Ø®ÙŠØµ Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        st.subheader("Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:")
        model_status = ai_manager.get_model_status()
        for model_name, status in model_status.items():
            if "Ø¬Ø§Ù‡Ø²" in status:
                st.success(f"âœ… {ai_manager.models[model_name]['name']}: {status}")
            elif "Ù…ÙÙ‚ÙˆØ¯" in status:
                st.error(f"âŒ {ai_manager.models[model_name]['name']}: {status}")
            else:
                st.info(f"ğŸ’¡ {ai_manager.models[model_name]['name']}: {status}")
        
        # Ø­Ø§Ù„Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        st.subheader("Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©:")
        if model:
            st.success("âœ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø¬Ø§Ù‡Ø²")
        else:
            st.error("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†")
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©
        if "query_count" not in st.session_state:
            st.session_state.query_count = 0
        
        st.subheader("Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©", st.session_state.query_count)

    # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
    knowledge_base = load_knowledge_base(model) if model else None
    ai_responder = EnhancedAIResponder(ai_manager)

    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    if "messages" not in st.session_state:
        welcome_msg = """Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.1 Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù† ğŸ§¬

**Ù…Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯:**
- âœ… ØªØ´Ø®ÙŠØµ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ø®Ø·Ø§Ø¡
- âœ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù…ÙØ§ØªÙŠØ­ API
- âœ… Ù†Ù…Ø· Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
- âœ… ÙˆØ§Ø¬Ù‡Ø© ØªØ´Ø®ÙŠØµÙŠØ© ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ

ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„ØªÙƒ Ø­ÙˆÙ„ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù… Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ØªÙƒÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ù…ØªØ§Ø­Ø©!"""
        
        st.session_state.messages = [{"role": "assistant", "content": welcome_msg}]

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    if prompt := st.chat_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ Ù…ØªØ¹Ù„Ù‚ Ø¨ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.session_state.query_count += 1
        
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ ÙŠØ¨Ø­Ø« ÙˆÙŠÙÙƒØ±... ğŸ¤”"):
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
                relevant_docs = []
                if knowledge_base and model:
                    relevant_docs = search_semantic_knowledge(prompt, model, knowledge_base)
                    if relevant_docs:
                        st.info(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(relevant_docs)} ÙˆØ«ÙŠÙ‚Ø© Ø°Ø§Øª ØµÙ„Ø©")
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                answer, source_info, answer_type = ai_responder.get_comprehensive_answer(prompt, relevant_docs)
                
                # ØªØ­Ø¯ÙŠØ¯ Ù„ÙˆÙ† Ø§Ù„Ù…ØµØ¯Ø±
                if answer_type.startswith("Ù…Ø­Ù„ÙŠ"):
                    source_color = "ğŸ "
                elif "Gemini" in answer_type:
                    source_color = "ğŸ§ "
                elif "DeepSeek" in answer_type:
                    source_color = "ğŸš€"
                elif "HuggingFace" in answer_type:
                    source_color = "ğŸ¤—"
                else:
                    source_color = "ğŸ”„"
                
                response_with_source = f"{answer}\n\n---\n*{source_color} Ø§Ù„Ù…ØµØ¯Ø±: {source_info} ({answer_type})*"
                
                st.markdown(response_with_source)
                st.session_state.messages.append({"role": "assistant", "content": response_with_source})

if __name__ == "__main__":
    main()
