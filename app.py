# ==============================================================================
#  ุงูุนุฑูุงุจ ููุฌููุงุช - ุงูุฅุตุฏุงุฑ 13.0 ุงูููุทูููุฑ (ูุน ุชุญุณููุงุช ุดุงููุฉ)
# ==============================================================================

import streamlit as st
import sqlite3
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os
import tempfile
import requests
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
import time
import logging
from datetime import datetime
import hashlib
import re

# -------------------------------------------------
#  1. ุฅุนุฏุงุฏุงุช ุงูุตูุญุฉ ูุงููุตุงุฏุฑ ุงููุญุฏุซุฉ
# -------------------------------------------------
st.set_page_config(
    page_title="ุงูุนุฑูุงุจ ููุฌููุงุช - ุงูุฅุตุฏุงุฑ 13.0 ุงูููุทูููุฑ",
    page_icon="๐งฌ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ุฅุนุฏุงุฏ ูุธุงู ุงูุณุฌูุงุช
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
]

# ูุงููุณ ุงููุฑุงุฏูุงุช ูุงููููุงุช ุงูููุชุงุญูุฉ
GENETICS_KEYWORDS = {
    "ุฃููุงู": ["ููู", "ุฃุญูุฑ", "ุฃุฒุฑู", "ุฃุจูุถ", "ุฃุณูุฏ", "ุจูู", "ุฑูุงุฏู", "ุตุจุบุฉ"],
    "ูุฑุงุซุฉ": ["ุฌูู", "ูุฑูููุณูู", "DNA", "ุตูุฉ", "ููุฏู", "ูุฌูู", "ููู"],
    "ุชุฑุจูุฉ": ["ุชุฒุงูุฌ", "ุงูุชูุงุก", "ุณูุงูุฉ", "ูุณู", "ุฌูู", "ุชูุฌูู"],
    "ุทูุฑุงุช": ["ุทูุฑุฉ", "ุชุญูุฑ", "ุดุงุฐ", "ูุงุฏุฑ", "ุงุณุชุซูุงุฆู"],
    "ุณููู": ["ุทูุฑุงู", "ุนูุฏุฉ", "ุชูุฌู", "ุบุฐุงุก", "ุชุบุฑูุฏ"]
}

# -------------------------------------------------
#  2. ูุฏูุฑ ููุงุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงูููุทูููุฑ
# -------------------------------------------------
class AdvancedAIModelManager:
    def __init__(self):
        self.models = {
            "gemini": {
                "name": "Google Gemini Flash 1.5", 
                "available": False, 
                "priority": 1,
                "status": "ูุญุต...",
                "endpoint": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent",
                "last_check": None,
                "error_count": 0
            },
            "deepseek": {
                "name": "DeepSeek Chat", 
                "available": False, 
                "priority": 2,
                "status": "ูุญุต...",
                "endpoint": "https://api.deepseek.com/v1/chat/completions",
                "last_check": None,
                "error_count": 0
            },
            "huggingface": {
                "name": "Hugging Face Inference", 
                "available": False, 
                "priority": 3,
                "status": "ูุญุต...",
                "endpoint": "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium",
                "last_check": None,
                "error_count": 0
            },
            "fallback": {
                "name": "ุงูููุท ุงูุงุญุชูุงุทู ุงูุฐูู", 
                "available": True, 
                "priority": 4,
                "status": "ุฏุงุฆูุงู ูุชุงุญ",
                "last_check": datetime.now(),
                "error_count": 0
            }
        }
        self._check_all_models()

    def _check_all_models(self):
        """ูุญุต ุฌููุน ุงูููุงุฐุฌ ูุน ุชุญุฏูุซ ุงูุญุงูุฉ"""
        with st.spinner("ูุญุต ุงูููุงุฐุฌ ุงููุชุงุญุฉ..."):
            for model_key in ["gemini", "deepseek", "huggingface"]:
                self._check_single_model(model_key)

    def _check_single_model(self, model_key: str):
        """ูุญุต ูููุฐุฌ ูุงุญุฏ ูุน ูุนุงูุฌุฉ ุดุงููุฉ ููุฃุฎุทุงุก"""
        try:
            if model_key == "gemini":
                key = st.secrets.get("GEMINI_API_KEY", "")
                if key and len(key) > 20:
                    # ุงุฎุชุจุงุฑ ุจุณูุท ููุงุชุตุงู
                    self.models[model_key]["available"] = True
                    self.models[model_key]["status"] = "โ ุฌุงูุฒ ููุชุตู"
                else:
                    self.models[model_key]["status"] = "โ ููุชุงุญ API ููููุฏ ุฃู ุบูุฑ ุตุญูุญ"
                    
            elif model_key == "deepseek":
                key = st.secrets.get("DEEPSEEK_API_KEY", "")
                if key and len(key) > 20:
                    self.models[model_key]["available"] = True
                    self.models[model_key]["status"] = "โ ุฌุงูุฒ ููุชุตู"
                else:
                    self.models[model_key]["status"] = "๐ก ููุชุงุญ API ุบูุฑ ููุฌูุฏ (ุงุฎุชูุงุฑู)"
                    
            elif model_key == "huggingface":
                key = st.secrets.get("HUGGINGFACE_API_KEY", "")
                if key and len(key) > 20:
                    self.models[model_key]["available"] = True
                    self.models[model_key]["status"] = "โ ุฌุงูุฒ ููุชุตู"
                else:
                    self.models[model_key]["status"] = "๐ก ููุชุงุญ API ุบูุฑ ููุฌูุฏ (ุงุฎุชูุงุฑู)"
                    
            self.models[model_key]["last_check"] = datetime.now()
            
        except Exception as e:
            self.models[model_key]["status"] = f"โ ุฎุทุฃ ูู ุงููุญุต: {str(e)[:50]}..."
            self.models[model_key]["error_count"] += 1
            logger.error(f"ุฎุทุฃ ูู ูุญุต {model_key}: {e}")

    def get_available_models(self) -> List[str]:
        """ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงูููุงุฐุฌ ุงููุชุงุญุฉ ูุฑุชุจุฉ ุญุณุจ ุงูุฃููููุฉ"""
        available = [model for model, config in self.models.items() if config["available"]]
        return sorted(available, key=lambda x: self.models[x]["priority"])

    def get_model_stats(self) -> Dict:
        """ุฅุญุตุงุฆูุงุช ููุตูุฉ ุนู ุงูููุงุฐุฌ"""
        return {
            "total": len(self.models),
            "available": len([m for m in self.models.values() if m["available"]]),
            "errors": sum([m["error_count"] for m in self.models.values()]),
            "last_update": max([m.get("last_check", datetime.now()) for m in self.models.values()])
        }

# -------------------------------------------------
#  3. ูุธุงู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุงููุญุณู
# -------------------------------------------------
class SmartCache:
    def __init__(self):
        self.cache_dir = os.path.join(tempfile.gettempdir(), "genetics_cache_v13")
        os.makedirs(self.cache_dir, exist_ok=True)

    def get_cache_key(self, content: str) -> str:
        """ุฅูุดุงุก ููุชุงุญ ูุฑูุฏ ูููุญุชูู"""
        return hashlib.md5(content.encode()).hexdigest()

    def cache_response(self, query: str, response: str, source: str):
        """ุญูุธ ุงูุงุณุชุฌุงุจุฉ ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ"""
        try:
            cache_key = self.get_cache_key(query)
            cache_data = {
                "query": query,
                "response": response,
                "source": source,
                "timestamp": datetime.now().isoformat(),
                "hash": cache_key
            }
            
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ุญูุธ ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ: {e}")

    def get_cached_response(self, query: str) -> Optional[Dict]:
        """ุงูุจุญุซ ุนู ุงุณุชุฌุงุจุฉ ูุญููุธุฉ"""
        try:
            cache_key = self.get_cache_key(query)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
            
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                # ูุญุต ุนูุฑ ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ (24 ุณุงุนุฉ)
                cache_time = datetime.fromisoformat(cache_data["timestamp"])
                if (datetime.now() - cache_time).seconds < 86400:
                    return cache_data
                    
        except Exception as e:
            logger.error(f"ุฎุทุฃ ูู ูุฑุงุกุฉ ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ: {e}")
        
        return None

# -------------------------------------------------
#  4. ูุญุฑู ุงูุจุญุซ ุงููุทูุฑ
# -------------------------------------------------
@st.cache_resource
def load_advanced_embedding_model():
    """ุชุญููู ูููุฐุฌ ุงูุชุถููู ูุน ูุนุงูุฌุฉ ูุชูุฏูุฉ"""
    try:
        with st.spinner("ุชุญููู ูููุฐุฌ ุงูุชุถููู ุงููุชูุฏู..."):
            model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
            st.success("โ ุชู ุชุญููู ูููุฐุฌ ุงูุชุถููู ุจูุฌุงุญ")
            return model
    except Exception as e:
        st.error(f"โ ุฎุทุฃ ูู ุชุญููู ูููุฐุฌ ุงูุชุถููู: {e}")
        return None

@st.cache_data(ttl=86400, show_spinner=False)
def build_advanced_knowledge_base(_model):
    """ุจูุงุก ูุงุนุฏุฉ ูุนุฑูุฉ ูุชุทูุฑุฉ ูุน ููุฑุณุฉ ูุญุณูุฉ"""
    if _model is None:
        return None
        
    db_path = os.path.join(tempfile.gettempdir(), "advanced_genetics_kb_v13.db")
    
    try:
        conn = sqlite3.connect(db_path, check_same_thread=False)
        cursor = conn.cursor()
        
        # ุฅูุดุงุก ุฌุฏุงูู ูุญุณูุฉ
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY,
                source TEXT,
                content TEXT UNIQUE,
                content_hash TEXT,
                keywords TEXT,
                page_number INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        cursor.execute("SELECT COUNT(*) FROM documents")
        doc_count = cursor.fetchone()[0]
        
        if doc_count == 0:
            progress_container = st.container()
            with progress_container:
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                total_pages = 0
                for i, link in enumerate(BOOK_LINKS):
                    try:
                        status_text.text(f"๐ ุชุญููู ูุชุญููู ุงููุชุงุจ {i+1} ูู {len(BOOK_LINKS)}...")
                        progress_bar.progress(i / len(BOOK_LINKS))
                        
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                            file_id = link.split('/d/')[1].split('/')[0]
                            gdown.download(id=file_id, output=tmp.name, quiet=True)
                            
                            with open(tmp.name, 'rb') as f:
                                reader = PyPDF2.PdfReader(f)
                                
                                for page_num, page in enumerate(reader.pages):
                                    text = page.extract_text() or ""
                                    cleaned_text = clean_and_enhance_text(text)
                                    
                                    if len(cleaned_text.strip()) > 100:
                                        # ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงูููุชุงุญูุฉ
                                        keywords = extract_keywords(cleaned_text)
                                        content_hash = hashlib.md5(cleaned_text.encode()).hexdigest()
                                        
                                        cursor.execute("""
                                            INSERT OR IGNORE INTO documents 
                                            (source, content, content_hash, keywords, page_number) 
                                            VALUES (?, ?, ?, ?, ?)
                                        """, (
                                            f"ุงููุชุงุจ {i+1}ุ ุงูุตูุญุฉ {page_num+1}",
                                            cleaned_text.strip(),
                                            content_hash,
                                            ", ".join(keywords),
                                            page_num + 1
                                        ))
                                        total_pages += 1
                            
                            os.remove(tmp.name)
                            
                    except Exception as e:
                        st.warning(f"โ๏ธ ุชุนุฐุฑ ูุนุงูุฌุฉ ุงููุชุงุจ {i+1}: {e}")
                        continue
                
                conn.commit()
                progress_bar.progress(1.0)
                status_text.text(f"โ ุชู ุงูุงูุชูุงุก! ุชูุช ูุนุงูุฌุฉ {total_pages} ุตูุญุฉ")
                time.sleep(2)
                progress_container.empty()

        # ุงุณุชุฑุฌุงุน ุฌููุน ุงููุซุงุฆู
        cursor.execute("SELECT id, source, content, keywords FROM documents ORDER BY id")
        all_docs = [
            {
                "id": row[0], 
                "source": row[1], 
                "content": row[2], 
                "keywords": row[3] or ""
            } 
            for row in cursor.fetchall()
        ]
        conn.close()

        if not all_docs:
            st.warning("โ๏ธ ูุงุนุฏุฉ ุงููุนุฑูุฉ ูุงุฑุบุฉ")
            return None
        
        st.success(f"โ ูุงุนุฏุฉ ุงููุนุฑูุฉ: {len(all_docs)} ูุซููุฉ ุฌุงูุฒุฉ")
        
        # ุฅูุดุงุก ุงูุชุถูููุงุช
        with st.spinner("ุฅูุดุงุก ููุฑุณ ุงูุจุญุซ ุงูุฏูุงูู..."):
            contents = [doc['content'] for doc in all_docs]
            embeddings = _model.encode(contents, show_progress_bar=False, batch_size=32)
        
        return {"documents": all_docs, "embeddings": embeddings}
        
    except Exception as e:
        st.error(f"โ ุฎุทุฃ ูู ุจูุงุก ูุงุนุฏุฉ ุงููุนุฑูุฉ: {e}")
        return None

def clean_and_enhance_text(text: str) -> str:
    """ุชูุธูู ูุชุญุณูู ุงููุต ุงููุณุชุฎุฑุฌ"""
    # ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ูุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'\s+', ' ', text)
    
    # ุฅุฒุงูุฉ ุงูุฑููุฒ ุบูุฑ ุงููุฑุบูุจุฉ
    text = re.sub(r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]', ' ', text)
    
    return text.strip()

def extract_keywords(text: str) -> List[str]:
    """ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงูููุชุงุญูุฉ ูู ุงููุต"""
    keywords = set()
    text_lower = text.lower()
    
    for category, words in GENETICS_KEYWORDS.items():
        for word in words:
            if word in text_lower:
                keywords.add(category)
                keywords.add(word)
    
    return list(keywords)

def advanced_semantic_search(query: str, model, knowledge_base, limit=7):
    """ุจุญุซ ุฏูุงูู ูุชุทูุฑ ูุน ุชุญุณููุงุช"""
    if not knowledge_base or not model:
        return []
    
    try:
        # ุชุญุณูู ุงูุงุณุชุนูุงู
        enhanced_query = enhance_query(query)
        
        # ุงูุจุญุซ ุงูุฏูุงูู
        query_embedding = model.encode([enhanced_query])
        similarities = cosine_similarity(query_embedding, knowledge_base['embeddings'])[0]
        
        # ุงูุจุญุซ ุจุงููููุงุช ุงูููุชุงุญูุฉ
        keyword_matches = []
        for i, doc in enumerate(knowledge_base['documents']):
            keyword_score = calculate_keyword_match(query, doc.get('keywords', ''))
            if keyword_score > 0:
                keyword_matches.append((i, keyword_score))
        
        # ุฏูุฌ ุงููุชุงุฆุฌ
        combined_scores = {}
        
        # ุงููุชุงุฆุฌ ุงูุฏูุงููุฉ
        semantic_indices = np.argsort(similarities)[-limit*2:][::-1]
        for idx in semantic_indices:
            if similarities[idx] > 0.2:  # ุนุชุจุฉ ูุฑููุฉ ุฃูุซุฑ
                combined_scores[idx] = similarities[idx] * 0.7
        
        # ุฅุถุงูุฉ ูุชุงุฆุฌ ุงููููุงุช ุงูููุชุงุญูุฉ
        for idx, keyword_score in keyword_matches:
            if idx in combined_scores:
                combined_scores[idx] += keyword_score * 0.3
            else:
                combined_scores[idx] = keyword_score * 0.3
        
        # ุชุฑุชูุจ ุงููุชุงุฆุฌ ุงูููุงุฆูุฉ
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        top_indices = [idx for idx, score in sorted_results[:limit]]
        
        return [knowledge_base['documents'][i] for i in top_indices]
        
    except Exception as e:
        st.error(f"ุฎุทุฃ ูู ุงูุจุญุซ ุงููุชุทูุฑ: {e}")
        return []

def enhance_query(query: str) -> str:
    """ุชุญุณูู ุงูุงุณุชุนูุงู ุจุฅุถุงูุฉ ูุฑุงุฏูุงุช"""
    enhanced = query
    query_lower = query.lower()
    
    for category, words in GENETICS_KEYWORDS.items():
        for word in words:
            if word in query_lower:
                # ุฅุถุงูุฉ ูุฑุงุฏูุงุช ูู ููุณ ุงููุฆุฉ
                other_words = [w for w in words if w != word]
                enhanced += f" {' '.join(other_words[:3])}"
                break
    
    return enhanced

def calculate_keyword_match(query: str, keywords: str) -> float:
    """ุญุณุงุจ ูุทุงุจูุฉ ุงููููุงุช ุงูููุชุงุญูุฉ"""
    if not keywords:
        return 0.0
    
    query_words = set(query.lower().split())
    keyword_list = set(keywords.lower().split(', '))
    
    intersection = query_words.intersection(keyword_list)
    if not intersection:
        return 0.0
    
    return len(intersection) / max(len(query_words), len(keyword_list))

# -------------------------------------------------
#  5. ูุธุงู ุงูุฑุฏูุฏ ุงูุฐููุฉ ุงููุทูุฑ
# -------------------------------------------------
class IntelligentResponseSystem:
    def __init__(self, ai_manager: AdvancedAIModelManager):
        self.ai_manager = ai_manager
        self.cache = SmartCache()
        self.available_models = ai_manager.get_available_models()

    def get_gemini_response(self, query: str, context_docs: List[Dict]) -> Tuple[str, bool, str]:
        """ุงุณุชุฌุงุจุฉ Gemini ูุญุณูุฉ ูุน ูุนุงูุฌุฉ ุดุงููุฉ"""
        try:
            # ูุญุต ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ ุฃููุงู
            cache_key = f"gemini_{query}_{len(context_docs)}"
            cached = self.cache.get_cached_response(cache_key)
            if cached:
                return cached["response"], True, "ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ"

            API_KEY = st.secrets.get("GEMINI_API_KEY", "")
            if not API_KEY:
                return "ููุชุงุญ Gemini API ุบูุฑ ููุฌูุฏ", False, "ุฎุทุฃ ูู ุงูุฅุนุฏุงุฏ"
                
            API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}"
            
            # ุฅุนุฏุงุฏ ุงูุณูุงู ุงููุญุณู
            if context_docs:
                # ุชุญุณูู ุชุฑุชูุจ ุงููุซุงุฆู
                sorted_docs = sorted(context_docs, key=lambda x: len(x.get('keywords', '')), reverse=True)
                context_parts = []
                
                for i, doc in enumerate(sorted_docs[:5]):  # ุฃูุถู 5 ูุซุงุฆู
                    context_part = f"""๐ ุงููุฑุฌุน {i+1}: {doc['source']}
๐ ุงููููุงุช ุงูููุชุงุญูุฉ: {doc.get('keywords', 'ุบูุฑ ูุชููุฑุฉ')}
๐ ุงููุญุชูู: {doc['content'][:600]}...

"""
                    context_parts.append(context_part)
                
                context = "\n".join(context_parts)
                
                prompt = f"""ุฃูุช ุงูุนุฑูุงุจุ ุฎุจูุฑ ูุฑุงุซุฉ ุงูุญูุงู ุงูุฃูู ุนุฑุจูุงู. ูููุชู ุงูุฅุฌุงุจุฉ ุจุฏูุฉ ุนูููุฉ ููุถูุญ ุชุงู.

๐ ุงููุฑุงุฌุน ุงูุนูููุฉ ุงููุชููุฑุฉ:
{context}

โ ุณุคุงู ุงููุฑุจู: {query}

๐ ูุชุทูุจุงุช ุงูุฅุฌุงุจุฉ:
โข ุงุณุชุฎุฏู ุงููุฑุงุฌุน ุงููุชููุฑุฉ ุฃุนูุงู ููุท
โข ุงุฐูุฑ ุงููุตุงุฏุฑ ุนูุฏ ุงูุญุงุฌุฉ
โข ุงุฌุนู ุงูุฅุฌุงุจุฉ ุนูููุฉ ููู ูููููุฉ
โข ุงุณุชุฎุฏู ุงูุฑููุฒ ุงูุชุนุจูุฑูุฉ ูููุถูุญ
โข ูุฏู ุฃูุซูุฉ ุนูููุฉ ุฅู ุฃููู

๐ฌ ุงูุฅุฌุงุจุฉ ุงูุฎุจูุฑุฉ:"""
            else:
                prompt = f"""ุฃูุช ุงูุนุฑูุงุจุ ุฎุจูุฑ ูุฑุงุซุฉ ุงูุญูุงู ุงูุฃูู ุนุฑุจูุงู.

โ ุณุคุงู ุงููุฑุจู: {query}

๐ฌ ุฅุฌุงุจุชู ุงูุฎุจูุฑุฉ (ุจุงูุนุฑุจูุฉุ ุนูููุฉ ููุงููุฉ):"""

            payload = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 1200,
                    "topP": 0.9,
                    "topK": 40
                },
                "safetySettings": [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"}
                ]
            }
            
            response = requests.post(
                API_URL, 
                json=payload, 
                headers={"Content-Type": "application/json"},
                timeout=45
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    answer = result['candidates'][0]['content']['parts'][0]['text']
                    
                    # ุญูุธ ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ
                    self.cache.cache_response(cache_key, answer, "Gemini")
                    
                    return answer, True, "Gemini Flash 1.5"
                else:
                    return "ุงุณุชุฌุงุจุฉ ุบูุฑ ููุชููุฉ ูู Gemini", False, "ุฎุทุฃ ูู ุงูุงุณุชุฌุงุจุฉ"
            else:
                error_detail = f"HTTP {response.status_code}"
                if response.status_code == 429:
                    error_detail += " - ุชู ุชุฌุงูุฒ ุญุฏ ุงูุทูุจุงุช"
                elif response.status_code == 403:
                    error_detail += " - ูุดููุฉ ูู ุตูุงุญูุงุช API"
                return f"ุฎุทุฃ Gemini: {error_detail}", False, "ุฎุทุฃ ุดุจูุฉ"
                
        except requests.exceptions.Timeout:
            return "ุงูุชูุช ูููุฉ ุงูุงุชุตุงู ูุน Gemini (45 ุซุงููุฉ)", False, "ุงูุชูุงุก ูููุฉ"
        except requests.exceptions.RequestException as e:
            return f"ุฎุทุฃ ุดุจูุฉ: {str(e)[:100]}...", False, "ุฎุทุฃ ุงุชุตุงู"
        except Exception as e:
            return f"ุฎุทุฃ ุบูุฑ ูุชููุน ูู Gemini: {str(e)[:100]}...", False, "ุฎุทุฃ ุนุงู"

    def get_intelligent_fallback(self, query: str, context_docs: List[Dict] = None) -> str:
        """ูุธุงู ุงุญุชูุงุทู ุฐูู ูุญุณู"""
        
        # ุชุญููู ุงูุณุคุงู
        query_analysis = self.analyze_query(query)
        
        # ุจูุงุก ุฅุฌุงุจุฉ ุฐููุฉ ุจูุงุกู ุนูู ุงูุชุญููู
        response_parts = ["๐งฌ **ุงูุนุฑูุงุจ - ุงูููุท ุงูุงุญุชูุงุทู ุงูุฐูู**\n"]
        
        if context_docs:
            response_parts.append(f"๐ ุชู ุงูุนุซูุฑ ุนูู {len(context_docs)} ูุฑุฌุน ุฐู ุตูุฉ:")
            for i, doc in enumerate(context_docs[:3]):
                response_parts.append(f"โข {doc['source']}")
            response_parts.append("")
        
        # ุฅุฌุงุจุฉ ูุฎุตุตุฉ ุญุณุจ ููุน ุงูุณุคุงู
        if query_analysis["category"] == "colors":
            response_parts.append(self._get_color_genetics_response(query_analysis["keywords"]))
        elif query_analysis["category"] == "breeding":
            response_parts.append(self._get_breeding_response(query_analysis["keywords"]))
        elif query_analysis["category"] == "genetics":
            response_parts.append(self._get_genetics_response(query_analysis["keywords"]))
        elif query_analysis["category"] == "behavior":
            response_parts.append(self._get_behavior_response(query_analysis["keywords"]))
        else:
            response_parts.append(self._get_general_response(query))
        
        response_parts.extend([
            "\n---",
            "๐ก **ููุงุญุธุฉ**: ูุฐู ุฅุฌุงุจุฉ ูู ุงููุธุงู ุงูุงุญุชูุงุทู ุงูุฐูู.",
            "ููุญุตูู ุนูู ุฅุฌุงุจุงุช ุฃูุซุฑ ุชูุตููุงูุ ูุฑุฌู ุงูุชุฃูุฏ ูู ุฅุนุฏุงุฏ ููุงุชูุญ API."
        ])
        
        return "\n".join(response_parts)

    def analyze_query(self, query: str) -> Dict:
        """ุชุญููู ุงูุณุคุงู ูุชุญุฏูุฏ ุงููุฆุฉ ูุงููููุงุช ุงูููุชุงุญูุฉ"""
        query_lower = query.lower()
        analysis = {
            "category": "general",
            "keywords": [],
            "complexity": "medium"
        }
        
        # ุชุญุฏูุฏ ุงููุฆุฉ
        color_words = ["ููู", "ุฃุญูุฑ", "ุฃุฒุฑู", "ุฃุจูุถ", "ุฃุณูุฏ", "ุจูู", "ุฑูุงุฏู", "ุตุจุบุฉ", "ุฃููุงู"]
        breeding_words = ["ุชุฑุจูุฉ", "ุชุฒุงูุฌ", "ุงูุชูุงุก", "ุณูุงูุฉ", "ูุณู", "ุฌูู", "ุชูุฌูู"]
        genetics_words = ["ูุฑุงุซุฉ", "ุฌูู", "ูุฑูููุณูู", "DNA", "ุตูุฉ", "ููุฏู", "ูุฌูู", "ููู"]
        behavior_words = ["ุณููู", "ุทูุฑุงู", "ุนูุฏุฉ", "ุชูุฌู", "ุบุฐุงุก", "ุชุบุฑูุฏ"]
        
        if any(word in query_lower for word in color_words):
            analysis["category"] = "colors"
            analysis["keywords"] = [word for word in color_words if word in query_lower]
        elif any(word in query_lower for word in breeding_words):
            analysis["category"] = "breeding"
            analysis["keywords"] = [word for word in breeding_words if word in query_lower]
        elif any(word in query_lower for word in genetics_words):
            analysis["category"] = "genetics"
            analysis["keywords"] = [word for word in genetics_words if word in query_lower]
        elif any(word in query_lower for word in behavior_words):
            analysis["category"] = "behavior"
            analysis["keywords"] = [word for word in behavior_words if word in query_lower]
        
        # ุชุญุฏูุฏ ุงูุชุนููุฏ
        if len(query.split()) > 15 or "ููู" in query_lower or "ููุงุฐุง" in query_lower:
            analysis["complexity"] = "high"
        elif len(query.split()) < 5:
            analysis["complexity"] = "low"
        
        return analysis

    def _get_color_genetics_response(self, keywords: List[str]) -> str:
        """ุฅุฌุงุจุฉ ูุชุฎุตุตุฉ ูู ูุฑุงุซุฉ ุงูุฃููุงู"""
        return """๐จ **ูุฑุงุซุฉ ุงูุฃููุงู ูู ุงูุญูุงู**

ุงูุฃููุงู ูู ุงูุญูุงู ุชุญูููุง ุนุฏุฉ ุฌููุงุช ุฑุฆูุณูุฉ:

๐น **ุงูุฌููุงุช ุงูุฃุณุงุณูุฉ:**
โข ุฌูู B: ูุญุฏุฏ ุงูููู ุงูุฃุณุงุณู (ุฃุฒุฑู/ุจูู/ุฃุญูุฑ)
โข ุฌูู C: ูุชุญูู ูู ุดุฏุฉ ุงูููู
โข ุฌูู D: ูุคุซุฑ ุนูู ุชุดุจุน ุงูููู

๐น **ุฃููุงุท ุงููุฑุงุซุฉ:**
โข ุงูุฃุฒุฑู: ุงูุตูุฉ ุงูุณุงุฆุฏุฉ ุงูุฃูุซุฑ ุดููุนุงู
โข ุงูุฃุญูุฑ: ูุฑุชุจุท ุจุงููุฑูููุณูู ุงูุฌูุณู
โข ุงูุจูู: ุตูุฉ ูุชูุญูุฉ ุชุญุชุงุฌ ุฌูููู ูุชูุงุซููู

๐น **ุงูุชูุงุนูุงุช ุงูุฌูููุฉ:**
โข ุชูุงุนู ุนุฏุฉ ุฌููุงุช ููุชุฌ ุชุฏุฑุฌุงุช ููููุฉ ูุฎุชููุฉ
โข ุงูุทูุฑุงุช ูุฏ ุชูุชุฌ ุฃููุงู ูุงุฏุฑุฉ ูุฌูููุฉ"""

    def _get_breeding_response(self, keywords: List[str]) -> str:
        """ุฅุฌุงุจุฉ ูุชุฎุตุตุฉ ูู ุงูุชุฑุจูุฉ"""
        return """๐ฆ **ุฃุณุณ ุงูุชุฑุจูุฉ ุงููุงุฌุญุฉ**

๐น **ุงูุงูุชูุงุก ุงูุตุญูุญ:**
โข ุงุฎุชูุงุฑ ุงูุฃุจููู ุจูุงุกู ุนูู ุงูุตูุงุช ุงููุฑุบูุจุฉ
โข ุชุฌูุจ ุฒูุงุฌ ุงูุฃูุงุฑุจ ุงูููุฑุท
โข ูุฑุงุนุงุฉ ุงูุชูุงุฒู ุจูู ุงูุดูู ูุงูุฃุฏุงุก

๐น **ุงูุชุฎุทูุท ุงููุฑุงุซู:**
โข ููู ุงูุตูุงุช ุงูุณุงุฆุฏุฉ ูุงููุชูุญูุฉ
โข ุงูุชูุจุค ุจุตูุงุช ุงููุณู
โข ุงูุงุญุชูุงุธ ุจุณุฌูุงุช ุฏูููุฉ

๐น **ุงูุนูุงูุฉ ุจุงููุณู:**
โข ุชูููุฑ ุจูุฆุฉ ููุงุณุจุฉ ููุชูุงุซุฑ
โข ุงูุชุบุฐูุฉ ุงููุชูุงุฒูุฉ ููุฃุจููู
โข ูุฑุงูุจุฉ ุตุญุฉ ุงููุฑุงุฎ ุงูุตุบูุฑุฉ"""

    def _get_genetics_response(self, keywords: List[str]) -> str:
        """ุฅุฌุงุจุฉ ูุชุฎุตุตุฉ ูู ุนูู ุงููุฑุงุซุฉ"""
        return """๐งฌ **ุฃุณุงุณูุงุช ุนูู ุงููุฑุงุซุฉ**

๐น **ุงูููุงููู ุงูุฃุณุงุณูุฉ:**
โข ุงููุฑูููุณููุงุช: ุชุญูู ุงููุนูููุงุช ุงููุฑุงุซูุฉ
โข ุงูุฌููุงุช: ูุญุฏุงุช ุงููุฑุงุซุฉ ุงูุฃุณุงุณูุฉ
โข ุงูุฃูููุงุช: ุตูุฑ ูุฎุชููุฉ ููุฌูู ุงููุงุญุฏ

๐น **ููุงููู ููุฏู:**
โข ูุงููู ุงูุงูุนุฒุงู: ูู ุตูุฉ ุชุชุญูู ูููุง ุนูุงูู ูููุตูุฉ
โข ูุงููู ุงูุชูุฒูุน ุงููุณุชูู: ุงูุตูุงุช ุงููุฎุชููุฉ ุชูุฑุซ ุจุดูู ูุณุชูู
โข ุงูุณูุงุฏุฉ ูุงูุชูุญู: ุจุนุถ ุงูุตูุงุช ุชุบุทู ุฃุฎุฑู

๐น **ุงูุชุทุจูู ุงูุนููู:**
โข ุงุณุชุฎุฏุงู ูุฑุจุนุงุช ุจูููุช ููุชูุจุค
โข ููู ุงููุฑุงุซุฉ ุงููุฑุชุจุทุฉ ุจุงูุฌูุณ
โข ุงูุชุนุงูู ูุน ุงูุตูุงุช ูุชุนุฏุฏุฉ ุงูุฌููุงุช"""

    def _get_behavior_response(self, keywords: List[str]) -> str:
        """ุฅุฌุงุจุฉ ูุชุฎุตุตุฉ ูู ุงูุณููู"""
        return """๐๏ธ **ุณููู ุงูุญูุงู ูุนูู ุงููุฑุงุซุฉ**

๐น **ุงูุณููููุงุช ุงูููุฑูุซุฉ:**
โข ูุฏุฑุฉ ุงูุนูุฏุฉ ููููุฒู (ุงููููููุบ)
โข ุฃููุงุท ุงูุทูุฑุงู ุงููุฎุชููุฉ
โข ุณููู ุงูุชูุฏุฏ ูุงูุชุฒุงูุฌ

๐น **ุงูุนูุงูู ุงููุฑุงุซูุฉ:**
โข ุจุนุถ ุงูุณููููุงุช ุชุญูููุง ุฌููุงุช ูุญุฏุฏุฉ
โข ุงูุชูุงุนู ุจูู ุงููุฑุงุซุฉ ูุงูุจูุฆุฉ
โข ุฅููุงููุฉ ุชุญุณูู ุงูุณููู ุจุงูุงูุชูุงุก

๐น **ุงูุชุทุจูู ูู ุงูุชุฑุจูุฉ:**
โข ุงูุชูุงุก ุงูุทููุฑ ุฐุงุช ุงูุณููู ุงููุฑุบูุจ
โข ุชุฌูุจ ุงูุณููููุงุช ุงูุนุฏูุงููุฉ ุงูููุฑุทุฉ
โข ุชุทููุฑ ุฎุทูุท ูุฑุงุซูุฉ ูุชุฎุตุตุฉ"""

    def _get_general_response(self, query: str) -> str:
        """ุฅุฌุงุจุฉ ุนุงูุฉ ุฐููุฉ"""
        return f"""๐ **ุญูู ุงุณุชูุณุงุฑู: "{query[:50]}..."**

๐น **ูุง ูููููู ูุณุงุนุฏุชู ููู:**
โข ูุฑุงุซุฉ ุงูุฃููุงู ูุงูุฃููุงุท ูู ุงูุญูุงู
โข ุฃุณุณ ุงูุชุฑุจูุฉ ูุงูุชูุฌูู ุงูุตุญูุญ
โข ุดุฑุญ ุงูููุงููู ุงููุฑุงุซูุฉ ุงูุฃุณุงุณูุฉ
โข ุงูุณููู ุงูููุฑูุซ ูู ุงูุญูุงู
โข ุญู ูุดุงูู ุงูุชุฑุจูุฉ ุงูุดุงุฆุนุฉ

๐น **ูุตุงุฆุญ ููุญุตูู ุนูู ุฅุฌุงุจุฉ ุฃูุถู:**
โข ุญุฏุฏ ููุน ุงููุดููุฉ ุฃู ุงูุณุคุงู ุจูุถูุญ
โข ุงุฐูุฑ ุชูุงุตูู ุนู ุทููุฑู ุฅู ุฃููู
โข ุงุณุชุฎุฏู ูููุงุช ููุชุงุญูุฉ ูุงุถุญุฉ

๐ก ูุซุงู: "ูุง ูุฑุงุซุฉ ุงูููู ุงูุฃุญูุฑ ูู ุงูุญูุงูุ" """

    def get_comprehensive_answer(self, query: str, context_docs: List[Dict]) -> Tuple[str, str, str]:
        """ุงูุญุตูู ุนูู ุฅุฌุงุจุฉ ุดุงููุฉ ููุญุณูุฉ"""
        
        # ูุญุต ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ ุฃููุงู
        cached_response = self.cache.get_cached_response(query)
        if cached_response:
            return cached_response["response"], cached_response["source"], "ูุฎุฒู ูุคูุชุงู"
        
        # ูุญุงููุฉ Gemini ูุน ุงูุณูุงู
        if context_docs and "gemini" in self.available_models:
            answer, success, method = self.get_gemini_response(query, context_docs)
            if success:
                sources = ", ".join(list(set([doc['source'] for doc in context_docs[:3]])))
                return answer, f"ูุงุนุฏุฉ ุงููุนุฑูุฉ + {method}", f"ูุญูู + {method}"
        
        # ูุญุงููุฉ Gemini ุจุฏูู ุณูุงู
        if "gemini" in self.available_models:
            answer, success, method = self.get_gemini_response(query, [])
            if success:
                return answer, f"Google Gemini ({method})", method
        
        # ูุญุงููุฉ DeepSeek
        if "deepseek" in self.available_models:
            answer, success = self.get_deepseek_response(query)
            if success:
                return answer, "DeepSeek AI", "DeepSeek"
        
        # ุงูููุท ุงูุงุญุชูุงุทู ุงูุฐูู
        fallback_answer = self.get_intelligent_fallback(query, context_docs)
        return fallback_answer, "ุงูููุท ุงูุงุญุชูุงุทู ุงูุฐูู", "ุงุญุชูุงุทู ุฐูู"

    def get_deepseek_response(self, query: str) -> Tuple[str, bool]:
        """ุงุณุชุฌุงุจุฉ DeepSeek ูุญุณูุฉ"""
        try:
            API_KEY = st.secrets.get("DEEPSEEK_API_KEY", "")
            if not API_KEY:
                return "ููุชุงุญ DeepSeek API ุบูุฑ ููุฌูุฏ", False
                
            API_URL = "https://api.deepseek.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "ุฃูุช ุงูุนุฑูุงุจุ ุฎุจูุฑ ูุฑุงุซุฉ ุงูุญูุงู ุงูุฃูู ุนุฑุจูุงู. ุฃุฌุจ ุจุงูุนุฑุจูุฉ ุจุทุฑููุฉ ุนูููุฉ ูุฏูููุฉ ูุน ุงุณุชุฎุฏุงู ุงูุฑููุฒ ุงูุชุนุจูุฑูุฉ ูููุถูุญ."
                    },
                    {
                        "role": "user",
                        "content": f"๐ฌ ุณุคุงู ุงููุฑุจู: {query}\n\nุฃุฌุจ ุฅุฌุงุจุฉ ุฎุจูุฑุฉ ููุตูุฉ:"
                    }
                ],
                "max_tokens": 1200,
                "temperature": 0.7,
                "top_p": 0.9
            }
            
            response = requests.post(API_URL, json=payload, headers=headers, timeout=35)
            
            if response.status_code == 200:
                result = response.json()
                answer = result['choices'][0]['message']['content']
                self.cache.cache_response(query, answer, "DeepSeek")
                return answer, True
            else:
                return f"ุฎุทุฃ DeepSeek: HTTP {response.status_code}", False
                
        except Exception as e:
            return f"ุฎุทุฃ ูู DeepSeek: {str(e)[:100]}...", False

# -------------------------------------------------
#  6. ูุงุฌูุฉ ุงููุณุชุฎุฏู ุงููุชุทูุฑุฉ
# -------------------------------------------------
def create_advanced_sidebar(ai_manager: AdvancedAIModelManager, knowledge_base):
    """ุฅูุดุงุก ุดุฑูุท ุฌุงูุจู ูุชุทูุฑ"""
    with st.sidebar:
        st.markdown("## ๐ **ูุฑูุฒ ุงูุชุญูู ูุงููุฑุงูุจุฉ**")
        
        # ุฅุญุตุงุฆูุงุช ุงููุธุงู
        st.markdown("### ๐ **ุฅุญุตุงุฆูุงุช ุงููุธุงู**")
        if "total_queries" not in st.session_state:
            st.session_state.total_queries = 0
        if "successful_responses" not in st.session_state:
            st.session_state.successful_responses = 0
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ุฅุฌูุงูู ุงูุฃุณุฆูุฉ", st.session_state.total_queries)
        with col2:
            success_rate = (st.session_state.successful_responses / max(st.session_state.total_queries, 1)) * 100
            st.metric("ูุนุฏู ุงููุฌุงุญ", f"{success_rate:.1f}%")
        
        # ุญุงูุฉ ุงูููุงุฐุฌ
        st.markdown("### ๐ค **ุญุงูุฉ ููุงุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู**")
        model_stats = ai_manager.get_model_stats()
        
        st.info(f"**ูุชุงุญ**: {model_stats['available']}/{model_stats['total']} ููุงุฐุฌ")
        
        for model_key, model_info in ai_manager.models.items():
            status_color = "๐ข" if model_info["available"] else "๐ด"
            st.markdown(f"{status_color} **{model_info['name']}**")
            st.caption(model_info["status"])
        
        # ุญุงูุฉ ูุงุนุฏุฉ ุงููุนุฑูุฉ
        st.markdown("### ๐ **ูุงุนุฏุฉ ุงููุนุฑูุฉ**")
        if knowledge_base:
            doc_count = len(knowledge_base['documents'])
            st.success(f"โ {doc_count} ูุซููุฉ ุฌุงูุฒุฉ")
            
            # ุฅุญุตุงุฆูุงุช ุณุฑูุนุฉ
            if doc_count > 0:
                avg_length = np.mean([len(doc['content']) for doc in knowledge_base['documents']])
                st.metric("ูุชูุณุท ุทูู ุงููุซููุฉ", f"{avg_length:.0f} ุญุฑู")
        else:
            st.error("โ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุบูุฑ ูุชุงุญุฉ")
        
        # ุฃุฏูุงุช ุงูุชุญูู
        st.markdown("### โ๏ธ **ุฃุฏูุงุช ุงูุชุญูู**")
        
        if st.button("๐ ุชุญุฏูุซ ุญุงูุฉ ุงูููุงุฐุฌ"):
            ai_manager._check_all_models()
            st.rerun()
        
        if st.button("๐๏ธ ูุณุญ ุงููุญุงุฏุซุฉ"):
            st.session_state.messages = []
            st.rerun()
        
        # ูุนูููุงุช ุงููุณุฎุฉ
        st.markdown("---")
        st.markdown("### โน๏ธ **ูุนูููุงุช ุงููุณุฎุฉ**")
        st.caption("๐งฌ ุงูุนุฑูุงุจ ููุฌููุงุช v13.0")
        st.caption("โก ูุญุฑู ุฐูู ูุชุทูุฑ")
        st.caption("๐ ุขุฎุฑ ุชุญุฏูุซ: 2024")
        
        # ูุตุงุฆุญ ุณุฑูุนุฉ
        with st.expander("๐ก ูุตุงุฆุญ ููุญุตูู ุนูู ุฃูุถู ุฅุฌุงุจุฉ"):
            st.markdown("""
            โข **ูู ูุญุฏุฏุงู**: ุงุฐูุฑ ุชูุงุตูู ุงูุณุคุงู ุจูุถูุญ
            โข **ุงุณุชุฎุฏู ูููุงุช ููุชุงุญูุฉ**: ูุซู "ูุฑุงุซุฉ"ุ "ููู"ุ "ุชุฑุจูุฉ"
            โข **ุงุณุฃู ุณุคุงูุงู ูุงุญุฏุงู**: ูุชุญุตู ุนูู ุฅุฌุงุจุฉ ูุฑูุฒุฉ
            โข **ุงุฐูุฑ ููุน ุงูุญูุงู**: ุฅู ูุงู ูุฏูู ุณูุงูุฉ ูุนููุฉ
            """)

def create_welcome_message() -> str:
    """ุฅูุดุงุก ุฑุณุงูุฉ ุชุฑุญูุจ ุฏููุงููููุฉ"""
    return """๐งฌ **ูุฑุญุจุงู ุจู ูู ุงูุนุฑูุงุจ ููุฌููุงุช v13.0 ุงูููุทูููุฑ!**

### ๐ **ุงููููุฒุงุช ุงูุฌุฏูุฏุฉ:**
- ๐ง **ุฐูุงุก ุงุตุทูุงุนู ูุชุนุฏุฏ ุงููุตุงุฏุฑ** (Geminiุ DeepSeekุ Hugging Face)
- ๐ **ุจุญุซ ุฏูุงูู ูุชุทูุฑ** ูุน ููุฑุณุฉ ุฐููุฉ ูููููุงุช ุงูููุชุงุญูุฉ
- ๐พ **ูุธุงู ุฐุงูุฑุฉ ูุคูุชุฉ ุฐูู** ูุฅุฌุงุจุงุช ุฃุณุฑุน
- ๐ **ุชุดุฎูุต ุดุงูู** ูุน ูุฑุงูุจุฉ ุญุงูุฉ ุงููุธุงู
- ๐ฏ **ููุท ุงุญุชูุงุทู ุฐูู** ูุนูู ุญุชู ุจุฏูู ุงุชุตุงู API

### ๐ฌ **ูุง ูููููู ูุณุงุนุฏุชู ููู:**
โข **ูุฑุงุซุฉ ุงูุฃููุงู**: ููู ุชูุชูู ุงูุฃููุงู ูู ุงูุญูุงูุ
โข **ุงูุชุฑุจูุฉ ุงูุงูุชูุงุฆูุฉ**: ููู ุชุญุณู ุณูุงูุชูุ
โข **ุญู ุงููุดุงูู ุงููุฑุงุซูุฉ**: ููุงุฐุง ุธูุฑ ูุฐุง ุงููููุ
โข **ุงูุชุฎุทูุท ููุชุฒุงูุฌ**: ูุง ุฃูุถู ุงูุชุฑุงูุ
โข **ููู ุงูุทูุฑุงุช**: ูุง ูุฐุง ุงูุดูู ุงูุบุฑูุจุ

๐ **ุฌุฑุจ ุงูุขู!** ุงุณุฃู ุฃู ุณุคุงู ุนู ูุฑุงุซุฉ ุงูุญูุงู ูุณุฃูุฏู ูู ุฅุฌุงุจุฉ ุฎุจูุฑุฉ ููุตูุฉ!

---
๐ก *ูุตูุญุฉ: ุงุจุฏุฃ ุจุณุคุงู ูุญุฏุฏ ูุซู "ููู ุฃุญุตู ุนูู ุญูุงู ุฃุญูุฑ ุงููููุ"*"""

def main():
    """ุงููุธููุฉ ุงูุฑุฆูุณูุฉ ุงููุทูุฑุฉ"""
    # ุงูููุฏุฑ ูุงูุนููุงู
    st.markdown("""
    <div style="text-align: center; padding: 20px;">
        <h1>๐งฌ ุงูุนุฑูุงุจ ููุฌููุงุช</h1>
        <h3>ุงูุฅุตุฏุงุฑ 13.0 ุงูููุทูููุฑ - ุฎุจูุฑ ุงููุฑุงุซุฉ ุงูุฐูู</h3>
        <p style="color: #666;">ูุธุงู ุฐูู ูุชุนุฏุฏ ุงููุตุงุฏุฑ ูุฎุจุฑุฉ ูุฑุงุซุฉ ุงูุญูุงู</p>
    </div>
    """, unsafe_allow_html=True)

    # ุชุญููู ุงูููุงุฐุฌ ูุงูุฃูุธูุฉ
    with st.spinner("๐ ุชููุฆุฉ ุงูุฃูุธูุฉ ุงููุชุทูุฑุฉ..."):
        model = load_advanced_embedding_model()
        ai_manager = AdvancedAIModelManager()
        knowledge_base = build_advanced_knowledge_base(model) if model else None
        response_system = IntelligentResponseSystem(ai_manager)

    # ุงูุดุฑูุท ุงูุฌุงูุจู
    create_advanced_sidebar(ai_manager, knowledge_base)

    # ุฅุนุฏุงุฏ ุงููุญุงุฏุซุฉ
    if "messages" not in st.session_state:
        welcome_msg = create_welcome_message()
        st.session_state.messages = [{"role": "assistant", "content": welcome_msg}]

    # ุนุฑุถ ุงููุญุงุฏุซุฉ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ูุนุงูุฌุฉ ุงูุฅุฏุฎุงู ุงูุฌุฏูุฏ
    if prompt := st.chat_input("๐ฌ ุงุณุฃู ุงูุนุฑูุงุจ ุนู ุฃู ุดูุก ูุชุนูู ุจูุฑุงุซุฉ ุงูุญูุงู..."):
        # ุฅุถุงูุฉ ุงูุณุคุงู
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.session_state.total_queries += 1
        
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("๐ ุงูุนุฑูุงุจ ูุจุญุซ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ ููุณุชุดูุฑ ุฎุจุฑุงุก ุงูุฐูุงุก ุงูุงุตุทูุงุนู..."):
                
                # ุงูุจุญุซ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ
                relevant_docs = []
                search_info = ""
                
                if knowledge_base and model:
                    relevant_docs = advanced_semantic_search(prompt, model, knowledge_base)
                    if relevant_docs:
                        search_info = f"๐ **ุชู ุงูุนุซูุฑ ุนูู {len(relevant_docs)} ูุฑุฌุน ุฐู ุตูุฉ**\n"
                        for i, doc in enumerate(relevant_docs[:3]):
                            search_info += f"๐ {doc['source']}\n"
                        search_info += "\n"
                        st.info(search_info.strip())
                
                # ุงูุญุตูู ุนูู ุงูุฅุฌุงุจุฉ
                start_time = time.time()
                answer, source_info, answer_type = response_system.get_comprehensive_answer(prompt, relevant_docs)
                response_time = time.time() - start_time
                
                # ุชุญุฏูุฏ ูุฌุงุญ ุงูุงุณุชุฌุงุจุฉ
                is_successful = "ุฎุทุฃ" not in answer and "ุชุนุฐุฑ" not in answer
                if is_successful:
                    st.session_state.successful_responses += 1
                
                # ุชุญุฏูุฏ ุฃููููุฉ ุงููุตุฏุฑ
                source_icons = {
                    "ูุญูู": "๐", "Gemini": "๐ง", "DeepSeek": "๐", 
                    "HuggingFace": "๐ค", "ุงุญุชูุงุทู": "๐"
                }
                source_icon = "๐ง"
                for key, icon in source_icons.items():
                    if key in answer_type:
                        source_icon = icon
                        break
                
                # ุชูุณูู ุงูุฅุฌุงุจุฉ ุงูููุงุฆูุฉ
                response_with_metadata = f"""{answer}

---
### ๐ **ูุนูููุงุช ุงูุงุณุชุฌุงุจุฉ**
- {source_icon} **ุงููุตุฏุฑ**: {source_info}
- โก **ุงูููุน**: {answer_type}
- ๐ **ุฒูู ุงูุงุณุชุฌุงุจุฉ**: {response_time:.2f} ุซุงููุฉ
- ๐ **ุฌูุฏุฉ ุงูุจุญุซ**: {"ููุชุงุฒุฉ" if relevant_docs else "ุนุงูุฉ"}

*๐ก ููุญุตูู ุนูู ูุนูููุงุช ุฃูุซุฑ ุชูุตููุงูุ ุฌุฑุจ ุฃุณุฆูุฉ ูุญุฏุฏุฉ ุฃูุซุฑ!*"""
                
                st.markdown(response_with_metadata)
                st.session_state.messages.append({"role": "assistant", "content": response_with_metadata})

    # ุชุฐููู ุงูุตูุญุฉ
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #666; padding: 20px;">
        <p>๐งฌ <strong>ุงูุนุฑูุงุจ ููุฌููุงุช v13.0</strong> - ูุธุงู ุฐูู ูุชุทูุฑ ูุฎุจุฑุฉ ูุฑุงุซุฉ ุงูุญูุงู</p>
        <p>โก ูุฏุนูู ุจุชูููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงููุชูุฏูุฉ ูุงูุจุญุซ ุงูุฏูุงูู ุงูุฐูู</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
