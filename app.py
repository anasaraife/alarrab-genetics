# ==============================================================================
#  HOT-PATCH FOR SQLITE3 VERSION ON STREAMLIT CLOUD
#  This is a workaround for the issue where Streamlit's default sqlite3
#  version is too old for ChromaDB. This code must run BEFORE chromadb is imported.
# ==============================================================================
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# ==============================================================================


# ==============================================================================
#  ูุดุฑูุน ุงูุนุฑูุงุจ ููุฌููุงุช - ุงูุฅุตุฏุงุฑ 3.1 (ูุน ุญู ูุดููุฉ ุงูุชูุงูู)
# ==============================================================================

import streamlit as st
import chromadb
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os
import tempfile

# -------------------------------------------------
#  1. ุฅุนุฏุงุฏุงุช ุงูุตูุญุฉ ูุงููุตุงุฏุฑ
# -------------------------------------------------
st.set_page_config(
    page_title="ุงูุนุฑูุงุจ ููุฌููุงุช",
    page_icon="๐๏ธ",
    layout="wide",
)

# ูุงุฆูุฉ ุฑูุงุจุท ุงููุชุจ (ุณูุนุงูุฌ ุฃูู ูุชุงุจูู ููุท ูู ุงูุจุฏุงูุฉ)
BOOK_LINKS = [
    "[https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing](https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing)",
    "[https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing](https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing)",
    # "[https://drive.google.com/file/d/18pc9PptjfcjQfPyVCiaSq30RFs3ZjXF4/view?usp=sharing](https://drive.google.com/file/d/18pc9PptjfcjQfPyVCiaSq30RFs3ZjXF4/view?usp=sharing)", # ูุนุทู ูุคูุชุงู
    # ... ุจููุฉ ุงูุฑูุงุจุท ูุนุทูุฉ ูุคูุชุงู ูุชุญุณูู ุณุฑุนุฉ ุงููุดุฑ ุงูุฃููู
]

# -------------------------------------------------
#  2. ุชุญููู ุงูููุงุฐุฌ ูุฅุนุฏุงุฏ ูุงุนุฏุฉ ุงูุจูุงูุงุช (ุจุดูู ูุญุณูู)
# -------------------------------------------------

@st.cache_resource
def load_embedding_model():
    """
    ุชุญููู ูููุฐุฌ ุงูุชุถููู ูุชุฎุฒููู ูู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ.
    """
    try:
        return SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
    except Exception as e:
        st.error(f"ูุดู ูุงุฏุญ ูู ุชุญููู ูููุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู: {e}")
        st.stop()

@st.cache_resource
def init_chroma_db():
    """
    ุฅุนุฏุงุฏ ChromaDB ูู ูุฌูุฏ ูุคูุช ุขูู ุนูู ุงูุฎุงุฏู.
    """
    try:
        # ุงุณุชุฎุฏุงู ูุฌูุฏ ูุคูุช ูุถูุงู ุงูุชูุงูู ูุน ุฃู ุจูุฆุฉ ุชุดุบูู
        temp_dir = tempfile.gettempdir()
        db_path = os.path.join(temp_dir, "chroma_db_godfather")
        client = chromadb.PersistentClient(path=db_path)
        collection = client.get_or_create_collection(name="pigeon_genetics_knowledge")
        return collection
    except Exception as e:
        st.error(f"ูุดู ูุงุฏุญ ูู ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช: {e}")
        st.stop()

@st.cache_data(ttl=3600) # ุชุฎุฒูู ุงูุจูุงูุงุช ููุฏุฉ ุณุงุนุฉ
def build_knowledge_base(_collection, _model):
    """
    ุจูุงุก ูุงุนุฏุฉ ุงููุนุฑูุฉ ููุท ุฅุฐุง ูุงูุช ูุงุฑุบุฉ.
    """
    if _collection.count() == 0:
        with st.status("โ๏ธ ูุชู ุจูุงุก ูุงุนุฏุฉ ุงููุนุฑูุฉ ูุฃูู ูุฑุฉ...", expanded=True) as status:
            all_chunks, all_metadata, all_ids = [], [], []
            doc_id_counter = 0

            for i, link in enumerate(BOOK_LINKS):
                status.update(label=f"ุฌุงุฑู ูุนุงูุฌุฉ ุงููุชุงุจ {i+1}/{len(BOOK_LINKS)}...")
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                        file_id = link.split('/d/')[1].split('/')[0]
                        gdown.download(id=file_id, output=tmp.name, quiet=True)
                        
                        text = ""
                        with open(tmp.name, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page in reader.pages:
                                text += (page.extract_text() or "") + "\n"
                        
                        # ุชูุณูู ุงููุต ุฅูู ุฃุฌุฒุงุก
                        chunks = text.split('\n\n')
                        for chunk in chunks:
                            if len(chunk.strip()) > 150:
                                all_chunks.append(chunk.strip())
                                all_metadata.append({'source': link})
                                all_ids.append(f"doc_{doc_id_counter}")
                                doc_id_counter += 1
                except Exception as e:
                    st.warning(f"ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ูุนุงูุฌุฉ ุงููุชุงุจ {i+1}. ุณูุชู ุชุฎุทูู. ุงูุฎุทุฃ: {e}")
                finally:
                    if 'tmp' in locals() and os.path.exists(tmp.name):
                        os.remove(tmp.name)

            if all_chunks:
                status.update(label="ุฌุงุฑู ุชุญููู ุงููุตูุต ุฅูู ูุชุฌูุงุช...")
                embeddings = _model.encode(all_chunks).tolist()
                _collection.add(embeddings=embeddings, documents=all_chunks, metadatas=all_metadata, ids=all_ids)
            
            status.update(label="โ ุงูุชูู ุจูุงุก ูุงุนุฏุฉ ุงููุนุฑูุฉ ุจูุฌุงุญ!", state="complete")
    return True

# -------------------------------------------------
#  3. ูุงุฌูุฉ ุงููุณุชุฎุฏู ุงูุฑุฆูุณูุฉ
# -------------------------------------------------
st.title("๐๏ธ ุงูุนุฑูุงุจ ููุฌููุงุช - ุงูุฅุตุฏุงุฑ 3.1 (ูุณุชูุฑ)")
st.write("ุงุจุญุซ ูู ุงููุฑุงุฌุน ุงูุนูููุฉ ููุฑุงุซุฉ ุงูุญูุงู.")

# ุชุญููู ุงูููููุงุช ุงูุฃุณุงุณูุฉ
model = load_embedding_model()
db_collection = init_chroma_db()
build_knowledge_base(db_collection, model)

# ูุฑุจุน ุงูุจุญุซ
query = st.text_input("ุงูุชุจ ุณุคุงูู ููุง:", placeholder="ูุซุงู: ูุง ูู ุชุฃุซูุฑ ุฌูู Spreadุ")

if query:
    with st.spinner("ุฌุงุฑู ุงูุจุญุซ..."):
        # ุชุญููู ุงูุณุคุงู ุฅูู ูุชุฌู ููุจุญุซ
        query_embedding = model.encode([query]).tolist()
        results = db_collection.query(query_embeddings=query_embedding, n_results=3)

        documents = results.get('documents', [[]])[0]
        if documents:
            for i, doc in enumerate(documents):
                similarity = (1 - results['distances'][0][i]) * 100
                source = results['metadatas'][0][i]['source']
                
                if i == 0:
                    st.success(f"๐ ุฃูุถู ูุชูุฌุฉ (ุจูุณุจุฉ ุชุดุงุจู ~{similarity:.0f}%):")
                    st.markdown(f"> {doc}")
                    st.caption(f"ุงููุตุฏุฑ: {source}")
                else:
                    with st.expander(f"ูุชูุฌุฉ ุฅุถุงููุฉ (ุจูุณุจุฉ ุชุดุงุจู ~{similarity:.0f}%)"):
                        st.info(doc)
                        st.caption(f"ุงููุตุฏุฑ: {source}")
        else:
            st.warning("ูู ูุชู ุงูุนุซูุฑ ุนูู ูุชุงุฆุฌ ูุทุงุจูุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุญุงููุฉ.")
