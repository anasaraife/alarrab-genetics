# ==============================================================================
#  ูุดุฑูุน ุงูุนุฑูุงุจ ููุฌููุงุช: ุงููุงุฌูุฉ ุงูุชูุงุนููุฉ (ุจูุญุฑู ChromaDB)
#  ุงููุฑุญูุฉ 2: ุจูุงุก ุงููุงุฌูุฉ ูุงูุชูุงูู
#  -- ุงูุฅุตุฏุงุฑ 2.0: ุชู ุงูุชุญุฏูุซ ููุนูู ูุน ChromaDB ููุชุฌูุจ ูุดุงูู ุงููุดุฑ --
# ==============================================================================

import streamlit as st
import chromadb
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os
import time

# -------------------------------------------------
#  1. ุฅุนุฏุงุฏุงุช ุงูุตูุญุฉ ูุงููุตุงุฏุฑ
# -------------------------------------------------
st.set_page_config(
    page_title="ุงูุนุฑูุงุจ ููุฌููุงุช",
    page_icon="๐๏ธ",
    layout="wide",
)

# ูุงุฆูุฉ ุฑูุงุจุท ุงููุชุจ ุงูุนูููุฉ (PDFs)
BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
    "https://drive.google.com/file/d/18pc9PptjfcjQfPyVCiaSq30RFs3ZjXF4/view?usp=sharing",
    "https://drive.google.com/file/d/17hklyXm2R6ChYRddDbYRkqrtD8mE_nC_/view?usp=sharing",
    "https://drive.google.com/file/d/1Mq3zgz4NDm6guelOzuni3O4_2kaQpJAi/view?usp=sharing",
    "https://drive.google.com/file/d/1hoCxIPU9xJgsl1J-AnEG2E0AX3H5c5Kg/view?usp=sharing",
    "https://drive.google.com/file/d/14qInRfBTOhOJYsjs6tYRxAq1xFDrD-_O/view?usp=sharing",
    "https://drive.google.com/file/d/1kaVob_EdCP5v_H71nUS3O1-YairROV1b/view?usp=sharing"
]

# -------------------------------------------------
#  2. ุชุญููู ุงูููุงุฐุฌ ูุฅุนุฏุงุฏ ChromaDB
# -------------------------------------------------

@st.cache_resource
def load_embedding_model(model_name='paraphrase-multilingual-mpnet-base-v2'):
    """
    ุชููู ุจุชุญููู ูููุฐุฌ ุชุญููู ุงููุตูุต ุฅูู ูุชุฌูุงุช.
    """
    return SentenceTransformer(model_name)

def initialize_chroma_db(model):
    """
    ุชููู ุจุฅุนุฏุงุฏ ChromaDBุ ูุจูุงุก ูุงุนุฏุฉ ุงููุนุฑูุฉ ุฅุฐุง ูู ุชูู ููุฌูุฏุฉ.
    """
    # ุฅุนุฏุงุฏ ุงูุนููู ุงูุฏุงุฆู (ุณูุญูุธ ุงูุจูุงูุงุช ูู ูุฌูุฏ ุนูู ุงูุฎุงุฏู)
    client = chromadb.PersistentClient(path="chroma_db_store")
    
    # ุงูุญุตูู ุนูู ุงููุฌููุนุฉ ุฃู ุฅูุดุงุคูุง
    collection = client.get_or_create_collection(name="pigeon_genetics_knowledge")

    # ุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ูุงุนุฏุฉ ุงููุนุฑูุฉ ูุงุฑุบุฉ ูุชุญุชุงุฌ ููุจูุงุก
    if collection.count() == 0:
        with st.status("โณ ูุชู ุจูุงุก ูุงุนุฏุฉ ุงููุนุฑูุฉ ูุฃูู ูุฑุฉุ ูุฑุฌู ุงูุงูุชุธุงุฑ...", expanded=True) as status:
            st.write("ุงูุฎุทูุฉ 1/3: ุชุญููู ุงููุฑุงุฌุน ุงูุนูููุฉ (PDFs)...")
            all_texts = []
            for i, link in enumerate(BOOK_LINKS):
                try:
                    file_id = link.split('/d/')[1].split('/')[0]
                    output_filename = f"{file_id}.pdf"
                    gdown.download(id=file_id, output=output_filename, quiet=True)
                    
                    text = ""
                    with open(output_filename, 'rb') as f:
                        reader = PyPDF2.PdfReader(f)
                        for page in reader.pages:
                            text += page.extract_text() or "" + "\n"
                    
                    all_texts.append({'source': link, 'content': text})
                    os.remove(output_filename)
                except Exception as e:
                    st.warning(f"ูุดู ุชุญููู ุฃู ูุฑุงุกุฉ ุงููุชุงุจ: {link}. ุงูุฎุทุฃ: {e}")
            
            st.write("ุงูุฎุทูุฉ 2/3: ุชูุณูู ุงููุตูุต ุฅูู ุฃุฌุฒุงุก ูุงุจูุฉ ููุจุญุซ...")
            all_chunks = []
            all_metadata = []
            all_ids = []
            doc_id_counter = 0
            for doc in all_texts:
                chunks = doc['content'].split('\n\n')
                for chunk in chunks:
                    if len(chunk.strip()) > 150:
                        all_chunks.append(chunk.strip())
                        all_metadata.append({'source': doc['source']})
                        all_ids.append(f"doc_{doc_id_counter}")
                        doc_id_counter += 1
            
            st.write(f"ุงูุฎุทูุฉ 3/3: ุชุญููู ุงููุตูุต ุฅูู ูุชุฌูุงุช ูุฅุถุงูุชูุง ููุงุนุฏุฉ ุงููุนุฑูุฉ...")
            if all_chunks:
                # ุฅุถุงูุฉ ุงูุจูุงูุงุช ุฅูู ุงููุฌููุนุฉ ุนูู ุฏูุนุงุช ูุชุฌูุจ ุงุณุชููุงู ุงูุฐุงูุฑุฉ
                batch_size = 100
                for i in range(0, len(all_chunks), batch_size):
                    collection.add(
                        embeddings=model.encode(all_chunks[i:i+batch_size]).tolist(),
                        documents=all_chunks[i:i+batch_size],
                        metadatas=all_metadata[i:i+batch_size],
                        ids=all_ids[i:i+batch_size]
                    )
                    time.sleep(1) # ุฅุนุทุงุก ุงูุฎุงุฏู ูุฑุตุฉ ููุชููุณ
            
            status.update(label="โ ุงูุชูู ุจูุงุก ูุงุนุฏุฉ ุงููุนุฑูุฉ ุจูุฌุงุญ!", state="complete", expanded=False)

    return collection

# -------------------------------------------------
#  3. ุฏุงูุฉ ุงูุจุญุซ ุงูุฌุฏูุฏุฉ
# -------------------------------------------------
def search_knowledge_base(query, model, collection, n_results=5):
    """
    ุชุจุญุซ ุนู ุฅุฌุงุจุฉ ุจุงุณุชุฎุฏุงู ChromaDB.
    """
    query_embedding = model.encode([query]).tolist()
    
    results = collection.query(
        query_embeddings=query_embedding,
        n_results=n_results
    )
    return results

# -------------------------------------------------
#  4. ุจูุงุก ูุงุฌูุฉ ุงููุณุชุฎุฏู
# -------------------------------------------------
st.title("๐๏ธ ุงูุนุฑูุงุจ ููุฌููุงุช: ุงูุฅุตุฏุงุฑ 2.0 (ุจูุญุฑู ChromaDB)")
st.write("ุงุทุฑุญ ุณุคุงูุงู ููุญุตูู ุนูู ุฅุฌุงุจุงุช ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุนูููุฉ ุงูุชู ุจูููุงูุง.")

# ุชุญููู ุงููููุฐุฌ ูุฅุนุฏุงุฏ ูุงุนุฏุฉ ุงูุจูุงูุงุช
embedding_model = load_embedding_model()
knowledge_collection = initialize_chroma_db(embedding_model)

# ูุฑุจุน ุฅุฏุฎุงู ุงูุณุคุงู
user_query = st.text_input("ุงุณุฃู ุนู ุฃู ุดูุก ูู ูุฑุงุซุฉ ุงูุญูุงู...", placeholder="ูุซุงู: ูุง ูู ุฌูู ุงูุฃูุจุงู ุงูุณุงุฆุฏุ")

if user_query:
    with st.spinner("ุฌุงุฑู ุงูุจุญุซ ูู ุงููุฑุงุฌุน ุงูุนูููุฉ..."):
        search_results = search_knowledge_base(user_query, embedding_model, knowledge_collection)

    st.subheader("ูุชุงุฆุฌ ุงูุจุญุซ:")
    
    # ุงุณุชุฎุฑุงุฌ ุงููุณุชูุฏุงุช ูุงููุตุงุฏุฑ ูู ูุชุงุฆุฌ ChromaDB
    documents = search_results.get('documents', [[]])[0]
    metadatas = search_results.get('metadatas', [[]])[0]
    distances = search_results.get('distances', [[]])[0]

    if not documents:
        st.warning("ูู ุฃุชููู ูู ุงูุนุซูุฑ ุนูู ุฅุฌุงุจุฉ ุฏูููุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุญุงููุฉ.")
    else:
        for i, doc in enumerate(documents):
            source = metadatas[i].get('source', 'ุบูุฑ ูุนุฑูู')
            # ุชุญููู ุงููุณุงูุฉ ุฅูู ุฏุฑุฌุฉ ุชุดุงุจู (ุชูุฑูุจู)
            similarity = (1 - distances[i]) * 100 
            
            if i == 0:
                st.success(f"**ุฃูุถู ูุชูุฌุฉ (ุจูุณุจุฉ ุชุดุงุจู ~{similarity:.0f}%):**")
                st.markdown(f"> {doc}")
                st.caption(f"ุงููุตุฏุฑ: {source}")
            else:
                with st.expander(f"ูุชูุฌุฉ ุฅุถุงููุฉ (ุจูุณุจุฉ ุชุดุงุจู ~{similarity:.0f}%)"):
                    st.info(doc)
                    st.caption(f"ุงููุตุฏุฑ: {source}")

