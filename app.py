# ==============================================================================
#  Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª: Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© (Ø¨Ù…Ø­Ø±Ùƒ ChromaDB)
#  Ø§Ù„Ø¥ØµØ¯Ø§Ø± 2.0: Ù†Ø³Ø®Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Streamlit Cloud
# ==============================================================================

import streamlit as st
import chromadb
from sentence_transformers import SentenceTransformer
import gdown
import PyPDF2
import os

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
# -------------------------------------------------
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª",
    page_icon="ğŸ•Šï¸",
    layout="wide",
)

BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
    "https://drive.google.com/file/d/18pc9PptjfcjQfPyVCiaSq30RFs3ZjXF4/view?usp=sharing",
    "https://drive.google.com/file/d/17hklyXm2R6ChYRddDbYRkqrtD8mE_nC_/view?usp=sharing",
    "https://drive.google.com/file/d/1Mq3zgz4NDm6guelOzuni3O4_2kaQpJAi/view?usp=sharing",
    "https://drive.google.com/file/d/1hoCxIPU9xJgsl1J-AnEG2E0AX3H5c5Kg/view?usp=sharing",
    "https://drive.google.com/file/d/14qInRfBTOhOJYsjs6tYRxAq1xFDrD-_O/view?usp=sharing",
    "https://drive.google.com/file/d/1kaVob_EdCP5v_H71nUS3O1-YairROV1b/view?usp=sharing"
]

# -------------------------------------------------
#  2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# -------------------------------------------------
@st.cache_resource
def load_embedding_model(model_name='paraphrase-multilingual-mpnet-base-v2'):
    return SentenceTransformer(model_name)

# -------------------------------------------------
#  3. Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ChromaDB)
# -------------------------------------------------
@st.cache_resource
def initialize_chroma_db(_model):
    client = chromadb.PersistentClient(path="chroma_db_store")
    collection = client.get_or_create_collection(name="pigeon_genetics_knowledge")

    if collection.count() == 0:
        with st.status("â³ ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„Ø£ÙˆÙ„ Ù…Ø±Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±.", expanded=True) as status:
            st.write("Ø§Ù„Ø®Ø·ÙˆØ© 1/3: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© (PDFs).")
            all_texts = []
            for link in BOOK_LINKS:
                try:
                    file_id = link.split('/d/')[1].split('/')[0]
                    output_filename = f"{file_id}.pdf"
                    gdown.download(id=file_id, output=output_filename, quiet=True)

                    text = ""
                    with open(output_filename, 'rb') as f:
                        reader = PyPDF2.PdfReader(f)
                        if reader.is_encrypted:
                            reader.decrypt("")
                        for page in reader.pages:
                            text += (page.extract_text() or "") + "\n"

                    all_texts.append({'source': link, 'content': text})
                    os.remove(output_filename)
                except Exception as e:
                    st.warning(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒØªØ§Ø¨: {link}. Ø§Ù„Ø®Ø·Ø£: {e}")

            st.write("Ø§Ù„Ø®Ø·ÙˆØ© 2/3: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ.")
            all_chunks, all_metadata, all_ids = [], [], []
            doc_id_counter = 0
            for doc in all_texts:
                chunks = doc['content'].split('\n\n')
                for chunk in chunks:
                    if len(chunk.strip()) > 150:
                        all_chunks.append(chunk.strip())
                        all_metadata.append({'source': doc['source']})
                        all_ids.append(f"doc_{doc_id_counter}")
                        doc_id_counter += 1

            st.write("Ø§Ù„Ø®Ø·ÙˆØ© 3/3: Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.")
            if all_chunks:
                collection.add(
                    documents=all_chunks,
                    metadatas=all_metadata,
                    ids=all_ids
                )

            status.update(label="âœ… Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø¬Ø§Ø­!", state="complete", expanded=False)

    return collection

# -------------------------------------------------
#  4. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
# -------------------------------------------------
def search_knowledge_base(query, collection, n_results=5):
    return collection.query(query_texts=[query], n_results=n_results)

# -------------------------------------------------
#  5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# -------------------------------------------------
st.title("ğŸ•Šï¸ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª (ChromaDB)")
st.write("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ø±Ø§Ø¬Ø¹ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù….")

embedding_model = load_embedding_model()
knowledge_collection = initialize_chroma_db(embedding_model)

user_query = st.text_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ ÙÙŠ ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù…...", placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡Ùˆ Ø¬ÙŠÙ† Ø§Ù„Ø£ÙˆØ¨Ø§Ù„ØŸ")

if user_query:
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø«..."):
        search_results = search_knowledge_base(user_query, knowledge_collection)

    st.subheader("Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«:")
    documents = search_results.get('documents', [[]])[0]
    metadatas = search_results.get('metadatas', [[]])[0]
    distances = search_results.get('distances', [[]])[0]

    if not documents:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©.")
    else:
        for i, doc in enumerate(documents):
            source = metadatas[i].get('source', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
            similarity = (1 - distances[i]) * 100
            if i == 0:
                st.success(f"**Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© (~{similarity:.0f}%):**")
                st.markdown(f"> {doc}")
                st.caption(f"Ø§Ù„Ù…ØµØ¯Ø±: {source}")
            else:
                with st.expander(f"Ù†ØªÙŠØ¬Ø© Ø¥Ø¶Ø§ÙÙŠØ© (~{similarity:.0f}%)"):
                    st.info(doc)
                    st.caption(f"Ø§Ù„Ù…ØµØ¯Ø±: {source}")
