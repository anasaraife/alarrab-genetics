# ===================================================================
# ğŸš€ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V4.0 - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙˆØ§Ù„Ù…ØªØ·ÙˆØ±Ø©
# Ù…Ù…ÙŠØ²Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©: Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ©ØŒ ØªØµØ¯ÙŠØ±ØŒ Ø°Ø§ÙƒØ±Ø© Ù…Ø­Ø§Ø¯Ø«Ø©ØŒ ÙˆØ§Ø¬Ù‡Ø© Ù…Ø­Ø³Ù†Ø©
# ===================================================================

import streamlit as st
from itertools import product
import collections
import pandas as pd
import numpy as np
import pickle
import os
import json
from datetime import datetime
from typing import List, Dict
import plotly.express as px
import hashlib
import time

# --- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ---
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

try:
    import faiss
    from sentence_transformers import SentenceTransformer
    VECTOR_SEARCH_AVAILABLE = True
except ImportError:
    VECTOR_SEARCH_AVAILABLE = False

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(
    layout="wide",
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V4.0",
    page_icon="ğŸ§¬",
    initial_sidebar_state="expanded"
)

# --- 2. CSS Ù…Ø®ØµØµ Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© ---
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# --- 3. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¬Ù„Ø³Ø© ---
def initialize_session_state():
    """ØªÙ‡ÙŠØ¦Ø© Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø©."""
    defaults = {
        "messages": [],
        "search_history": [],
        "conversation_id": hashlib.md5(str(time.time()).encode()).hexdigest()[:8],
        "session_stats": {
            "queries_count": 0,
            "successful_searches": 0,
            "charts_generated": 0
        }
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

# --- 4. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ---
@st.cache_resource(show_spinner="Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©...")
def load_enhanced_resources():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ© ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬."""
    resources = {
        "vector_db": None, "embedder": None, "model": None,
        "status": "loading", "backup_data": None
    }
    
    if VECTOR_SEARCH_AVAILABLE:
        vector_db_path = "vector_db.pkl"
        if os.path.exists(vector_db_path):
            try:
                with open(vector_db_path, "rb") as f:
                    resources["vector_db"] = pickle.load(f)
                resources["embedder"] = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                resources["status"] = "ready"
            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª: {e}")
                resources["status"] = "failed"
        else:
            resources["status"] = "no_db_file"
    else:
        resources["status"] = "vector_search_unavailable"

    if GEMINI_AVAILABLE and "GEMINI_API_KEY" in st.secrets:
        try:
            genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
            resources["model"] = genai.GenerativeModel('gemini-1.5-flash',
                generation_config={"temperature": 0.1, "max_output_tokens": 4096})
        except Exception as e:
            st.error(f"ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Gemini: {e}")
    
    if not resources["vector_db"]:
        resources["backup_data"] = create_sample_genetics_data()
        
    return resources

def create_sample_genetics_data():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©."""
    return { "genetics_info": { "brown_gene": { "name": "Ø§Ù„Ø¬ÙŠÙ† Ø§Ù„Ø¨Ù†ÙŠ (Brown)", "symbol": "b", "description": "ÙŠØªØ­ÙƒÙ… ÙÙŠ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØµØ¨ØºØ© Ø§Ù„Ø¨Ù†ÙŠØ©.", "inheritance": "Ù…ØªÙ†Ø­ÙŠ" }}}

# --- 5. ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ ---
def search_knowledge(query: str, resources: dict, top_k: int = 5) -> List[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©."""
    if resources["vector_db"] and resources["embedder"]:
        index = resources["vector_db"]["index"]
        chunks = resources["vector_db"]["chunks"]
        metadata = resources["vector_db"].get("metadata", [])
        query_embedding = resources["embedder"].encode([query])
        distances, indices = index.search(np.array(query_embedding, dtype=np.float32), top_k)
        return [{"content": chunks[idx], "metadata": metadata[idx] if idx < len(metadata) else {}, "score": float(1 - dist)} for dist, idx in zip(distances[0], indices[0]) if idx < len(chunks)]
    return []

def research_agent(query: str, resources: dict):
    """Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¨Ø­Ø«ÙŠ Ø§Ù„Ù…Ø­Ù„Ù„."""
    if not resources.get("model"):
        return "âŒ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ù…Ù‡ÙŠØ£ (API KEY Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ ØºÙŠØ± ØµØ§Ù„Ø­)."

    q_lower = query.lower().strip()
    if any(word in q_lower for word in ["Ø³Ù„Ø§Ù…", "Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù‡Ù„Ø§", "Ù‡Ø§ÙŠ", "Ø´ÙƒØ±Ø§"]):
        return "ğŸ¤— ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…! Ø£Ù†Ø§ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ V4.0ØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"

    with st.spinner("ğŸ”¬ Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„..."):
        search_results = search_knowledge(query, resources)
        if not search_results:
            return "ğŸ¤” Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø©. Ø¬Ø±Ø¨ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„."

        context_parts = []
        for i, result in enumerate(search_results):
            source = result['metadata'].get('source', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
            page = result['metadata'].get('page', 'N/A')
            context_parts.append(f"[Ù…ØµØ¯Ø± {i+1} Ù…Ù† '{source}' ØµÙØ­Ø© {page}]:\n{result['content']}")
        context_text = "\n\n---\n\n".join(context_parts)

        prompt = f"""
        Ø£Ù†Øª "Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ø§Ù„Ø°ÙƒÙŠ V4.0"ØŒ Ø®Ø¨ÙŠØ± Ø¹Ø§Ù„Ù…ÙŠ ÙÙŠ Ø¹Ù„Ù… ÙˆØ±Ø§Ø«Ø© Ø§Ù„Ø­Ù…Ø§Ù….
        Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØµÙŠØ§ØºØ© Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….

        **Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©:**
        ---
        {context_text}
        ---
        **Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {query}
        **ØªØ¹Ù„ÙŠÙ…Ø§Øª:** Ø£Ø¬Ø¨ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¹Ù„Ù…ÙŠ ÙˆÙ…Ù†Ø¸Ù…ØŒ ÙˆØ§Ø³ØªÙ†ØªØ¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.
        **Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠØ©:**
        """
        try:
            response = resources["model"].generate_content(prompt)
            return response.text
        except Exception as e:
            return f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØµÙŠØ§ØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {str(e)}"

# --- 6. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ---
def main():
    """Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚."""
    initialize_session_state()
    resources = load_enhanced_resources()

    st.markdown('<div class="main-header"><h1>ğŸš€ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª V4.0</h1><p>Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© - ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„ÙˆØ±Ø§Ø«Ø© ÙˆØ£Ù„ÙˆØ§Ù† Ø§Ù„Ø­Ù…Ø§Ù… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</p></div>', unsafe_allow_html=True)

    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("ğŸ”§ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…")
        if resources["status"] == "ready":
            st.success("âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¬Ø§Ù‡Ø²Ø©")
            st.metric("ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹", len(resources["vector_db"]['chunks']))
        else:
            st.error("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
        
        if resources.get("model"):
            st.success("âœ… Gemini Ù…ØªØµÙ„")
        else:
            st.error("âŒ Gemini ØºÙŠØ± Ù…ØªØ§Ø­")
        
        st.divider()
        st.header("ğŸ“¥ ØªØµØ¯ÙŠØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        json_data = json.dumps(st.session_state.messages, ensure_ascii=False, indent=2)
        st.download_button(label="â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ JSON", data=json_data, file_name="conversation.json", mime="application/json")

    # ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø°ÙƒÙŠØ©")
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø¨Ù†ÙŠØŸ"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            response = research_agent(prompt, resources)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
        st.rerun()

# --- 7. ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ---
if __name__ == "__main__":
    main()
