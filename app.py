# ==============================================================================
#  Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 9.0 (Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù…ÙˆØ«Ù‚ Ø¨Ø§Ù„Ù…ØµØ§Ø¯Ø± - RAG)
#  - ÙŠØ¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹ Ø«Ù… ÙŠØ³ØªØ®Ø¯Ù… Gemini Ù„ØµÙŠØ§ØºØ© Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ«Ù‚Ø©.
# ==============================================================================

import streamlit as st
import sqlite3
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import gdown
import PyPDF2
import os
import tempfile
import hashlib
import requests
import json

# -------------------------------------------------
#  1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø±
# -------------------------------------------------
st.set_page_config(
    page_title="Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 9.0",
    page_icon="ğŸ•Šï¸",
    layout="wide",
)

# Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ¨
BOOK_LINKS = [
    "https://drive.google.com/file/d/1CRwW78pd2RsKVd37elefz71RqwaCaute/view?usp=sharing",
    "https://drive.google.com/file/d/1894OOW1nEc3SkanLKKEzaXu_XhXYv8rF/view?usp=sharing",
    "https://drive.google.com/file/d/18pc9PptjfcjQfPyVCiaSq30RFs3ZjXF4/view?usp=sharing",
    "https://drive.google.com/file/d/17hklyXm2R6ChYRddDbYRkqrtD8mE_nC_/view?usp=sharing",
    "https://drive.google.com/file/d/1Mq3zgz4NDm6guelOzuni3O4_2kaQpJAi/view?usp=sharing",
    "https://drive.google.com/file/d/1hoCxIPU9xJgsl1J-AnEG2E0AX3H5c5Kg/view?usp=sharing",
    "https://drive.google.com/file/d/14qInRfBTOhOJYsjs6tYRxAq1xFDrD-_O/view?usp=sharing",
    "https://drive.google.com/file/d/1kaVob_EdCP5v_H71nUS3O1-YairROV1b/view?usp=sharing"
]

# -------------------------------------------------
#  2. Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© (SQLite)
# -------------------------------------------------

@st.cache_resource
def init_sqlite_db():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite ÙÙŠ Ù…Ø¬Ù„Ø¯ Ù…Ø¤Ù‚Øª."""
    db_path = os.path.join(tempfile.gettempdir(), "genetics_knowledge_v9.db")
    conn = sqlite3.connect(db_path, check_same_thread=False)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS knowledge_base (
            id INTEGER PRIMARY KEY, content TEXT NOT NULL, source TEXT NOT NULL, content_hash TEXT UNIQUE
        )
    """)
    conn.commit()
    return conn

@st.cache_data(ttl=86400) # ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯
def build_knowledge_base_from_sources(_conn):
    """Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„ÙƒØªØ¨ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ÙØ§Ø±ØºØ©."""
    cursor = _conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM knowledge_base")
    if cursor.fetchone()[0] == 0:
        with st.spinner("ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ©..."):
            for i, link in enumerate(BOOK_LINKS):
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
                        file_id = link.split('/d/')[1].split('/')[0]
                        gdown.download(id=file_id, output=tmp.name, quiet=True)
                        with open(tmp.name, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page_num, page in enumerate(reader.pages):
                                text = page.extract_text() or ""
                                if len(text.strip()) > 150:
                                    content_hash = hashlib.md5(text.encode()).hexdigest()
                                    cursor.execute("INSERT OR IGNORE INTO knowledge_base (content, source, content_hash) VALUES (?, ?, ?)",
                                                   (text.strip(), f"Book {i+1}, Page {page_num+1}", content_hash))
                        os.remove(tmp.name)
                except Exception as e:
                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒØªØ¨ Ø§Ù„ØªÙŠ ØªÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¨ØµÙ…Øª Ù„ØªØ¬Ù†Ø¨ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
                    print(f"Could not process book {i+1}: {e}")
                    pass
            _conn.commit()
    return True

# -------------------------------------------------
#  3. Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (RAG)
# -------------------------------------------------

def search_local_knowledge(query, conn, limit=3):
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚."""
    cursor = conn.cursor()
    cursor.execute("SELECT content, source FROM knowledge_base")
    results = cursor.fetchall()
    if not results: return []
    
    documents = [row[0] for row in results]
    sources = [row[1] for row in results]
    
    try:
        vectorizer = TfidfVectorizer(max_features=1000).fit(documents)
        tfidf_matrix = vectorizer.transform(documents)
        query_vector = vectorizer.transform([query])
        
        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
        top_indices = similarities.argsort()[-limit:][::-1]
        
        return [{"content": documents[i], "source": sources[i]} for i in top_indices if similarities[i] > 0.1]
    except ValueError:
        # ÙŠØ­Ø¯Ø« Ù‡Ø°Ø§ Ø§Ù„Ø®Ø·Ø£ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙØ§Ø±ØºÙ‹Ø§ (Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª)
        return []


@st.cache_data
def get_rag_answer_with_gemini(query, context_docs):
    """
    ÙŠØ³ØªØ®Ø¯Ù… Gemini API Ù„ØµÙŠØ§ØºØ© Ø¥Ø¬Ø§Ø¨Ø© Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø±.
    """
    API_KEY = ""
    API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}"
    
    context = "\n\n".join([f"Source: {doc['source']}\nContent: {doc['content']}" for doc in context_docs])
    
    prompt = f"""
    You are a world-class expert in pigeon genetics named 'Al-Arrab'.
    Based **exclusively** on the scientific context provided below, answer the user's question in clear, conversational Arabic.
    If the answer is not in the context, you MUST state that the information is not available in the provided documents. Do not use any prior knowledge.

    **Scientific Context:**
    ---
    {context}
    ---

    **User's Question:**
    {query}

    **Your Answer (in Arabic, based only on the context):**
    """
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    
    try:
        response = requests.post(API_URL, json=payload, headers={"Content-Type": "application/json"})
        response.raise_for_status()
        result = response.json()
        return result['candidates'][0]['content']['parts'][0]['text']
    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰. Ø§Ù„Ø®Ø·Ø£: {str(e)}"

# -------------------------------------------------
#  4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# -------------------------------------------------

st.title("ğŸ•Šï¸ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ Ù„Ù„Ø¬ÙŠÙ†Ø§Øª - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 9.0 (Ø§Ù„Ø°ÙƒÙŠ ÙˆØ§Ù„Ù…ÙˆØ«Ù‚)")
st.markdown("Ø­Ø§ÙˆØ± Ø®Ø¨ÙŠØ± Ø§Ù„ÙˆØ±Ø§Ø«Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©")

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
db_conn = init_sqlite_db()
build_knowledge_base_from_sources(db_conn)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø¬Ù„Ø³Ø© Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨. Ø§Ø³Ø£Ù„Ù†ÙŠ Ø£ÙŠ Ø³Ø¤Ø§Ù„ØŒ ÙˆØ³Ø£Ø¨Ø­Ø« Ù„Ùƒ Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠØ©."}]

# Ø¹Ø±Ø¶ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Ø­Ù‚Ù„ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
if prompt := st.chat_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø¬ÙŠÙ†ØŒ Ø·ÙØ±Ø©ØŒ Ø£Ùˆ Ù†Ù…Ø· ÙˆØ±Ø§Ø«ÙŠ..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Ø§Ù„Ø¹Ø±Ù‘Ø§Ø¨ ÙŠØ¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙˆÙŠÙÙƒØ±..."):
            # 1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚
            relevant_docs = search_local_knowledge(prompt, db_conn)
            
            if not relevant_docs:
                response = "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©."
            else:
                # 2. ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø°ÙƒÙŠØ© ÙˆÙ…ÙˆØ«Ù‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚
                response = get_rag_answer_with_gemini(prompt, relevant_docs)
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                sources = "ØŒ ".join(list(set([doc['source'] for doc in relevant_docs])))
                response += f"\n\n*Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„ÙŠÙ‡Ø§: {sources}*"
        
        st.markdown(response)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
